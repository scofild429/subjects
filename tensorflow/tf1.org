#+TITLE: tf1
#+OPTIONS: num:nil
#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup
#+STARTUP: content
* Tensorflow
** Constand additional und multiply
*** addition
#+BEGIN_SRC python   :results output :session
  import tensorflow as tf
  a = tf.constant([1.0, 2.0])
  b = tf.constant([3.0, 4.0])

  a_m = tf.constant([[1.0, 2.0]])
  b_m = tf.constant([[3.0], [4.0]])

  result_add = a + b
  result_multpl = tf.matmul(a_m, b_m)

  with tf.Session() as sess:
      print (sess.run(result_add))
      print (sess.run(result_multpl))

  print (result_add)
  print (result_multpl)
#+END_SRC


*** multiply

#+BEGIN_SRC python   :results output :session

  import tensorflow as tf
  #创建常量
  m1= tf.constant([[3,3]])
  m2=tf.constant([[1],[2]])
  #相乘
  product = tf.matmul(m1,m2)
  print(product)

  #定义一个会话，启动默认图
  sess = tf.Session()
  #调用sess，执行乘法运算
  result = sess.run(product)
  print(result)
  #不要忘了关闭sess
  sess.close()

  with tf.Session() as sess:
      result = sess.run(product)
      print(result)
  #使用with不需要专门关闭sess


#+END_SRC

#+RESULTS:
: Tensor("MatMul:0", shape=(1, 1), dtype=int32)
: [[9]]
: [[9]]

** Variable
*** the first using of variable & subtract add 
#+BEGIN_SRC python :results output
  import tensorflow as tf
  x = tf.Variable([1,2])
  a = tf.constant([3,3])
  sub = tf.subtract(x,a)
  add = tf.add(x,sub)
# 对于变量，要初始化init
  init = tf.global_variables_initializer()
  with tf.Session() as sess:
      sess.run(init)
      print(sess.run(sub))
      print(sess.run(add))


#+END_SRC

#+RESULTS:
: [-2 -1]
: [-1  1]

*** assign a value for variable and update & assign 
#+BEGIN_SRC python :results output
  #............................．．．．．．变量计算
  #变量可以被起名，初始化为０
  import tensorflow as tf
  state = tf.Variable(0, name = 'counter')
  new_value = tf.add(state,1)
  #赋值功能assign
  update = tf.assign(state,new_value)
  init = tf.global_variables_initializer()
  with tf.Session() as sess:
      sess.run(init)
      print(state)
      for _ in range(5):
          sess.run(update)
          print(sess.run(state))
          print(sess.run(update))


#+END_SRC

#+RESULTS:
#+begin_example
<tf.Variable 'counter:0' shape=() dtype=int32_ref>
1
2
3
4
5
6
7
8
9
10
#+end_example

*** fetch and Feed &placeholder
#+BEGIN_SRC python :results output
  import tensorflow as tf
  # Fetch 在一个会话里执行多个op
  input1 = tf.constant(3.0)
  input2 = tf.constant(2.0)
  input3 = tf.constant(5.0)

  add = tf.add(input2,input3)
  mul = tf.multiply(input1,add)

  with tf.Session() as sess:
      result = sess.run([add,mul])  #有[]
      print(result)

  #------------------Feed
  #创建占位符
  input4= tf.placeholder(tf.float32)
  input5 = tf.placeholder(tf.float32)
  output = tf.multiply(input4, input5)
  with tf.Session() as sess:
      print(sess.run(output,feed_dict = {input4:[7.0],input5:[2.0]}))  #随后赋值是用字典的方式进行的feed_dict = {input4:[7.0],input5:[2.0]}, 数字还加了方括号．


#+END_SRC

#+RESULTS:
: [7.0, 21.0]
: [14.]

** Tuning
*** change learing rate
#+BEGIN_SRC python
  #coding:utf-8

  import tensorflow as tf
  LEARING_RATE_BASE = 0.1
  LEARING_RATE_DECAY = 0.99
  LEARING_RATE_STEP= 1

  global_step = tf.Variable(0,trainable = False)
  learning_rate = tf.train.exponential_decay(LEARING_RATE_BASE, global_step,
  LEARING_RATE_STEP, LEARING_RATE_DECAY, staircase = True)
  
  w = tf.Variable(tf.constant(5, dtype = tf.float32))
  loss = tf.square(w+1)

  train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step = global_step)

  with tf.Session() as sess:
      init_op = tf.global_variables_initializer()
      sess.run(init_op)
      for i in range(40):
          sess.run(train_step)
          learnin_rate_val = sess.run(learning_rate)
          global_step_val = sess.run(global_step)
          w_val = sess.run(w)
          loss_val = sess.run(loss)
          print(" After {} steps: global_step is {}, w is {}, learnin_rate is {}, loss is {}"
  .format(i, global_step_val, w_val, learnin_rate_val, loss_val))


#+END_SRC
*** learing rate for loss
#+BEGIN_SRC python :results output
  import tensorflow as tf
  w = tf.Variable(tf.constant(5,dtype=tf.float32))
  loss = tf.square(w+1)
  train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)

  with tf.Session() as sess:
      init_op=tf.global_variables_initializer()
      sess.run(init_op)
      for i in range(50):
          sess.run(train_step)
          w_var = sess.run(w)
          loss_val = sess.run(loss)
          print("After {} steps: w is {}, loss is {}".format(i,w_var,loss_val))
          #print("After %s steps: w is %f, loss is %f." %(i,w_var, loss_val))


#+END_SRC

** train examples
*** the first train case
#+BEGIN_SRC python :results output
  import tensorflow as tf
  import numpy as np
  import matplotlib.pyplot as plt
  #生成随机数据
  x_date = np.random.rand(100)
  y_date = x_date*0.4 + 2

  #构造线性模型
  b = tf.Variable(0.)
  k = tf.Variable(0.)
  y = k*x_date + b

  #构造二次代价函数
  loss = tf.reduce_mean(tf.square(y_date-y))
  #定义梯度下降的优化器
  optimizer = tf.train.GradientDescentOptimizer(0.2)
  #定义一个最小化代价函数
  train = optimizer.minimize(loss)
  init = tf.global_variables_initializer()
  with tf.Session() as sess:
      sess.run(init)
      for steps in range(201):
          sess.run(train)
          if steps%20 == 0:
              print(steps, sess.run([k,b]))
      prediction_value = sess.run(y)
      plt.figure()
      plt.scatter(x_date, y_date)
      plt.plot(x_date,prediction_value,'r-',lw=5)
      plt.show()



#+END_SRC

#+RESULTS:
#+begin_example
0 [0.44114023, 0.87820524]
20 [0.7041128, 1.841991]
40 [0.58822596, 1.9022033]
60 [0.5164994, 1.9394703]
80 [0.47210538, 1.9625362]
100 [0.44462854, 1.9768122]
120 [0.4276222, 1.9856484]
140 [0.4170963, 1.9911172]
160 [0.41058153, 1.9945022]
180 [0.40654916, 1.9965973]
200 [0.40405348, 1.9978939]
#+end_example

*** non linear regression case
#+BEGIN_SRC python
  #----------------------------非线性回归
  import tensorflow as tf
  import numpy as np
  import matplotlib.pyplot as plt

  #构造数据
  x_date = np.linspace (-0.5,0.5,100)[:,np.newaxis] #np.newaxis 功能同None，将行变列
  noise = np.random.normal(0,0.02,x_date.shape)
  y_date = np.square(x_date)+noise

  x = tf.placeholder(tf.float32,[None,1])
  y = tf.placeholder(tf.float32,[None,1])

  #构建神经网络
  Weight_L1 = tf.Variable(tf.random_normal([1,10]))
  Biase_L1 = tf.Variable(tf.zeros([1,10]))
  Wx_plus_b_L1 = tf.matmul(x,Weight_L1)+Biase_L1
  L1 = tf.nn.tanh(Wx_plus_b_L1)

  #定义输出层
  Weight_L2 = tf.Variable(tf.random_normal([10,1]))
  Biase_L2 = tf.Variable(tf.zeros([1,1]))
  Wx_plus_b_L2 = tf.matmul(L1,Weight_L2)+ Biase_L2
  prediction = tf.nn.tanh(Wx_plus_b_L2)

  #二次代价函数
  loss = tf.reduce_mean(tf.square(y-prediction))
  train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      for _ in range(2000):
          sess.run(train_step,feed_dict={x:x_date,y:y_date})

      #训练好后，用来做预测
      prediction_value = sess.run(prediction,feed_dict={x:x_date})
      plt.figure()
      plt.scatter(x_date, y_date)
      plt.plot(x_date,prediction_value,'r-',lw=5)
      plt.show()


#+END_SRC

#+RESULTS:
: None

*** the first train with data for accuary
#+BEGIN_SRC python
  import tensorflow as tf
  from tensorflow.examples.tutorials.mnist import input_data
  #载入数据集
  mnist= input_data.read_data_sets('MNIST_data', one_hot = True)

  #设定每个批次的大小
  batch_size = 100
  #计算总共的批次
  n_batch = mnist.train.num_examples // batch_size

  #参数统计
  def variable_summries(var):
      with tf.name_scope('summaries'):
          mean = tf.reduce_mean(var)
          tf.summary.scalar('mean',mean)
          with tf.name_scope('stddev'):
              stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))
          tf.summary.scalar('stddev',stddev)
          tf.summary.scalar('max',tf.reduce_max(var))
          tf.summary.scalar('min',tf.reduce_min(var))
          tf.summary.histogram('histogram',var)

  #命名空间
  with tf.name_scope('input'):
      x = tf.placeholder(tf.float32,[None,784])
      y = tf.placeholder(tf.float32,[None,10])

  with tf.name_scope('layers'):
      with tf.name_scope('wight'):
          W = tf.Variable(tf.truncated_normal([784,10]))
          variable_summries(W)
      with tf.name_scope('biases'):
          B = tf.Variable(tf.zeros([10])+0.1)
          variable_summries(B)
      with tf.name_scope('wx_plus_b'):
          wx_plus_b=tf.matmul(x,W)+B
      with tf.name_scope('softmax'):    
          prediction = tf.nn.tanh(wx_plus_b)

  #定义二次代价函数
  #loss = tf.reduce_mean(tf.square(y-prediction))
  #重新定义对数(交叉熵)
  with tf.name_scope('loss'):
      loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))
      tf.summary.scalar('loss',loss)
  #使用梯度下降法

  with tf.name_scope('train'):
      train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
  #    train_step = tf.train.AdamOptimizer(0.05).minimize(loss)
  #初始化
  init = tf.global_variables_initializer()

  #测试准确率
  with tf.name_scope('accuracy'):
      with tf.name_scope('correct_prediction'):
          correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))
      with tf.name_scope('accuracy'):
          accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
          tf.summary.scalar('accuracy',accuracy)

  #合并summary
  merged = tf.summary.merge_all()
          #训练开始
  with tf.Session() as sess:
      writer = tf.summary.FileWriter('pics/',sess.graph)
      sess.run(init)
      for epoch in range(51):
          for batch in range(n_batch):
              batch_xs, batch_ys = mnist.train.next_batch(batch_size)
              summary,_ = sess.run([merged,train_step], feed_dict={x:batch_xs, y:batch_ys})
  #            writer.add_summary(summary,batch)
          writer.add_summary(summary,epoch)
          acc = sess.run(accuracy,feed_dict={x:mnist.test.images, y:mnist.test.labels})
          print("准确率为:　在"+str(epoch)+"回,"+str(acc))



#+END_SRC

#+RESULTS:
: None

*** a simple CNN case
#+BEGIN_SRC python  :results output
  import tensorflow as tf
  import numpy as np
  BATCH_SIZE = 8
  seed = 23455

  rng = np.random.RandomState(seed)
  X = rng.rand(32, 2)
  Y = [[int(X0+X1 < 1)] for (X0, X1) in X]
  # print ("X is :", X)
  # print ("Y is :", Y)

  x = tf.placeholder(tf.float32, shape = (None, 2))
  y_ = tf.placeholder(tf.float32, shape = (None, 1))

  w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1))
  w = tf.Variable(tf.random_normal([3,3], stddev=1, seed=1))
  w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1))

  a = tf.matmul(x,w1)
  b = tf.matmul(a,w)
  y = tf.matmul(b,w2)

  loss = tf.reduce_mean(tf.square(y-y_))
  train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)

  with tf.Session() as sess:
      init_op = tf.global_variables_initializer()
      sess.run(init_op)
      print("w1 is :", sess.run(w1))
      print("w is :", sess.run(w))
      print("w2 is :", sess.run(w2))

      steps= 30000
      for i in range(steps):
          start = (i*BATCH_SIZE) % 32
          end = start + BATCH_SIZE
          sess.run(train_step, feed_dict={x:X[start:end], y_:Y[start:end]})
          if i % 5000 == 0:
              total_loss = sess.run(loss, feed_dict={x:X, y_:Y})
              print("After %d training steps, loss on all data is %g" %(i,total_loss))
      print("\n")
      print("w1 is :", sess.run(w1))
      print("w is :", sess.run(w))
      print("w2 is :", sess.run(w2))

#+END_SRC

#+RESULTS:
#+begin_example
w1 is : [[-0.8113182   1.4845988   0.06532937]
 [-2.4427042   0.0992484   0.5912243 ]]
w is : [[-0.8113182   1.4845988   0.06532937]
 [-2.4427042   0.0992484   0.5912243 ]
 [ 0.59282297 -2.1229296  -0.72289723]]
w2 is : [[-0.8113182 ]
 [ 1.4845988 ]
 [ 0.06532937]]
After 0 training steps, loss on all data is 27.0734
After 5000 training steps, loss on all data is 0.383562
After 10000 training steps, loss on all data is 0.383562
After 15000 training steps, loss on all data is 0.383562
After 20000 training steps, loss on all data is 0.383562
After 25000 training steps, loss on all data is 0.383561


w1 is : [[-0.84913164  1.3203508   0.219825  ]
 [-1.9427874   0.7993799  -0.2589098 ]]
w is : [[-0.31725118  0.802253    0.02743915]
 [-2.471998    0.16294315  0.5947651 ]
 [ 0.56031865 -2.0743563  -0.7197714 ]]
w2 is : [[-0.45712712]
 [ 0.67528236]
 [ 0.02388799]]
#+end_example

*** CNN train
#+BEGIN_SRC python
  import tensorflow as tf
  from tensorflow.examples.tutorials.mnist import input_data

  mnist=input_data.read_data_sets("MNIST_data",one_hot = True)

  batch_size = 100
  n_batch = mnist.train.num_examples // batch_size

  #初始化权值
  def weight_variable(shape):
      return tf.Variable(tf.truncated_normal(shape,stddev =0.01))

  #初始化偏置
  def bias_variable(shape):
      return tf.Variable(tf.constant(0.1,shape= shape))

  #定义卷积层
  def conv2d(x,W):
      return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding="SAME") 

  #池化层定义
  def max_pool_2x2(x):
      return tf.nn.max_pool(x,ksize =[1,2,2,1],strides=[1,2,2,1], padding = "SAME")

  x = tf.placeholder(tf.float32,[None, 784])
  y = tf.placeholder(tf.float32,[None,10])

  x_image = tf.reshape(x,[-1,28,28,1])

  #初始化第一个卷积层的权值和偏置，输入其要求的形状
  W_convl = weight_variable([5,5,1,32]) #5x5的采样窗口大小，１通道对黑白，３通道对彩色
  b_convl = bias_variable([32])

  #现在卷积
  h_conv1 = tf.nn.relu(conv2d(x_image,W_convl) +b_convl)
  #现在池化
  h_pool1 = max_pool_2x2(h_conv1) 

  ###定义第二个卷积层
  W_convl2 = weight_variable([5,5,32,64]) #5x5的采样窗口大小，１通道对黑白，３通道对彩色
  b_convl2 = bias_variable([64])

  #现在卷积
  h_conv2 = tf.nn.relu(conv2d(h_pool1 ,W_convl2) +b_convl2)
  #现在池化
  h_pool2 = max_pool_2x2(h_conv2)

  #池化后将结果扁平化处理，以便输入网络
  h_pool2_flat = tf.reshape(h_pool2, [-1,7*7*64])

  #建立第一个神经网络的全连接层，初始化其权重和偏置
  W_fcl = weight_variable([7*7*64, 100])
  b_fcl = bias_variable([100])

  #第一层的计算
  h_fcl = tf.nn.relu(tf.matmul(h_pool2_flat, W_fcl) + b_fcl)

  #dropout
  keep_prob = tf.placeholder(tf.float32)
  h_fcl_drop = tf.nn.dropout(h_fcl, keep_prob)

  #建立第二个神经层
  W_fc2 = weight_variable([100,10])
  b_fc2 = bias_variable([10])
  prediction = tf.nn.softmax(tf.matmul(h_fcl_drop, W_fc2)+b_fc2)

  #交叉熵
  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits=prediction))
  #优化
  train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

  #结果
  correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))
  #准确率
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      for epoch in range(21):
          for batch in range(n_batch):
              batch_xs, batch_ys = mnist.train.next_batch(batch_size)
              sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys, keep_prob:0.7})
          acc= sess.run(accuracy, feed_dict={x:mnist.test.images,y:mnist.test.labels, keep_prob:1.0})
          print("在第"+str(epoch)+"轮，准确率为"+str(acc))

#+END_SRC
*** classification
#+BEGIN_SRC python

  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  import tensorflow as tf
  from tensorflow.examples.tutorials.mnist import input_data
  # number 1 to 10 data
  mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

  def add_layer(inputs, in_size, out_size, activation_function=None,):
      # add one more layer and return the output of this layer
      Weights = tf.Variable(tf.random_normal([in_size, out_size]))
      biases = tf.Variable(tf.zeros([1, out_size]) + 0.1,)
      Wx_plus_b = tf.matmul(inputs, Weights) + biases
      if activation_function is None:
          outputs = Wx_plus_b
      else:
          outputs = activation_function(Wx_plus_b,)
      return outputs

  def compute_accuracy(v_xs, v_ys):
      global prediction
      y_pre = sess.run(prediction, feed_dict={xs: v_xs})
      correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
      result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys})
      return result

  # define placeholder for inputs to network
  xs = tf.placeholder(tf.float32, [None, 784]) # 28x28
  ys = tf.placeholder(tf.float32, [None, 10])

  # add output layer
  prediction = add_layer(xs, 784, 10,  activation_function=tf.nn.softmax)

  # the error between prediction and real data
  cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),
                                                reduction_indices=[1]))       # loss
  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

  sess = tf.Session()
  # important step
  # tf.initialize_all_variables() no long valid from
  # 2017-03-02 if using tensorflow >= 0.12
  if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:
      init = tf.initialize_all_variables()
  else:
      init = tf.global_variables_initializer()
  sess.run(init)

  for i in range(1000):
      batch_xs, batch_ys = mnist.train.next_batch(100)
      sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys})
      if i % 50 == 0:
          print(compute_accuracy(
              mnist.test.images, mnist.test.labels))


#+END_SRC
*** optimizer varialbe (with error)
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  import tensorflow as tf
  import numpy as np


  def add_layer(inputs, in_size, out_size, n_layer, activation_function=None):
      # add one more layer and return the output of this layer
      layer_name = 'layer%s' % n_layer
      with tf.name_scope(layer_name):
          with tf.name_scope('weights'):
              Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')
              tf.summary.histogram(layer_name + '/weights', Weights)
          with tf.name_scope('biases'):
              biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name='b')
              tf.summary.histogram(layer_name + '/biases', biases)
          with tf.name_scope('Wx_plus_b'):
              Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)
          if activation_function is None:
              outputs = Wx_plus_b
          else:
              outputs = activation_function(Wx_plus_b, )
          tf.summary.histogram(layer_name + '/outputs', outputs)
      return outputs


  # Make up some real data
  x_data = np.linspace(-1, 1, 300)[:, np.newaxis]
  noise = np.random.normal(0, 0.05, x_data.shape)
  y_data = np.square(x_data) - 0.5 + noise

  # define placeholder for inputs to network
  with tf.name_scope('inputs'):
      xs = tf.placeholder(tf.float32, [None, 1], name='x_input')
      ys = tf.placeholder(tf.float32, [None, 1], name='y_input')

  # add hidden layer
  l1 = add_layer(xs, 1, 10, n_layer=1, activation_function=tf.nn.relu)
  # add output layer
  prediction = add_layer(l1, 10, 1, n_layer=2, activation_function=None)

  # the error between prediciton and real data
  with tf.name_scope('loss'):
      loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),
                                          reduction_indices=[1]))
      tf.summary.scalar('loss', loss)

  with tf.name_scope('train'):
      train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

  sess = tf.Session()
  merged = tf.summary.merge_all()

  writer = tf.summary.FileWriter("logs/", sess.graph)

  init = tf.global_variables_initializer()
  sess.run(init)

  for i in range(1000):
      sess.run(train_step, feed_dict={xs: x_data, ys: y_data})
      if i % 50 == 0:
          result = sess.run(merged,
                            feed_dict={xs: x_data, ys: y_data})
          writer.add_summary(result, i)

  # direct to the local dir and run this in terminal:
  # $ tensorboard --logdir logs


#+END_SRC
*** dropout
#+BEGIN_SRC python
import tensorflow as tf
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data
#载入数据集
mnist= input_data.read_data_sets('MNIST_data', one_hot = True)

#设定每个批次的大小
batch_size = 100
#计算总共的批次
n_batch = mnist.train.num_examples//batch_size

x = tf.placeholder(tf.float32,[None,784])
y = tf.placeholder(tf.float32,[None,10])
keep_prob = tf.placeholder(tf.float32)
#构建神经网络
W = tf.Variable(tf.truncated_normal([784,2000], stddev = 0.1))
B = tf.Variable(tf.zeros([2000])+0.1)
p1 = tf.nn.softmax(tf.matmul(x,W)+B)
p1_dropout = tf.nn.dropout(p1,keep_prob)

W1 = tf.Variable(tf.truncated_normal([2000,2000]))
B1 = tf.Variable(tf.zeros([2000])+0.1)
p2 = tf.nn.softmax(tf.matmul(p1_dropout,W1)+B1)
p2_dropout = tf.nn.dropout(p2,keep_prob)

W2 = tf.Variable(tf.truncated_normal([2000,10]))
B2 = tf.Variable(tf.zeros([10])+0.1)
prediction = tf.nn.softmax(tf.matmul(p2_dropout,W2)+B2)

#定义二次代价函数
#loss = tf.reduce_mean(tf.square(y-prediction))
#重新定义对数(交叉熵)
loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))
#使用梯度下降法
train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)

#初始化
init = tf.global_variables_initializer()

#测试准确率
correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))

#训练开始
with tf.Session() as sess:
    sess.run(init)
    for epoch in range(21):
        for batch in range(n_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys,keep_prob:1.0})

        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images, y:mnist.test.labels, keep_prob:1.0})
        train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images, y:mnist.train.labels, keep_prob:1.0})
        print("准确率为:　在"+str(epoch)+"回,"+str(test_acc)+", 但是在训练集中为"+str(train_acc))


#+END_SRC
*** number identification
#+BEGIN_SRC python
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
#载入数据集
mnist= input_data.read_data_sets('MNIST_data', one_hot = True)

#设定每个批次的大小
batch_size = 100
#计算总共的批次
n_batch = mnist.train.num_examples // batch_size

x = tf.placeholder(tf.float32,[None,784])
y = tf.placeholder(tf.float32,[None,10])
keep_prob = tf.placeholder(tf.float32)
lr= tf.Variable(0.001,dtype=tf.float32)
#构建神经网络

# W = tf.Variable(tf.truncated_normal([784,10]))
# B = tf.Variable(tf.zeros([10])+0.1)
# prediction = tf.nn.softmax(tf.matmul(x,W)+B)

# W = tf.Variable(tf.zeros([784,10]))
# B = tf.Variable(tf.zeros([10]))
# prediction = tf.nn.softmax(tf.matmul(x,W)+B)
 
#构建神经网络
W = tf.Variable(tf.truncated_normal([784,500],stddev =0.1))
B = tf.Variable(tf.zeros([500])+0.1)
p1 = tf.nn.tanh(tf.matmul(x,W)+B)
p1_dropout = tf.nn.dropout(p1,keep_prob)

W1 = tf.Variable(tf.truncated_normal([500,200],stddev=0.1))
B1 = tf.Variable(tf.zeros([200])+0.1)
p2 = tf.nn.tanh(tf.matmul(p1_dropout,W1)+B1)
p2_dropout = tf.nn.dropout(p2,keep_prob)

W2 = tf.Variable(tf.truncated_normal([200,10],stddev = 0.1))
B2 = tf.Variable(tf.zeros([10])+0.1)
prediction = tf.nn.softmax(tf.matmul(p2_dropout,W2)+B2)



#定义二次代价函数
#loss = tf.reduce_mean(tf.square(y-prediction))
#重新定义对数(交叉熵)
loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))
#使用梯度下降法
#train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
train_step = tf.train.AdamOptimizer(lr).minimize(loss)
#初始化
init = tf.global_variables_initializer()

#测试准确率
correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))

# #训练开始
# with tf.Session() as sess:
#     sess.run(init)
#     for epoch in range(20):
#         for batch in range(n_batch):
#             batch_xs, batch_ys = mnist.train.next_batch(batch_size)
#             sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys})
#         acc = sess.run(accuracy,feed_dict={x:mnist.test.images, y:mnist.test.labels})
#         print("准确率为:　在"+str(epoch)+"回,"+str(acc))


#训练开始
with tf.Session() as sess:
    sess.run(init)
    for epoch in range(41):
        sess.run(tf.assign(lr,0.001*(0.95**epoch)))
        for batch in range(n_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys,keep_prob:1.0})
        learing_rate = sess.run(lr)
        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,
y:mnist.test.labels, keep_prob:1.0})
        train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images,
y:mnist.train.labels, keep_prob:1.0})
        print("准确率为:　在"+str(epoch)+"回,"+str(test_acc)+",但是在训练集中为"+str(train_acc)+"同时学习率为"+str(learing_rate))


#+END_SRC
*** with L　regularizer 
#+BEGIN_SRC python

  import tensorflow as tf
  import numpy as np
  import matplotlib.pyplot as plt
  BATCH_SIZE = 30
  seed = 2
  rdm = np.random.RandomState(seed)
  X = rdm.randn(300,2)
  Y_ = [int(x0*x0 + x1*x1 < 2) for (x0,x1) in X]
  Y_c = [['red' if y else 'blue'] for y in Y_]
  X = np.vstack(X).reshape(-1,2)
  Y_ = np.vstack(Y_).reshape(-1,1)
  print(X)
  print(Y_)
  print(Y_c)
  plt.scatter(X[:,0],X[:,1], c = np.squeeze(Y_c))
  plt.show()

  def get_weight(shape, regularizer):
      w = tf.Variable(tf.random_normal(shape), dtype = tf.float32)
      tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))
      return w
  def get_bias(shape):
      b = tf.Variable(tf.constant(0.01, shape=shape))
      return b
  x = tf.placeholder(tf.float32, shape=(None, 2))
  y_ = tf.placeholder(tf.float32, shape = (None , 1))
  w1 = get_weight([2,11], 0.01)
  b1 = get_bias([11])
  y1 = tf.nn.relu(tf.matmul(x,w1)+b1)
  w2 = get_weight([11,1],0.01)
  b2 = get_bias([1])
  y = tf.matmul(y1,w2)+b2

  loss_mse = tf.reduce_mean(tf.square(y-y_))
  loss_total = loss_mse + tf.add_n(tf.get_collection('losses'))

  #########with Regularizer#########
  train_step_l = tf.train.AdamOptimizer(0.0001).minimize(loss_total)
  with tf.Session() as sess:
      init_op = tf.global_variables_initializer()
      sess.run(init_op)
      STEPS = 40000
      for i in range(STEPS):
          start = (i*BATCH_SIZE) % 300
          end = start + BATCH_SIZE
          sess.run(train_step_l, feed_dict = {x:X[start:end], y_:Y_[start:end]})
          if i % 2000 == 0:
              loss_total_v = sess.run(loss_total, feed_dict={x:X,y_:Y_})
              print('After %d steps, loss is: %f' %(i, loss_total_v))
      xx, yy = np.mgrid[-3:3:0.01, -3:3:0.01]
      grid = np.c_[xx.ravel(), yy.ravel()]
      probs = sess.run(y, feed_dict={x:grid})
      probs = probs.reshape(xx.shape)
      print ('w1 is \n:', sess.run(w1))
      print ('b1 is \n:', sess.run(b1))
      print ('w2 is \n:', sess.run(w2))
      print ('b2 is \n:', sess.run(b2))
  plt.scatter(X[:,0], X[:,1], c = np.squeeze(Y_c))
  plt.contour(xx, yy, probs, levels = [.5])
  plt.show()


  print('loss_mse_v * loss_total_v is :', loss_mse_v*loss_total_v)


#+END_SRC
*** without L regularizer
#+BEGIN_SRC python
  import tensorflow as tf
  import numpy as np
  import matplotlib.pyplot as plt
  BATCH_SIZE = 30
  seed = 2
  rdm = np.random.RandomState(seed)
  X = rdm.randn(300,2)
  Y_ = [int(x0*x0 + x1*x1 < 2) for (x0,x1) in X]
  Y_c = [['red' if y else 'blue'] for y in Y_]
  X = np.vstack(X).reshape(-1,2)
  Y_ = np.vstack(Y_).reshape(-1,1)
  print(X)
  print(Y_)
  print(Y_c)
  plt.scatter(X[:,0],X[:,1], c = np.squeeze(Y_c))
  plt.show()

  def get_weight(shape, regularizer):
      w = tf.Variable(tf.random_normal(shape), dtype = tf.float32)
      tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))
      return w
  def get_bias(shape):
      b = tf.Variable(tf.constant(0.01, shape=shape))
      return b
  x = tf.placeholder(tf.float32, shape=(None, 2))
  y_ = tf.placeholder(tf.float32, shape = (None , 1))
  w1 = get_weight([2,11], 0.01)
  b1 = get_bias([11])
  y1 = tf.nn.relu(tf.matmul(x,w1)+b1)
  w2 = get_weight([11,1],0.01)
  b2 = get_bias([1])
  y = tf.matmul(y1,w2)+b2

  loss_mse = tf.reduce_mean(tf.square(y-y_))
  loss_total = loss_mse + tf.add_n(tf.get_collection('losses'))
  #########without Regularizer#########
  train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_mse)
  with tf.Session() as sess:
      init_op = tf.global_variables_initializer()
      sess.run(init_op)
      STEPS = 40000
      for i in range(STEPS):
          start = (i*BATCH_SIZE) % 300
          end = start + BATCH_SIZE
          sess.run(train_step, feed_dict = {x:X[start:end], y_:Y_[start:end]})
          if i % 2000 == 0:
              loss_mse_v = sess.run(loss_mse, feed_dict={x:X,y_:Y_})
              print('After %d steps, loss is: %f' %(i, loss_mse_v))
      xx, yy = np.mgrid[-3:3:0.01, -3:3:0.01]
      grid = np.c_[xx.ravel(), yy.ravel()]
      probs = sess.run(y, feed_dict={x:grid})
      probs = probs.reshape(xx.shape)
      print ('w1 is \n:', sess.run(w1))
      print ('b1 is \n:', sess.run(b1))
      print ('w2 is \n:', sess.run(w2))
      print ('b2 is \n:', sess.run(b2))
  plt.scatter(X[:,0], X[:,1], c = np.squeeze(Y_c))
  plt.contour(xx, yy, probs, levels = [.5])
  plt.show()


#+END_SRC
*** exponen decay with L regularizer(this has error)
#+BEGIN_SRC python
  import numpy as np
  import matplotlib.pyplot as plt
  ######### import opt4_8_generateds
  ######### import opt4_8_forward


  try:
      import opt4_8_generateds
      import opt4_8_forward
  except:
      import pip 
      pip.main(['install','opt4_8_forward'])
      pip.main(['install','opt4_8_generateds'])
      import opt4_8_generateds
      import opt4_8_forward



  STEPS = 40000
  BATCH_SIZE = 30
  LEARNING_RATE_BASE = 0.001
  LEARNING_RATE_DECAY = 0.999
  REGULARIZER = 0.01

  def backward():
      x = tf.placeholder(tf.float32, shape = (None,2))
      y_ = tf.placeholder(tf.float32, shape = (None,1))

      x , Y_, Y_c = opt4_8_generateds.generateds()
      y = opt4_8_forward.forward(x,REGULARIZER)

      global_steps = tf.Variable(0, trainable= False)
      learning_rate = tf.train.exponential_decay(
          LEARNING_RATE_BASE,
          global_steps,
          300/BATCH_SIZE,
          LEARNING_RATE_DECAY,
          staircase = True)
      loss_mse = tf.reduce_mean(tf.square(y-y_))
      loss_total = loss_mse + tf.add_n(tf.get_collection('losses'))

      train_step = tf.train.AdamOptimizer(learning_rate).minimizer(loss_total)

      with tf.Session() as sess:
          init_op = tf.global_variables_initializer()
          sess.run(init_op)
          for i in range(STEPS):
              start = (i*BATCH_SIZE) % 300
              end = start + BATCH_SIZE
              sess.run(train_step, feed_dict = {x:X[start:end], y_:Y_[start:end]})
              if i % 2000 == 0:
                  loss_v = sess.run(loss_total, feed_dict={x:X,y_:Y_})
                  print('After %d steps, loss is: %f' %(i, loss_v))
          xx, yy = np.mgrid[-3:3:0.01, -3:3:0.01]
          grid = np.c_[xx.ravel(), yy.ravel()]
          probs = sess.run(y, feed_dict={x:grid})
          probs = probs.reshape(xx.shape)

      plt.scatter(X[:,0], X[:,1], c = np.squeeze(Y_c))
      plt.contour(xx, yy, probs, levels = [.5])
      plt.show()

  if __name__ == '__main__':
      backward()


#+END_SRC
*** read date for variables
#+BEGIN_SRC python :results output
  from tensorflow.examples.tutorials.mnist import input_data
  mnist = input_data.read_data_sets('./data', one_hot = True)

  # print(mnist.train.labels[0])
  # print(mnist.train.images[0])

  BATCH_SIZE = 200
  xs, ys = mnist.train.next_batch(BATCH_SIZE)
  print("xs shape :", xs.shape)
  print("ys shape :", ys.shape)


#+END_SRC

#+RESULTS:
: Extracting ./data/train-images-idx3-ubyte.gz
: Extracting ./data/train-labels-idx1-ubyte.gz
: Extracting ./data/t10k-images-idx3-ubyte.gz
: Extracting ./data/t10k-labels-idx1-ubyte.gz
: xs shape : (200, 784)
: ys shape : (200, 10)




#+startup: content
* keras
*** classification
#+BEGIN_SRC python


  # please note, all tutorial code are running under python3.5.
  # If you use the version like python2.7, please modify the code accordingly

  # 5 - Classifier example

  import numpy as np
  np.random.seed(1337)  # for reproducibility
  from keras.datasets import mnist
  from keras.utils import np_utils
  from keras.models import Sequential
  from keras.layers import Dense, Activation
  from keras.optimizers import RMSprop

  # download the mnist to the path '~/.keras/datasets/' if it is the first time to be called
  # X shape (60,000 28x28), y shape (10,000, )
  (X_train, y_train), (X_test, y_test) = mnist.load_data()

  # data pre-processing
  X_train = X_train.reshape(X_train.shape[0], -1) / 255.   # normalize
  X_test = X_test.reshape(X_test.shape[0], -1) / 255.      # normalize
  y_train = np_utils.to_categorical(y_train, num_classes=10)
  y_test = np_utils.to_categorical(y_test, num_classes=10)

  # Another way to build your neural net
  model = Sequential([
      Dense(32, input_dim=784),
      Activation('relu'),
      Dense(10),
      Activation('softmax'),
  ])

  # Another way to define your optimizer
  rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)

  # We add metrics to get more results you want to see
  model.compile(optimizer=rmsprop,
                loss='categorical_crossentropy',
                metrics=['accuracy'])

  print('Training ------------')
  # Another way to train the model
  model.fit(X_train, y_train, epochs=2, batch_size=32)

  print('\nTesting ------------')
  # Evaluate the model with the metrics we defined earlier
  loss, accuracy = model.evaluate(X_test, y_test)

  print('test loss: ', loss)
  print('test accuracy: ', accuracy)

#+END_SRC
*** regression
#+BEGIN_SRC python
  # # please note, all tutorial code are running under python3.5.
  # # If you use the version like python2.7, please modify the code accordingly

  # # 4 - Regressor example

  import numpy as np
  np.random.seed(1337)  # for reproducibility
  from keras.models import Sequential
  from keras.layers import Dense
  import matplotlib.pyplot as plt

  # create some data
  X = np.linspace(-1, 1, 200)
  np.random.shuffle(X)    # randomize the data
  Y = 0.5 * X + 2 + np.random.normal(0, 0.05, (200 ))
  # plot data
  plt.scatter(X, Y)
  plt.show()

  X_train, Y_train = X[:160], Y[:160]     # first 160 data points
  X_test, Y_test = X[160:], Y[160:]       # last 40 data points

  # build a neural network from the 1st layer to the last layer
  model = Sequential()

  model.add(Dense(units=1, input_dim=1)) 

  # choose loss function and optimizing method
  model.compile(loss='mse', optimizer='sgd')

  # training
  print('Training -----------')
  for step in range(301):
      cost = model.train_on_batch(X_train, Y_train)
      if step % 100 == 0:
          print('train cost: ', cost)

  # test
  print('\nTesting ------------')
  cost = model.evaluate(X_test, Y_test, batch_size=40)
  print('test cost:', cost)
  W, b = model.layers[0].get_weights()
  print('Weights=', W, '\nbiases=', b)

  # plotting the prediction
  Y_pred = model.predict(X_test)
  plt.scatter(X_test, Y_test)
  plt.plot(X_test, Y_pred)
  plt.show()
#+END_SRC
* sklearn
*** decisiontree
#+BEGIN_SRC python
  from sklearn.feature_extraction import DictVectorizer
  import csv
  from sklearn import preprocessing
  from sklearn import tree
  from sklearn.externals.six import StringIO
  import numpy as np
  import pandas as pd
  from pylab import *

  allElectronicsData = open("computer.csv")
  reader = csv.reader(allElectronicsData)
  headers = next(reader)
  print(headers)
  print(reader)

  featureList = []
  labelList = []
  for row in reader:
      labelList.append(row[len(row)-1])
      rowDict = {}
      for i in range(1,len(row)-1):
          rowDict[headers[i]] = row[i]
      featureList.append(rowDict)
  print(featureList)

  vec = DictVectorizer()
  dummyX = vec.fit_transform(featureList).toarray()
  print("dummyX: " + str(dummyX))
  print(vec.get_feature_names())

  print("labeList" + str(labelList))
  lb = preprocessing.LabelBinarizer()
  dummyY = lb.fit_transform(labelList)
  print("dummyY:" + str(dummyY))

  clf = tree.DecisionTreeClassifier(criterion= 'entropy')
  clf = clf.fit(dummyX,dummyY)
  print("clf :" + str(clf))

  # save as dot
  with open("output.dot","w") as f:
      f = tree.export_graphviz(clf, feature_names= vec.get_feature_names(), out_file = f)
    
  #in terminnal gives the flowwing comands

  #    dot -Tpdf -O output.dot
  #    xdg-open output.pdf

#+END_SRC
*** cross validation 1
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn.datasets import load_iris
  from sklearn.cross_validation import train_test_split
  from sklearn.neighbors import KNeighborsClassifier

  iris = load_iris()
  X = iris.data
  y = iris.target

  # test train split #
  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)
  knn = KNeighborsClassifier(n_neighbors=5)
  knn.fit(X_train, y_train)
  y_pred = knn.predict(X_test)
  print(knn.score(X_test, y_test))

  # this is cross_val_score #
  from sklearn.cross_validation import cross_val_score
  knn = KNeighborsClassifier(n_neighbors=5)
  scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')
  print(scores)

  # this is how to use cross_val_score to choose model and configs #
  from sklearn.cross_validation import cross_val_score
  import matplotlib.pyplot as plt
  k_range = range(1, 31)
  k_scores = []
  for k in k_range:
      knn = KNeighborsClassifier(n_neighbors=k)
      loss = -cross_val_score(knn, X, y, cv=10, scoring='mean_squared_error') # for regression
  ##    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy') # for classification
      k_scores.append(loss.mean())

  plt.plot(k_range, k_scores)
  plt.xlabel('Value of K for KNN')
  plt.ylabel('Cross-Validated Accuracy')
  plt.show()


#+END_SRC

*** cross validation 2
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn.learning_curve import  learning_curve
  from sklearn.datasets import load_digits
  from sklearn.svm import SVC
  import matplotlib.pyplot as plt
  import numpy as np

  digits = load_digits()
  X = digits.data
  y = digits.target
  train_sizes, train_loss, test_loss= learning_curve(
          SVC(gamma=0.01), X, y, cv=10, scoring='mean_squared_error',
          train_sizes=[0.1, 0.25, 0.5, 0.75, 1])
  train_loss_mean = -np.mean(train_loss, axis=1)
  test_loss_mean = -np.mean(test_loss, axis=1)

  plt.plot(train_sizes, train_loss_mean, 'o-', color="r",
               label="Training")
  plt.plot(train_sizes, test_loss_mean, 'o-', color="g",
               label="Cross-validation")

  plt.xlabel("Training examples")
  plt.ylabel("Loss")
  plt.legend(loc="best")
  plt.show()


#+END_SRC

*** corss validation 3
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn.learning_curve import  validation_curve
  from sklearn.datasets import load_digits
  from sklearn.svm import SVC
  import matplotlib.pyplot as plt
  import numpy as np

  digits = load_digits()
  X = digits.data
  y = digits.target
  param_range = np.logspace(-6, -2.3, 5)
  train_loss, test_loss = validation_curve(
          SVC(), X, y, param_name='gamma', param_range=param_range, cv=10,
          scoring='mean_squared_error')
  train_loss_mean = -np.mean(train_loss, axis=1)
  test_loss_mean = -np.mean(test_loss, axis=1)

  plt.plot(param_range, train_loss_mean, 'o-', color="r",
               label="Training")
  plt.plot(param_range, test_loss_mean, 'o-', color="g",
               label="Cross-validation")

  plt.xlabel("gamma")
  plt.ylabel("Loss")
  plt.legend(loc="best")
  plt.show()


#+END_SRC
*** data application
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn import datasets
  from sklearn.linear_model import LinearRegression
  import matplotlib.pyplot as plt

  loaded_data = datasets.load_boston()
  data_X = loaded_data.data
  data_y = loaded_data.target

  model = LinearRegression()
  model.fit(data_X, data_y)

  print(model.predict(data_X[:4, :]))
  print(data_y[:4])

  X, y = datasets.make_regression(n_samples=100, n_features=1, n_targets=1, noise=1)
  plt.scatter(X, y)
  plt.show()


#+END_SRC
*** eigenschaft function
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn import datasets
  from sklearn.linear_model import LinearRegression

  loaded_data = datasets.load_boston()
  data_X = loaded_data.data
  data_y = loaded_data.target

  model = LinearRegression()
  model.fit(data_X, data_y)

  print(model.predict(data_X[:4, :]))
  print(model.coef_)
  print(model.intercept_)
  print(model.get_params())
  print(model.score(data_X, data_y)) # R^2 coefficient of determination


#+END_SRC
*** iris
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn import datasets
  from sklearn.model_selection import train_test_split
  from sklearn.neighbors import KNeighborsClassifier

  iris = datasets.load_iris()
  iris_X = iris.data
  iris_y = iris.target

  ##print(iris_X[:2, :])
  ##print(iris_y)

  X_train, X_test, y_train, y_test = train_test_split(
      iris_X, iris_y, test_size=0.3)

  ##print(y_train)

  knn = KNeighborsClassifier()
  knn.fit(X_train, y_train)
  print(knn.predict(X_test))
  print(y_test)


#+END_SRC
*** normnalization
#+BEGIN_SRC python
# View more python learning tutorial on my Youtube and Youku channel!!!

# Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
# Youku video tutorial: http://i.youku.com/pythontutorial

"""
Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
"""
from __future__ import print_function
from sklearn import preprocessing
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets.samples_generator import make_classification
from sklearn.svm import SVC
import matplotlib.pyplot as plt

a = np.array([[10, 2.7, 3.6],
                     [-100, 5, -2],
                     [120, 20, 40]], dtype=np.float64)
print(a)
print(preprocessing.scale(a))

X, y = make_classification(n_samples=300, n_features=2 , n_redundant=0, n_informative=2, random_state=22, n_clusters_per_class=1, scale=100)
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.show()
X = preprocessing.scale(X)    # normalization step
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)
clf = SVC()
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))


#+END_SRC
*** save load
#+BEGIN_SRC python
  # View more python tutorials on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn import svm
  from sklearn import datasets

  clf = svm.SVC()
  iris = datasets.load_iris()
  X, y = iris.data, iris.target
  clf.fit(X, y)

  # # method 1: pickle
  # import pickle
  # # save
  # with open('save/clf.pickle', 'wb') as f:
  #     pickle.dump(clf, f)
  # # restore
  # with open('save/clf.pickle', 'rb') as f:
  #    clf2 = pickle.load(f)
  #    print(clf2.predict(X[0:1]))

  # method 2: joblib
  from sklearn.externals import joblib
  # Save
  joblib.dump(clf, 'save/clf.pkl')
  # restore
  clf3 = joblib.load('save/clf.pkl')
  print(clf3.predict(X[0:1]))


#+END_SRC
* theano 
*** basic
#+BEGIN_SRC python :results output
  # View more python tutorials on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  # 4 - basic usage
  #   from __future__ import print_function
  import numpy as np
  import theano.tensor as T
  from theano import function

  # basic
  x = T.dscalar('x')
  y = T.dscalar('y')
  z = x+y     # define the actual function in here
  f = function([x, y], z)  # the inputs are in [], and the output in the "z"

  print(f(2,3))  # only give the inputs "x and y" for this function, then it will calculate the output "z"

  # # to pretty-print the function
  from theano import pp
  print(pp(z))

  # # how about matrix
  x = T.dmatrix('x')
  y = T.dmatrix('y')
  z = x + y
  f = function([x, y], z)
  print(f(np.arange(12).reshape((3,4)), 10*np.ones((3,4))))


#+END_SRC

#+RESULTS:
: 5.0
: (x + y)
: [[10. 11. 12. 13.]
:  [14. 15. 16. 17.]
:  [18. 19. 20. 21.]]


