#+TITLE: tf2
#+OPTIONS: num:nil
#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup
#+STARTUP: content

tensorflow2手册的相关代码
* Check GPU
#+BEGIN_SRC python :results output :session checkoutgpu 
import tensorflow as tf
if (tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)==True):
    print("using gpu")
#+END_SRC

* Tensorflow foundation
** 数值类型 
|--------+------+---------+-----------------------+----------------------|
| type   | diam | shape   | example               | function             |
| Scalar |    0 | []      | []                    | acc                  |
| Vector |    1 | [n,]    | [1.0]                 | bias (b)             |
| Matrix |    2 | [n, m]  | [[1,2],[3,4]]         | weight (W)           |
| Tensor |   >2 | [n,m,p] | [[[1],[1]],[[2],[2]]] | input ( [n, h, w, 3] |
|--------+------+---------+-----------------------+----------------------|

#+begin_src python

a = tf.constant(example)
tf.constant() 功能类似于tf.convert_to_tensor()
b = tf.constant('Hello Deep learning')
c = tf.constant(True)
dtype=tf.int16, int32, int64,tf.float16, tf.float32, tf.float64
tf.cast(a, tf.float64)
tf.Variable(a)可以添加a的可训练属性
tf.zeros([n,m])  tf.zeros_like(a)  == tf.zeros(a.shape)
tf.ones([n,m])   tf.ones_like(a)   == tf.ones(a.shape)
tf.fill([2,2],10)
tf.random.normal([2,2]) == tf.random.normal([2,2], mean=0, stddev=1)
tf.random.uniform(shape, minval=0,maxval=10,dtype=tf.float32)
tf.range(10) == tf.range(0,10, delta = 2)

#+end_src
** Reference(,) and Segment(:)
#+begin_src python
x = tf.random.normal([4,32,32,3])
x[2][1][0][1]  == x[2,1,0,1]

[start:end:step] for each dimension.
x[1:3:2, 1:4:2, 2:4:2, 1:3:2]

#+end_src

** 改变视图
x = tf.random.normal([4,32,32,3]) 的数据是整体贮存的，可以合法的
reshape. 从后往前合并，拆分。

*** 增加维度
增加前置维度
tf.expand_dims(x, axis=0)
增加后置维度
tf.expand_dims(x, axis=-1)
也可以是其他值，表示在相应的位置增加一个维度

*** 删除维度
删除前置
tf.squeeze(x, axis=0)
后置和其他位置一样，删除dia=1的维度

** 交换维度
这会改变数据的贮存顺序
x = tf.random.normal([2,32,32,3])
x = tf.transpose(x, perm=[0,3,1,2])
以前的维度下表变换为perm
 
** Broadcasting
** tile
x = tf.random.normal([4,32,32,3])
y = tf.tile(x,[2,3,3,1]) 对应维度各复制成原来的2，3，3，1倍。

** Concatenate & Stack & Split & unstack
tf.concat([a,b],axis=0) 除了axis=0外的所有维度都应该一样
tf.stack([a,b],axis=0) a,b的所有维度应该都一样，插入的位置和
expand_dims() 遵守相同规则
tf.split(x, num_or_size_splits=10, axis=0) 拆分的维度不消失
tf.unstack(x, axis=0) 拆为步长为1           拆分的维度消失

** Statistik

L1 Norm  $||x_{1}|| = \sum_{i}|x_{i}|$
tf.norm(x, ord=1)
L2 Norm  $||x_{2}|| = \sqrt{\sum_{i}|x_{i}|^2 }$
tf.norm(x,ord=2)

tf.reduce_max(x, axis=0)
tf.reduce_min()
tf.reduce_mean()
tf.reduce_sum()
不指明axis则是对全局求解
tf.argmax(x, axis) 
tf.argmin(x,axis) axis 轴的极值坐标

#+begin_src python

out = tf.random.normal([100,10])
out = tf.nn.softmax(out, axis=1)
pred = tf.argmax(out, axis=1)
y = tf.random.uniform([100],dtype=tf.int64,maxval=10)
out = tf.equal(pred, y)
out = tf.cast(out, dtype=tf.float32)
cor = tf.reduce_sum(out)
#+end_src

** Padding
#+begin_src 
x = tf.pad(x,[[0,2]]) []内是padding 填充的方案，每个[]表示一个维度
#+end_src







** advance manipulation
*** tf.minimum
tf.minimum(x, a) 最小a
tf.maximum(x, b) 最大b
tf.minimum(tf.maximum(x,2),7) == tf.clip_by_value(x,2,7)

*** tf.gather
tf.gather(x, [0,2,4,5,7],axis =1) 抽取在axis=1上的[0,2,4,5,7]坐标的组
成数据，并可以重新定义组成数据的顺序
*** tf.gather_nd
tf.gather_nd(x,[[1,1],[2,2],[3,3]]) 
后面根的[]说明了所有成员要操作的维度，
第一成员的第一维坐标为1，第二维坐标为1..
所有成员组成List
*** tf.boolean_mask
tf.boolean_mask(x, mask=[True, False, True, False],axis =0)
在axis=0的轴上，只有mask成员是True才会被选中，mask 长度等于axis=0 轴的
长度。
*** tf.where
#+begin_src python
a = tf.ones([3,3])
b = tf.zeros([3,3])
cond = tf.constant([True,False,False],[False, True, True],[False, False, False])
c = tf.where(cond, a,b)
#+end_src
tf.where(cond) 返回所有值为True元素的下标
#+begin_src  python
x = tf.random.normal([3.3])
mask = x>0
ind = tf.where(mask)
a = tf.gather_nd(x,ind)
#+end_src
*** tf.scatter_nd
在一个长度为8的空白向量（全为0）里，将updates按照indices的位置写入
#+begin_src python
indices = tf.constant([[4],[3],[2],[1]])
updates = tf.constant([3,1,0,2])
tf.scatter_nd(indices, updates, [8])

#+end_src

*** tf.meshgrid
#+begin_src python
x = tf.linspace(-8., 8, 100)   #-8后面的. 不能省略
y = tf.linspace(-8., 8, 100)
x.shape = 100
x, y = tf.meshgrid(x,y)
x.shape = [100,100]
ax.contour3D(x.numpy(), y.numpy(), z.numpy(), 50)

#+end_src


* activation function
** no activation
输出为实数空间或某个区间， 连续变化。直接有输出值和真实值比较
** Sigmoid
$$ Sigmoid(x) = \frac{1}{1+e^{-x}}$$

导数：$$ \sigma'(x) = \sigma(x)(1-\sigma(x))$$

** ReLU
$$ Relu(x) = 
\begin{cases}
x&  x >=0 \\
0&  x < 0
\end{cases}$$

$$ Relu(x) = max(0,x)$$

** LeakyReLU

$$ LeakyReLU(x) = 
\begin{cases}
x& x >=0 \\
px& x <0, 0<p<<1
\end{cases}$$

** Tanh
$$ tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$
$$ tanh(x) = 2.sigmoid(2x)-1$$
导数：$$\tanh'(x) = 1- \tanh^{2}(x)$$

** MSE
$$ \mathcal{L} = MSE(y, o) =
\frac{1}{d_{out}}\sum_{i=1}^{d_{out}}(y_{i}-o^{i})^{2}$$
导数 ：$$ \frac{\partial \mathcal{L}}{\partial o_{i}}= (o_{i}-y_{i})$$

** inf entropy 
$$ H(p) = -\sum_{i}P(i)\log_{2}^{P_{i}}$$
** softmax
$$ p_{z_{i}} = \frac{e^{z_{i}}}{\sum_{j}e^{z_{j}}}$$
所有种类的概率之和为1
导数:
$$ \frac{ \partial p_{z_{i}}}{\partial z_{j}} = 
\begin{cases}
p_{i}(1-p_{j}) &  if i =j \\
-p_{i}p_{j}    & if \neq j
\end{cases}$$

** cross entropy
在计算交叉熵时， 一般是和 softmax 函数一起使用的
$$ H(p||q) = -\sum_{i} p(i) \log_{2}^{q_{i}}$$
$$H(p||q) = H(p) + D_{KL}(p||q)$$

for One-hot coding
$$ H(p||q) = D_{KL}(p||q) = \sum_{i}y_{i}log(\frac{y_{j}}{o_{j}}) = 
1 \cdot \log\frac{1}{o_{i}} + \sum_{j!=i}0 \cdot \log \frac{0}{o_{j}}
= -\log o_{i}$$
o_i 为1 时，预测正确，交叉熵为0。
导数：
$$ \mathcal{L} = -\sum_{k}y_{k}\log(p_{k})$$
$$\begin{aligned}
 \frac{\partial \mathcal{L}}{\partial z_{i}} & =
-\sum_{k} y_{k} \frac{\partial \log(p_{k})}{\partial z_{i}} \\
&= -\sum_{k} y_{k} \frac{\partial \log(p_{k})}{\partial p_{k}} \cdot
\frac{\partial p_{k}}{\partial z_{i}} \\
&= -\sum_{k} y_{k} \frac{1}{\partial p_{k}} \cdot
\frac{\partial p_{k}}{\partial z_{i}} \\
\end{aligned}
$$

用上面 softmax 的导数结果，分为k=i 和k!=i两种情况
$$ \frac{\partial \mathcal{L}}{z_{i}}=p_{i}-y_{i}$$

* Backpropagation

** 感知机
对x的向后更正，$x^{'}= x - \eta \cdot \frac{dy}{dx}$.
对于感知机的传递功能，$y = w^{T}x + b$.
由于感知机没有激活函数，所以直接对$$\mathcal{L} = \frac{1}{n}
\sum^{n}_{i=1}(w\cdot x^{i} +b -y^{i})^{2}$$.
$$ \frac{\partial \mathcal{L}}{\partial w} = \frac{2}{n}
\sum^{n}_{i=1}(wx^{i}+b-y^{i})x^{i}$$
$$ \frac{\partial \mathcal{L}}{\partial b}= \frac{2}{n}\sum^{n}_{i=1}(wx^{i}+b -y^{i})$$

** 多层神经网络

而对于多层神经网络，$z = w^{T}x + b$, $\frac{\partial z}{\partial w} =x$,  $\frac{\partial z}{\partial b} = 1$.
每层之间具有激活函数, $\sigma(z) = \frac{1}{1-e^{-z}}$,$\frac{\partial \sigma(x)}{\partial x} = \sigma (1-\sigma)$.
损失函数, $\mathcal{L} = \frac{1}{2}(\sigma - y^{i})^{2}$, $$\frac{\partial \mathcal{L}}{\partial \sigma} = (\sigma -y^{i})$$


$$\frac{\partial \mathcal{L}}{\partial w} = \frac{\partial
\mathcal{L}}{\partial \sigma} \cdot \frac{\partial \sigma }{\partial z} \cdot
\frac{\partial z}{\partial w}$$


$\frac{\partial \mathcal{L}}{\partial w} = (\sigma -
y)\sigma(1-\sigma) \cdot x$

$$\frac{\partial \mathcal{L}}{\partial b} = \frac{\partial
\mathcal{L}}{\partial \sigma} \cdot \frac{\partial \sigma }{\partial z} \cdot
\frac{\partial z}{\partial b}$$

$\frac{\partial  \mathcal{L}}{\partial b} = (\sigma -
y)\sigma(1-\sigma)$

如果对于多层神经网络，则需要逐层计算，其中$\frac{\partial
\mathcal{L}}{\partial w}$ 中的w就是相应层的权重，由最后的
L逐步回推到w。

* chapter 01

一个很简单的例子，用tf来求某个函数的导数
#+BEGIN_SRC python :results output
import tensorflow as tf
a = tf.constant(1.)
b = tf.constant(2.)
c = tf.constant(3.)
w = tf.constant(4.)

with tf.GradientTape() as tape:
    tape.watch([w])
    y = a*w**2 + b*w + c
[dy_dw] = tape.gradient(y, [w])
print(dy_dw)

#+END_SRC


检测cpu和gpu运行时的时间对比
#+begin_src python :results output 
  import tensorflow as tf
  import timeit

  n = 10000000
  with tf.device('/cpu:0'):
      cpu_a = tf.random.normal([1, n])
      cpu_b = tf.random.normal([n, 1])

  with tf.device('/gpu:0'):
      gpu_a = tf.random.normal([1, n])
      gpu_b = tf.random.normal([n, 1])

  def cpu_run():
      with tf.device('/cpu:0'):
          c = tf.matmul(cpu_a, cpu_b)
      return c

  def gpu_run():
      with tf.device('/gpu:0'):
          c = tf.matmul(gpu_a, cpu_b)
      return c

  cpu_time = timeit.timeit(cpu_run, number=10)
  gpu_time = timeit.timeit(cpu_run, number=10)
  print('run time: ', cpu_time, gpu_time)



#+end_src

#+RESULTS:
: run time:  0.0974298170003749 0.09593267899981583

不用tensorflow的API，使用纯函数来实现神经网络训练的例子
#+begin_src python :results output
import numpy as np
data = []
for i in range(100):
    x = np.random.uniform(-10., 10)
    y = 1.477*x + 0.089 + np.random.normal(0., 0.01)
    data.append([x, y])
data = np.array(data)


def mse(b, w, points):
    totalError = 0
    for i in range(0, len(points)):
        x = points[i, 0]
        y = points[i, 1]
        totalError += (y-(w*x+b))**2
    return totalError/float(len(points))


def step_gradient(b_current, w_current, points, lr):
    b_gradient = 0
    w_gradient = 0
    M = float(len(points))
    for i in range(0, len(points)):
        x = points[i, 0]
        y = points[i, 1]
        b_gradient += (2/M)*((w_current*x+b_current)-y)
        w_gradient += (2/M)*x*((w_current*x + b_current)-y)
    new_b = b_current - (lr*b_gradient)
    new_w = w_current - (lr*w_gradient)
    return [new_b, new_w]


def gradient_descent(points, staring_b, staring_w, lr, num_iterations):
    b = staring_b
    w = staring_w
    for step in range(num_iterations):
        b, w = step_gradient(b, w, np.array(points), lr)
        loss = mse(b, w, points)
        if step % 5000  == 0:
            print(f"iterations:{step}, loss :{loss}, w:{w}, b:{b}")
    return [b, w]


def main():
    lr = 0.001
    initial_b = 0
    initial_w = 0
    num_iterations = 100000
    [b, w] = gradient_descent(data, initial_b, initial_w, lr, num_iterations)
    loss = mse(b, w, data)
    print(f"Final loss :{loss}, w:{w}, b:{b}")


# if __name__ ==' __main__':
main()


#+end_src

#+RESULTS:
#+begin_example
iterations:0, loss :70.10079414302263, w:0.11067024890672035, b:0.003952550528332437
iterations:5000, loss :0.00010199717063390048, w:1.4769704836166537, b:0.08946956663994972
iterations:10000, loss :0.00010199716394347099, w:1.4769703908814773, b:0.08947221136809669
iterations:15000, loss :0.00010199716394347127, w:1.4769703908749494, b:0.08947221155430594
iterations:20000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:25000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:30000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:35000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:40000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:45000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:50000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:55000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:60000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:65000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:70000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:75000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:80000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:85000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:90000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:95000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
Final loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
#+end_example

* MNIST dataset

#+begin_src python :results output :session MNIST_image
  import tensorflow as tf
  from tensorflow import keras
  from tensorflow.keras import layers, optimizers, datasets
  w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))
  b1 = tf.Variable(tf.zeros([256]))
  w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))
  b2 = tf.Variable(tf.zeros([128]))
  w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))
  b3 = tf.Variable(tf.zeros([10]))
  (x,y),(x_val, y_val)=datasets.mnist.load_data()
  print('x:',x.shape, 'y:', y.shape,'x test:', x_val.shape, 'y test:', y_val)
  def preprocess(x, y):
      x = tf.cast(x, dtype = tf.float32)/255.
      x = tf.reshape(x, [-1,28*28])
      y = tf.cast(y,dtype=tf.int32)
      y = tf.one_hot(y, depth=10)
      return x,y
  train_db = tf.data.Dataset.from_tensor_slices((x,y))  #构建Dataset 对象
  train_db = train_db.shuffle(10000)                    # 打散样本顺序
  train_db = train_db.batch(128)                        #批训练
  train_db = train_db.map(preprocess)
  test_db = tf.data.Dataset.from_tensor_slices((x_val, y_val))
  test_db = test_db.shuffle(1000)
  test_db = test_db.batch(128)
  test_db = test_db.map(preprocess)
  lr = 0.001
  for epoch in range(8):
      for step, (x,y) in enumerate(train_db):
          with tf.GradientTape() as tape:
              h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])
              h1 = tf.nn.relu(h1)
              h2 = h1@w2 + b2
              h2 = tf.nn.relu(h2)
              out = h2@w3 + b3
              loss = tf.square(y - out)
              loss = tf.reduce_mean(loss)
              grads = tape.gradient(loss, [w1,b1,w2,b2,w3,b3])
              w1.assign_sub(lr *grads[0])
              b1.assign_sub(lr * grads[1])
              w2.assign_sub(lr *grads[2])
              b2.assign_sub(lr * grads[3])
              w3.assign_sub(lr *grads[4])
              b3.assign_sub(lr * grads[5])
  for x, y in test_db:
      h1 = x@w1 + b1
      h1 = tf.nn.relu(h1)
      h2 = h1@w2 + b2
      h2 = tf.nn.relu(h2)
      out = h2@w3 + b3
      pred = tf.argmax(out,axis=1)
      y = tf.argmax(y, axis=1)
      correct = tf.equal(pred, y)
      total_correct +=tf.reduce_sum(tf.cast(correct,dty=tf.int32)).numpy()



#+end_src

* Make_moons

all import 
#+begin_src python  :session make_moons
  import seaborn as sns
  import matplotlib.pyplot as plt
  #+end_src
#+RESULTS:


generate the data
#+begin_src python  :results output :session make_moons  
  from sklearn.datasets import make_moons
  from sklearn.model_selection import train_test_split
  N_samples = 2000
  Test_size = 0.3
  X, y = make_moons(n_samples = N_samples, noise = 0.2, random_state=100)
  X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = Test_size, random_state = 42)
  print(X.shape, y.shape)
  def make_plot(X, y, plot_name, file_name=None, XX=None, YY=None, preds=None,dark=False):
      if(dark):
          plt.style.use('dark_background')
      else:
          sns.set_style("whitegrid")
      plt.figure(figsize=(16,12))
      axes = plt.gca()
      axes.set(xlabel="$x_1$", ylabel="$x_2$")
      plt.title(plot_name,fontsize=30)
      plt.subplots_adjust(left=0.20)
      plt.subplots_adjust(right=0.8)
      if(XX is not None and YY is not None and preds is not None):
          plt.contourf(XX,YY,preds.reshape(XX.shape), 25, alpha=1,cmap = cm.Spectral)
          plt.contour(XX,YY, preds.reshape(XX.shape), levels=[.5],cmap="Greys", vmin=0,vmax=0.6)
          plt.scatter(X[:,0],X[:,1],c=y.ravel(), s=40, cmap=plt.cm.Spectral,edgecolors='none')
          plt.savefig('data.svg')
          plt.close()
  make_plot(X,y,None,"Classification Dataset Visualization")
#+end_src
#+RESULTS:
: (2000, 2) (2000,)


generate the  signal Layer class
#+begin_src python :session make_moons
  class Layer:
      def __init__(self, n_input, n_neurons,activation=None, weight=None,bias=None):
          self.weight = weight if weight is not None else np.random.randn(n_input,n_neurons)*np.sqrt(1/n_neurons)
          self.bias = bias if bias is not None else np.random.rand(n_neurons)*0.1
          self.activation = activation
          self.last_activation = None
          self.error = None
          self.delta = None
      def activate(self,x):
          r = np.dot(x, self.weight)+self.bias
          self.last_activation = self._apply_activation(r)
          return self.last_activation
      def _apply_activation(self, r):
          if self.activation is None:
              return r
          elif self.activation == 'relu':
              return np.maximum(r,0)
          elif self.activation == 'tanh':
              return np.tanh(r)
          elif self.activation == 'sigmoid':
              return 1/(1+np.exp(-r))
          return r
      def apply_activation_derivative(self, r):
          if self.activation is None:
              return np.ones_like(r)
          elif self.activation == 'relu':
              grad = np.array(r, copy=True)
              grad[r>0] = 1.
              grad[r<=0] =0.
              return grad
          elif self.activation == 'tanh':
              return 1-r**2
          elif self.activation == 'sigmoid':
              return r*(1-r)
          return r
            
#+end_src
#+RESULTS:
| 2000 | 2 |

generate the multi Layers Class NeuralNetwork
#+begin_src python :session make_moons
  class NeuralNetwork:
      def __init__(self):
          self._layers = []
      def add_layer(self, layer):
          self._layers.append(layer)
      def feed_forward(self, X):
          for layer in self._layers:
              X = layer.activate(X)
          return X

      def backpropagation(self, X, y,learning_rate):
          output = self.feed_forward(X)
          for i in reversed(range(len(self._layers))):
              layer = self._layers[i]
              if layer == self._layers[-1]:
                  layer.error = y-output
                  layer.delta = layer.error*layer.apply_activation_derivative(output)
              else:
                  next_layer = self._layers[i+1]
                  layer.error = np.dot(next_layer.weights, next_layer.delta)
                  layer.delta = layer.error*layer.apply_activation_derivative(layer.last_activation)

          for i in range(len(self._layers)):
              layer = self._layers[i]
              o_i = np.atleast_2d(X if i == 0 else  self._layers[i-1].last_activation)
              layer.weights += layer.delta*o_i.T*learning_rate


      def train(self, X_train, X_test, y_train, y_test, learning_rate, max_epochs):
          y_onehot = np.zeros((y_train.shape[0],2))
          y_onehot[np.arange(y_train.shape[0]),y_train] =1
          mses = []
          for i in range(max_epochs):
              for j in range(len(X_train)):
                  self.backpropagation(X_train[j], y_onehot[j], learning_rate)
              if i%10 == 0:
                  mse = np.mean(np.square(y_onehot - self.feed_forward(X_train)))
                  mses.apply(mse)
                  print('Epoch : #%s, MSE: %f' %(i, float(mse)))
                  print('Accuracy: %.2f%%' %(self.accuracy(self.predict(X_test),y_test.flatten())*100))
          return mses


#+end_src
#+RESULTS:

#+begin_src  python :session make_moons
nn = Neuralnetwork()
nn.add_layer(Layer (2, 25, 'sigmoid'))
nn.add_layer(Layer(25, 50, 'sigmoid'))
nn.add_layer(Layer(50, 25, 'sigmoid'))
nn.add_layer(Layer(25, 2, 'sigmoid'))
nn.backpropagation(X_train,y_train,0.001)
nn.train(X_train, X_test, y_train, y_test, 0.001,20)


#+end_src
#+RESULTS:

different Layers
#+begin_src python :session make_moons
  for n in range(5):
      model = Sequential()
      model.add(Dense(8,input_dim=2,activation='relu'))
      for _ in range(n):
          model.add(Dense(32,activation='relu'))
      model.add(Dense(1,activation='sigmoid'))
      model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
      history = model.fit(X_train,y_train,epochs = 20, verbose=1)
      preds = model.predict_classes(np.c_[XX.ravel(), YY_ravel()])
      title = "网络层数({})".format(n)
      file = "网络容量 %f.png" %(2+n*1)
      make_plot(X_train,y_train, title,file,XX,YY,preds)
    

#+end_src

#+RESULTS:

* Keras
** tf.keras 

#+begin_src python :results output
  import tensorflow as tf
  from tensorflow import keras
  from tensorflow.keras import layers

  x = tf.constant([2., 1., 0.1])
  print(tf.keras.layers.Softmax(axis=-1)(x))
  print(tf.nn.softmax(x))

#+end_src

#+RESULTS:
: tf.Tensor([0.6590012  0.24243298 0.09856589], shape=(3,), dtype=float32)
: tf.Tensor([0.6590012  0.24243298 0.09856589], shape=(3,), dtype=float32)

tf.keras.Model 是tf.keras.Sequential的父类(网络类）
Sequential类还有方法：
Sequential.compile()
Sequential.fit()
Sequential.predict()

tf.keras.layers.Layer 是网络层类的父类（网络层类）

** 模型的保存和加载
*** 保存数值
Sequential.save_weights('weights.ckpt')
重建了一样的网络后，重新加载
Sequential.load_weights('weights.ckpt')
*** 保存框架
tf.keras.Sequential.save('model.h5')
不需要重构模型，可以直接生成保存的模型
network = tf.keras.models.load_model('model.h5')
*** 跨系统平台保存恢复
tf.saved_model.save(network, 'model-savedmodel')
复制，分发该文件后，在跨平台上复现
network = tf.saved_model.load('model-savedmodel')
** self-def 
自定义网络层类，继承Layer
自定义网络类，继承Model

* regularation
** L0
控制网络中的非零权重

** L1
网络中的所有元素的绝对值之和
促使网络生成更多的稀疏矩阵
** L2
网络中的所有元素平方和
促使网络生成小比重的权值
* Dropout
tf.nn.dropout(x, rate = 0.5)
model.add(layers.Dropout(rate=0.5))
* Data Augmentation
** resize
tf.image.resize(x,[244,244])
** rote
tf.image.rot90(x,1) k为1时，代表一个90度的g逆时针旋转
** flip
tf.image.random_flip_left_right(x)
tf.image.random_flip_up_down(x)
** crop
先放大，再剪裁
tf.image.resize(x,[244,244])
tf.image.random_crop(x,[224,224,3])
* convolution neural network

