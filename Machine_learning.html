<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-07-31 Fr 12:57 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Machine Learning</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="sx" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/styles/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Machine Learning</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org08c0cd1">Math for ML</a>
<ul>
<li><a href="#org5d5a765">Bayes's Rule</a></li>
<li><a href="#org85d9cc8">Kovarianz Matrix</a></li>
<li><a href="#org3a2566e">Regularization</a></li>
<li><a href="#orgbaeda0a">Error Bias-Variance-trade-off</a></li>
<li><a href="#orgfbe7594">Multi variable Gaussian distribution</a></li>
<li><a href="#orgb52a118">Mahalanobis distance</a></li>
<li><a href="#org85ff82e">K-fold Cross Validation</a></li>
<li><a href="#org812a130">precision  recall</a></li>
<li><a href="#org0b6bb47">Gini</a></li>
</ul>
</li>
<li><a href="#org901a05e">activation function</a>
<ul>
<li><a href="#orgd0005b3">no activation</a></li>
<li><a href="#orgfb4b151">Sigmoid</a></li>
<li><a href="#org19d2e01">ReLU</a></li>
<li><a href="#orgd4e563f">LeakyReLU</a></li>
<li><a href="#org132bb75">Tanh</a></li>
<li><a href="#orga724aec">MSE</a></li>
<li><a href="#org34d6f45">inf entropy</a></li>
<li><a href="#org28083d5">softmax</a></li>
<li><a href="#orgff993e2">cross entropy</a></li>
</ul>
</li>
<li><a href="#org8feca63">Algorithms</a>
<ul>
<li><a href="#orgc2aff0d">Ordinary Least Squares(OLS)</a>
<ul>
<li><a href="#org9bb72e7">正规化方程</a></li>
<li><a href="#orgc354ab5">正则化正规化方程</a></li>
</ul>
</li>
<li><a href="#org9becc3b">朴素贝叶斯</a></li>
<li><a href="#org8f452f1">Decision List</a></li>
<li><a href="#org07bf123">Decision tree</a>
<ul>
<li><a href="#org0f104f9">方法描述和变量</a></li>
<li><a href="#orga982804">预剪枝和后剪枝</a></li>
<li><a href="#orge4cd32f">属性有连续值和缺失值</a></li>
<li><a href="#orgdf80fd9">多变量决策</a></li>
</ul>
</li>
<li><a href="#org50f689b">Bagging</a></li>
<li><a href="#org30deb56">Random Forest</a></li>
<li><a href="#org42520f8">Boosting</a></li>
<li><a href="#org43bc9a6">gradient decent</a></li>
<li><a href="#orgdfe2b9a">linear regression</a></li>
<li><a href="#org5b7b751">Support Vector Machine</a>
<ul>
<li><a href="#orga1db531">without soft margin</a></li>
<li><a href="#org6ea42f0">with soft margin</a></li>
</ul>
</li>
<li><a href="#orgdde39c7">Neural network</a>
<ul>
<li><a href="#org2abab32">Backpropagation</a></li>
</ul>
</li>
<li><a href="#orgcb8456b">Conversational neural Network</a></li>
<li><a href="#org7983b86">linear Discriminate Analysis</a>
<ul>
<li><a href="#org4e387fa">Fisher's linear discriminant</a></li>
<li><a href="#orga61f55c">Fisher's linear discriminant with Kernel method</a></li>
<li><a href="#org29ea8e0">Probabilistic Generative Model</a></li>
<li><a href="#orge2e4a8e">Probabilistic Discriminant Model</a></li>
</ul>
</li>
<li><a href="#org1c7b934">logistic regression</a></li>
<li><a href="#orgd1a04b7">Principe Component Analysis</a>
<ul>
<li><a href="#orgb018716">PCA Algorithms</a></li>
<li><a href="#org634ea2f">Probabilistic generative model for PCA</a></li>
</ul>
</li>
<li><a href="#org0de2bf5">K-Means</a></li>
<li><a href="#org89538e5">EM algorithms</a></li>
</ul>
</li>
<li><a href="#org5e35e58">link</a></li>
</ul>
</div>
</div>
<div id="outline-container-org08c0cd1" class="outline-2">
<h2 id="org08c0cd1">Math for ML</h2>
<div class="outline-text-2" id="text-org08c0cd1">
</div>
<div id="outline-container-org5d5a765" class="outline-3">
<h3 id="org5d5a765">Bayes's Rule</h3>
<div class="outline-text-3" id="text-org5d5a765">
<p>
if x and y are independent:
\[ p(x,y) =  p(y)p(x) = p(x) p(y) = p(y,x) \] 
</p>

<pre class="example">
条件概率：
条件概率 = 联合概率/边缘概率
先验概率和后验概率都是条件概率，但是条件已知是先验
</pre>
<p>
\[ P(y|x) = \frac{ P(x,y)}{P(x)}\]
</p>
<pre class="example">
全概率公式
</pre>
<p>
\[ p(y) = \sum_{i=1}^{n} p(y,x_{i}) \]
</p>
<pre class="example">
贝叶斯公式
</pre>

<p>
\[ P(AB)=P(BA) \]
\[ P(y|x)P(x) = P(x|y)P(y)\]
\[ P(y|x) = \frac{ P(x|y) P(y)}{P(x)}\]
</p>

<pre class="example">
贝叶斯公式 + 全概率公式 + 条件概率 
</pre>

\begin{eqnarray*}
P(A|B) &= \frac{P(B|A)P(A)}{P(B)} \\
       &= \frac{P(B|A)P(A)}{\sum_{i=1}^{n} p(B,A_{i})} \\
       &= \frac{P(B|A)P(A)}{\sum_{i=1}^{n} P(B|A_{i})P(A)} \\
\end{eqnarray*}
</div>
</div>

<div id="outline-container-org85d9cc8" class="outline-3">
<h3 id="org85d9cc8">Kovarianz Matrix</h3>
<div class="outline-text-3" id="text-org85d9cc8">
<p>
seeing link : <a href="https://scofild429.github.io/subjects/statistik.html">Statistik web</a>
org link : <a href="file:///home/sx/Dropbox/subjects/statistik.html">Startistik org</a>
</p>

<p>
for i = {1&#x2026;n}, \(x_{i}\) is a random variable, which belong to
Gaussian distribution
</p>

<p>
set 
 \[ X = \left( \begin{aligned}  x_{1} \\ x_{2}\\ . \\. \\x_{n}  \end{aligned}\right) \]
</p>

<p>
\[ \bar{X} = \left( \begin{aligned}  \bar{x}_{1}
\\ \bar{x}_{2}\\ . \\. \\ \bar{x}_{n}  \end{aligned} \right) \]
</p>

<p>
co-variance matrix \(\Sigma = E [(X-\bar{X})(X-\bar{X})^{T} ]\)
</p>

\begin{equation}
\Sigma = 
  \left(
  \begin{array}{c}
          x_{1}-\bar{x}_{1} \\
          x_{2}-\bar{x}_{2} \\
          x_{3}-\bar{x}_{3} \\
          ..                \\
          x_{n}-\bar{x}_{n} 
 \end{array}
 \right)
  \left(
  \begin{array}{ccccc}
          x_{1}-\bar{x}_{1} &
          x_{2}-\bar{x}_{2} &
          x_{3}-\bar{x}_{3} &
          ..                &
          x_{n}-\bar{x}_{n} 
  \end{array}
  \right)
\end{equation}
<p>
对角线上是对应元素的方差，其他是相对于两个元素的协方差
</p>
</div>
</div>

<div id="outline-container-org3a2566e" class="outline-3">
<h3 id="org3a2566e">Regularization</h3>
<div class="outline-text-3" id="text-org3a2566e">
<p>
\[ Loss = \frac{1}{2}\sum^{N}_{i=1}(y_{i}-w^{t}\phi(x_{i}))^{2}+\frac{\lambda}{2}\sum^{M}_{j=1}|w_{j}|^{q} \]
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">N</td>
<td class="org-left">example number</td>
</tr>

<tr>
<td class="org-left">M</td>
<td class="org-left">Eigenschaften Number, Diemension number</td>
</tr>

<tr>
<td class="org-left">L0</td>
<td class="org-left">控制网络中的非零权重</td>
</tr>

<tr>
<td class="org-left">L1</td>
<td class="org-left">网络中的所有元素的绝对值之和,促使网络生成更多的稀疏矩阵</td>
</tr>

<tr>
<td class="org-left">L2</td>
<td class="org-left">网络中的所有元素平方和,促使网络生成小比重的权值</td>
</tr>

<tr>
<td class="org-left">w</td>
<td class="org-left">w<sub>1</sub>, w<sub>2</sub></td>
</tr>

<tr>
<td class="org-left">q=1</td>
<td class="org-left">l1 regularization</td>
</tr>

<tr>
<td class="org-left">q=2</td>
<td class="org-left">l2 regularization</td>
</tr>

<tr>
<td class="org-left">&lambda;</td>
<td class="org-left">learning rate</td>
</tr>

<tr>
<td class="org-left">&sum;<sup>N</sup><sub>i=1</sub>(y<sub>i</sub>-w<sup>t</sup>&phi;(x<sub>i</sub>))<sup>2</sup></td>
<td class="org-left">依据w的同心圆</td>
</tr>

<tr>
<td class="org-left">&sum;<sup>M</sup><sub>j=1</sub>w<sub>j</sub><sup>q</sup></td>
<td class="org-left">q=1, 菱形， q=2, 圆形</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
Loss 要最小，part1 刚好和 part2 相接，l1会在坐标轴上，所以如果有较小分
量，会被直接设为0
</p>
</div>
</div>

<div id="outline-container-orgbaeda0a" class="outline-3">
<h3 id="orgbaeda0a">Error Bias-Variance-trade-off</h3>
<div class="outline-text-3" id="text-orgbaeda0a">
<pre class="example">
Error = Bias + Variance + noise
</pre>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Bias</td>
<td class="org-left">偏差</td>
<td class="org-left">欠拟合</td>
<td class="org-left">发挥，观测等主观因素影响</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Variance</td>
<td class="org-left">方差</td>
<td class="org-left">过过拟合</td>
<td class="org-left">稳定性，模型的构建决定</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">noise</td>
<td class="org-left">噪音</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">统难度</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-orgfbe7594" class="outline-3">
<h3 id="orgfbe7594">Multi variable Gaussian distribution</h3>
<div class="outline-text-3" id="text-orgfbe7594">
<p>
seeing the link 知乎  <a href="https://zhuanlan.zhihu.com/p/58987388">zhihu link</a>
</p>

<p>
\[
{\displaystyle f_{\mathbf {X} }(x_{1},\ldots ,x_{k})={\frac {\exp
\left(-{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm
{T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu
}})\right)}{\sqrt {(2\pi )^{k}|{\boldsymbol {\Sigma
}}|}}}}
\]
</p>

<div class="org-src-container">
<pre class="src src-python">
<span style="color: #4f97d7; font-weight: bold;">def</span> <span style="color: #bc6ec5; font-weight: bold;">gaussian</span>(x,mean,cov):
    <span style="color: #7590db;">dim</span> = np.shape(cov)[0] <span style="color: #2aa1ae; background-color: #292e34;">#</span><span style="color: #2aa1ae; background-color: #292e34;">&#32500;&#24230;</span>
    <span style="color: #2aa1ae; background-color: #292e34;">#</span><span style="color: #2aa1ae; background-color: #292e34;">&#20043;&#25152;&#20197;&#21152;&#20837;&#21333;&#20301;&#30697;&#38453;&#26159;&#20026;&#20102;&#38450;&#27490;&#34892;&#21015;&#24335;&#20026;0&#30340;&#24773;&#20917;</span>
    <span style="color: #7590db;">covdet</span> = np.linalg.det(cov+np.eye(dim)*0.01) <span style="color: #2aa1ae; background-color: #292e34;">#</span><span style="color: #2aa1ae; background-color: #292e34;">&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#34892;&#21015;&#24335;</span>
    <span style="color: #7590db;">covinv</span> = np.linalg.inv(cov+np.eye(dim)*0.01) <span style="color: #2aa1ae; background-color: #292e34;">#</span><span style="color: #2aa1ae; background-color: #292e34;">&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#36870;</span>
    <span style="color: #7590db;">xdiff</span> = x - mean
    <span style="color: #2aa1ae; background-color: #292e34;">#</span><span style="color: #2aa1ae; background-color: #292e34;">&#27010;&#29575;&#23494;&#24230;</span>
    <span style="color: #7590db;">prob</span> = 1.0/np.power(2*np.pi,1.0*dim/2)/np.sqrt(np.<span style="color: #4f97d7;">abs</span>(covdet))*np.exp(-1.0/2*np.dot(np.dot(xdiff,covinv),xdiff))
    <span style="color: #4f97d7; font-weight: bold;">return</span> prob

</pre>
</div>
</div>
</div>

<div id="outline-container-orgb52a118" class="outline-3">
<h3 id="orgb52a118">Mahalanobis distance</h3>
<div class="outline-text-3" id="text-orgb52a118">
<p>
\[ \Delta = \left(-{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm
{T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu
}})\right)
\]
\[ \Sigma = \sum U \Lambda U^{T} \]
\[ \Sigma^{-1} = \sum U \Lambda^{-1} U^{T} \]
</p>

<p>
\[ \Delta = -{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm
{T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu
}}) = -{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm
{T} }  U \Lambda^{-1} U^{T}({\mathbf {x} }-{\boldsymbol {\mu
}})
\]
马氏距离所使用的变换 : \[ Z = U^{T}(X - \mu) \],
</p>


<p>
\[ D = \sqrt{ZZ^{T}} \]
关于新的坐标，U 是变换的旋转，\(\Lambda\) 是基底的延伸，\((x-\mu)\) 是在其
上的投影，此后，在新坐标上，即为多变量，标准，不相关高斯分布
</p>
</div>
</div>

<div id="outline-container-org85ff82e" class="outline-3">
<h3 id="org85ff82e">K-fold Cross Validation</h3>
<div class="outline-text-3" id="text-org85ff82e">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">N</td>
<td class="org-left">total examples</td>
</tr>

<tr>
<td class="org-left">K</td>
<td class="org-left">number of sub-fold</td>
</tr>

<tr>
<td class="org-left">m</td>
<td class="org-left">number of each sub-fold</td>
</tr>

<tr>
<td class="org-left">big K</td>
<td class="org-left">small bias, with over fitting, big variance</td>
</tr>

<tr>
<td class="org-left">small K</td>
<td class="org-left">big bias, without fitting, low variance</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org812a130" class="outline-3">
<h3 id="org812a130">precision  recall</h3>
<div class="outline-text-3" id="text-org812a130">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">TPR</th>
<th scope="col" class="org-left">recall 查全 R</th>
<th scope="col" class="org-left">predict positive 测正 &amp;  实正 actually positive</th>
<th scope="col" class="org-left">predict negative 测反 &amp; 实正  actually positive</th>
</tr>

<tr>
<th scope="col" class="org-left">FPR</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">predict positive 测正 &amp;  实反 actually negative</th>
<th scope="col" class="org-left">predict negative  测反 &amp; 实反 actually negative</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">precision 查准 P</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
R-P Couve : R P
ROC couve : FPR TPR
</p>

<p>
\[ F_{1} = \frac{1}{2} \cdot (\frac{1}{P}+\frac{1}{R})\]
\[ F_{\beta} = \frac{1}{\beta^{2}} \cdot
(\frac{\beta^{2}}{P}+\frac{1}{R})\]
</p>

<p>
关联性属性 ： 高中低 （3,2,1）
非关联性属性： 猪狗羊 （(1,0,0), (0,1,0),(0,0,1)）
</p>
</div>
</div>


<div id="outline-container-org0b6bb47" class="outline-3">
<h3 id="org0b6bb47">Gini</h3>
<div class="outline-text-3" id="text-org0b6bb47">
<p>
吉尼系数
\[ Gini(D) = \sum^{|y|}_{k=1}\sum_{k^{'}\not = k} p_{k}p_{k^{'}} = 1- \sum^{|y|}_{k=1}p_{k}^{2}\]
</p>
</div>
</div>
</div>


<div id="outline-container-org901a05e" class="outline-2">
<h2 id="org901a05e">activation function</h2>
<div class="outline-text-2" id="text-org901a05e">
</div>
<div id="outline-container-orgd0005b3" class="outline-3">
<h3 id="orgd0005b3">no activation</h3>
<div class="outline-text-3" id="text-orgd0005b3">
<p>
输出为实数空间或某个区间， 连续变化。直接有输出值和真实值比较
</p>
</div>
</div>
<div id="outline-container-orgfb4b151" class="outline-3">
<h3 id="orgfb4b151">Sigmoid</h3>
<div class="outline-text-3" id="text-orgfb4b151">
<p>
\[ Sigmoid(x) = \frac{1}{1+e^{-x}}\]
</p>

<p>
导数：\[ \sigma'(x) = \sigma(x)(1-\sigma(x))\]
</p>
</div>
</div>

<div id="outline-container-org19d2e01" class="outline-3">
<h3 id="org19d2e01">ReLU</h3>
<div class="outline-text-3" id="text-org19d2e01">
<p>
\[ Relu(x) = 
\begin{cases}
x&  x >=0 \\
0&  x < 0
\end{cases}\]
</p>

<p>
\[ Relu(x) = max(0,x)\]
</p>
</div>
</div>

<div id="outline-container-orgd4e563f" class="outline-3">
<h3 id="orgd4e563f">LeakyReLU</h3>
<div class="outline-text-3" id="text-orgd4e563f">
<p>
\[ LeakyReLU(x) = \begin{cases}
x& x >=0 \\
px& x <0, 0<p<<1
\end{cases}\]
</p>
</div>
</div>

<div id="outline-container-org132bb75" class="outline-3">
<h3 id="org132bb75">Tanh</h3>
<div class="outline-text-3" id="text-org132bb75">
<p>
\[ tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\]
\[ tanh(x) = 2.sigmoid(2x)-1\]
导数：\[\tanh'(x) = 1- \tanh^{2}(x)\]
</p>
</div>
</div>

<div id="outline-container-orga724aec" class="outline-3">
<h3 id="orga724aec">MSE</h3>
<div class="outline-text-3" id="text-orga724aec">
<p>
\[ \mathcal{L} = MSE(y, o) =
\frac{1}{d_{out}}\sum_{i=1}^{d_{out}}(y_{i}-o^{i})^{2}\]
导数 ：\[ \frac{\partial \mathcal{L}}{\partial o_{i}}= (o_{i}-y_{i})\]
</p>
</div>
</div>

<div id="outline-container-org34d6f45" class="outline-3">
<h3 id="org34d6f45">inf entropy</h3>
<div class="outline-text-3" id="text-org34d6f45">
<p>
\[ H(p) = -\sum_{i}P(i)\log_{2}^{P_{i}}\]
</p>
</div>
</div>
<div id="outline-container-org28083d5" class="outline-3">
<h3 id="org28083d5">softmax</h3>
<div class="outline-text-3" id="text-org28083d5">
<p>
\[ p_{z_{i}} = \frac{e^{z_{i}}}{\sum_{j}e^{z_{j}}}\]
所有种类的概率之和为1
导数:
\[ \frac{ \partial p_{z_{i}}}{\partial z_{j}} = 
\begin{cases}
p_{i}(1-p_{j}) &  if i =j \\
-p_{i}p_{j}    & if \neq j
\end{cases}\]
</p>
</div>
</div>

<div id="outline-container-orgff993e2" class="outline-3">
<h3 id="orgff993e2">cross entropy</h3>
<div class="outline-text-3" id="text-orgff993e2">
<p>
在计算交叉熵时， 一般是和 softmax 函数一起使用的
\[ H(p||q) = -\sum_{i} p(i) \log_{2}^{q_{i}}\]
\[H(p||q) = H(p) + D_{KL}(p||q)\]
</p>

<p>
for One-hot coding
\[ H(p||q) = D_{KL}(p||q) = \sum_{i}y_{i}log(\frac{y_{j}}{o_{j}}) = 
1 \cdot \log\frac{1}{o_{i}} + \sum_{j!=i}0 \cdot \log \frac{0}{o_{j}}
= -\log o_{i}\]
o<sub>i</sub> 为1 时，预测正确，交叉熵为0。
导数：
\[ \mathcal{L} = -\sum_{k}y_{k}\log(p_{k})\]
\[\begin{aligned}
 \frac{\partial \mathcal{L}}{\partial z_{i}} & =
-\sum_{k} y_{k} \frac{\partial \log(p_{k})}{\partial z_{i}} \\
&= -\sum_{k} y_{k} \frac{\partial \log(p_{k})}{\partial p_{k}} \cdot
\frac{\partial p_{k}}{\partial z_{i}} \\
&= -\sum_{k} y_{k} \frac{1}{\partial p_{k}} \cdot
\frac{\partial p_{k}}{\partial z_{i}} \\
\end{aligned}
\]
</p>

<p>
用上面 softmax 的导数结果，分为k=i 和k!=i两种情况
\[ \frac{\partial \mathcal{L}}{z_{i}}=p_{i}-y_{i}\]
</p>
</div>
</div>
</div>


<div id="outline-container-org8feca63" class="outline-2">
<h2 id="org8feca63">Algorithms</h2>
<div class="outline-text-2" id="text-org8feca63">
</div>
<div id="outline-container-orgc2aff0d" class="outline-3">
<h3 id="orgc2aff0d">Ordinary Least Squares(OLS)</h3>
<div class="outline-text-3" id="text-orgc2aff0d">
</div>
<div id="outline-container-org9bb72e7" class="outline-4">
<h4 id="org9bb72e7">正规化方程</h4>
<div class="outline-text-4" id="text-org9bb72e7">
<p>
正则化方程的推导，用高斯分布的多变量分布的Maxisum likelihood,能一起求得对weight和bias值 : 
</p>

<p>
但是要添加一列1到 train 和 test，至于在前面还是后面有点怪异。
</p>

<p>
目前认为，在后面的话，多变量和参数可以按需求访问
</p>

<p>
Loss function:
\[ J = \frac{1}{2m}\sum(h_{\theta}(x_{i})-y_{i})^{2}\]
\[ \sigma = \frac{1}{2m}(X \theta -y)^{T} (X \theta -y)\]
对\(\theta\) 求导，并令其为0，
\[\theta = (X^{T}X)^{-1}X^{T}y \]
但是要求\(X^{T}X\) 必须可逆。
</p>
</div>
</div>

<div id="outline-container-orgc354ab5" class="outline-4">
<h4 id="orgc354ab5">正则化正规化方程</h4>
<div class="outline-text-4" id="text-orgc354ab5">
<p>
\[w = (\Phi^{T}\Phi + \lambda I)^{-1} \Phi^{T}y \]
</p>


<div class="org-src-container">
<pre class="src src-python"><span style="color: #4f97d7; font-weight: bold;">import</span> numpy <span style="color: #4f97d7; font-weight: bold;">as</span> np
<span style="color: #4f97d7; font-weight: bold;">import</span> random
<span style="color: #4f97d7; font-weight: bold;">from</span> sklearn <span style="color: #4f97d7; font-weight: bold;">import</span> linear_model
<span style="color: #7590db;">testsize</span> = 5

<span style="color: #7590db;">x</span> = np.array([a <span style="color: #4f97d7; font-weight: bold;">for</span> a <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(100)])
<span style="color: #7590db;">onesx</span> = np.ones(x.shape)
<span style="color: #7590db;">X</span> = np.c_[x,onesx]
<span style="color: #7590db;">y</span> = np.array([a*5 + 20 + random.randint(0,3) <span style="color: #4f97d7; font-weight: bold;">for</span> a <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(100)])
<span style="color: #4f97d7; font-weight: bold;">print</span>(<span style="color: #2d9574;">"the X shape is {}, and y shape is {}"</span>.<span style="color: #4f97d7;">format</span>(X.shape, y.shape))

<span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">weight = np.dot(np.dot(np.linalg.pinv(np.dot(X.T, X)), X.T), y)</span>
<span style="color: #7590db;">weight</span> = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

<span style="color: #4f97d7; font-weight: bold;">print</span>(<span style="color: #2d9574;">"OLS : the weight is{}, and the bais is {} "</span>.<span style="color: #4f97d7;">format</span>(weight[:-1], weight[-1]))
<span style="color: #4f97d7; font-weight: bold;">print</span>(<span style="color: #2d9574;">"the predect of 5 ele is {}"</span>.<span style="color: #4f97d7;">format</span>(np.dot(np.c_[np.arange(testsize),np.ones(testsize)], weight)))


</pre>
</div>

<pre class="example">
也可以是对变量 with multi variables
</pre>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #4f97d7; font-weight: bold;">import</span> numpy <span style="color: #4f97d7; font-weight: bold;">as</span> np
<span style="color: #4f97d7; font-weight: bold;">import</span> random
<span style="color: #4f97d7; font-weight: bold;">from</span> sklearn <span style="color: #4f97d7; font-weight: bold;">import</span> linear_model
<span style="color: #7590db;">testsize</span> = 5

<span style="color: #7590db;">x</span> = np.array([a <span style="color: #4f97d7; font-weight: bold;">for</span> a <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(100)])
<span style="color: #7590db;">onesx</span> = np.ones(x.shape)
<span style="color: #7590db;">X</span> = np.c_[onesx, x, 2*x]
<span style="color: #7590db;">y</span> = np.array([a*5 + 20 + random.randint(0,3) <span style="color: #4f97d7; font-weight: bold;">for</span> a <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(100)])
<span style="color: #4f97d7; font-weight: bold;">print</span>(<span style="color: #2d9574;">"the X shape is {}, and y shape is {}"</span>.<span style="color: #4f97d7;">format</span>(X.shape, y.shape))

<span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">ordinary  least squares (&#27491;&#35268;&#21270;&#26041;&#27861;&#65289;</span>
<span style="color: #7590db;">weight</span> = np.dot(np.dot(np.linalg.pinv(np.dot(X.T, X)), X.T), y)
<span style="color: #4f97d7; font-weight: bold;">print</span>(<span style="color: #2d9574;">"OLS : the weight is{}, and the bais is {} "</span>.<span style="color: #4f97d7;">format</span>(weight[:-1], weight[-1]))
<span style="color: #4f97d7; font-weight: bold;">print</span>(<span style="color: #2d9574;">"the predect of 5 ele is {}"</span>.<span style="color: #4f97d7;">format</span>(np.dot(np.c_[np.arange(testsize), np.arange(testsize),np.ones(testsize)], weight)))


</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org9becc3b" class="outline-3">
<h3 id="org9becc3b">朴素贝叶斯</h3>
<div class="outline-text-3" id="text-org9becc3b">
<p>
假设各个属性完全独立
要判断某示例的分类，分别计算出该示例在每个分类中的概论乘以该类别中该示
例的各种属性出现的条件概论， 谁大选谁。（注意样本不足引起的某属性的条
件为零）
</p>
</div>
</div>

<div id="outline-container-org8f452f1" class="outline-3">
<h3 id="org8f452f1">Decision List</h3>
<div class="outline-text-3" id="text-org8f452f1">
<p>
(f1,v1),(f2,v2)&#x2026;.(fr,vr)
fi is a term in CNF, vi belongs {0,1}, and the last term fr is always
true. and each term can be viewed as if else extended. if fi is
matched, so vi is its value.
</p>

<pre class="example">
for 0&lt;k&lt;n, k-CNF and k-DNF are proper 


</pre>
</div>
</div>

<div id="outline-container-org07bf123" class="outline-3">
<h3 id="org07bf123">Decision tree</h3>
<div class="outline-text-3" id="text-org07bf123">
</div>
<div id="outline-container-org0f104f9" class="outline-4">
<h4 id="org0f104f9">方法描述和变量</h4>
<div class="outline-text-4" id="text-org0f104f9">
<p>
在训练集内以最大信息增益来确定划分属性，在各个子区内再重复剩下的属性
</p>

<p>
信息熵
\[ Ent(D)=-\sum^{y}_{k=1}p_{k}log_{2}^{p_{k}}\]
</p>

<p>
信息熵增益
\[ Gain(D,a) = Ent(D)-\sum^{V}_{v=1}\frac{|D^{v}|}{|D|}Ent(D^{v}) \]
</p>

<p>
吉尼系数
\[ Gini(D) = \sum^{|y|}_{k=1}\sum_{k^{'}\not = k} p_{k}p_{k^{'}} = 1- \sum^{|y|}_{k=1}p_{k}^{2}\]
</p>
</div>
</div>

<div id="outline-container-orga982804" class="outline-4">
<h4 id="orga982804">预剪枝和后剪枝</h4>
<div class="outline-text-4" id="text-orga982804">
<p>
在利用训练集的最大信息增益确定划分属性后，用验证集来检验划分，如果验证
集的信息熵增加，（泛化结果不好)否定此次划分，设为叶节点
</p>

<p>
后剪枝是在这个树完成后，用验证集去检验每一个内节点，从下到上，如果去掉
该划分有更小的信息熵，则废除该划分。
</p>
</div>
</div>

<div id="outline-container-orge4cd32f" class="outline-4">
<h4 id="orge4cd32f">属性有连续值和缺失值</h4>
<div class="outline-text-4" id="text-orge4cd32f">
<p>
连续值离散化：排列该属性的所有取值n个，在n-1个区间中去中间值为离散值，
遍历所有离散值，找到最大信息增益的离散值，作二分。
</p>

<p>
缺失值，取出该属性的非缺失子集，再配以相应的比率计算信息增益，处理和以
前一样。如果选出的划分属性有缺失值，则给划分不作用到缺失样本，复制到每
个划分子集
</p>
</div>
</div>
<div id="outline-container-orgdf80fd9" class="outline-4">
<h4 id="orgdf80fd9">多变量决策</h4>
<div class="outline-text-4" id="text-orgdf80fd9">
<p>
每个划分属性是多个属性（变量）的线性组合
</p>
</div>
</div>
</div>

<div id="outline-container-org50f689b" class="outline-3">
<h3 id="org50f689b">Bagging</h3>
<div class="outline-text-3" id="text-org50f689b">
<p>
多次放回抽样，用不同抽样的数据集在多棵树上并行计算，
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">More Tree</td>
<td class="org-left">Bias remained</td>
<td class="org-left">Variance reduce to limit</td>
</tr>
</tbody>
</table>
<p>
所以刚开始选择偏差小，方差大的强模型
</p>
</div>
</div>

<div id="outline-container-org30deb56" class="outline-3">
<h3 id="org30deb56">Random Forest</h3>
<div class="outline-text-3" id="text-org30deb56">
<p>
Random Forest = Decision Tree + Bagging + random Eigenschaften
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">More deeper</td>
<td class="org-left">Bias reduce</td>
</tr>

<tr>
<td class="org-left">More Eigenschaft</td>
<td class="org-left">Bias reduce</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org42520f8" class="outline-3">
<h3 id="org42520f8">Boosting</h3>
<div class="outline-text-3" id="text-org42520f8">
<p>
固定数据集，在多个串行的模型上顺序计算，模型间强相关，防止过拟合，用弱
相关模型
</p>
</div>
</div>

<div id="outline-container-org43bc9a6" class="outline-3">
<h3 id="org43bc9a6">gradient decent</h3>
<div class="outline-text-3" id="text-org43bc9a6">
<p>
当数据点很多是，正则化方法计算量将非常大，此时较多使用梯度下降
</p>

<pre class="example">
sklearn API
</pre>

<div class="org-src-container">
<pre class="src src-python">
<span style="color: #4f97d7; font-weight: bold;">import</span> numpy <span style="color: #4f97d7; font-weight: bold;">as</span> np
<span style="color: #4f97d7; font-weight: bold;">import</span> random
<span style="color: #4f97d7; font-weight: bold;">from</span> sklearn <span style="color: #4f97d7; font-weight: bold;">import</span> linear_model
<span style="color: #7590db;">testsize</span> = 5

<span style="color: #7590db;">x</span> = np.array([a <span style="color: #4f97d7; font-weight: bold;">for</span> a <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(100)])
<span style="color: #7590db;">onesx</span> = np.ones(x.shape)
<span style="color: #7590db;">X</span> = np.c_[x, 2*x, onesx]
<span style="color: #7590db;">y</span> = np.array([a*5 + 20 + random.randint(0,3) <span style="color: #4f97d7; font-weight: bold;">for</span> a <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(100)])
<span style="color: #4f97d7; font-weight: bold;">print</span>(<span style="color: #2d9574;">"the X shape is {}, and y shape is {}"</span>.<span style="color: #4f97d7;">format</span>(X.shape, y.shape))

<span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">Sklearn API</span>
<span style="color: #7590db;">reg</span> = linear_model.LinearRegression()
<span style="color: #7590db;">model</span> = reg.fit(X,y)
<span style="color: #4f97d7; font-weight: bold;">print</span>(<span style="color: #2d9574;">"Sklearn: the weith is {}, and the intercept is {}"</span>.<span style="color: #4f97d7;">format</span>(model.coef_[:-1] ,model.intercept_))
<span style="color: #4f97d7; font-weight: bold;">print</span>(<span style="color: #2d9574;">"the predect of 3 ele is {}"</span>.<span style="color: #4f97d7;">format</span>(model.predict(np.c_[np.arange(testsize), np.arange(testsize),np.ones(testsize)])))


<span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">manual </span>
<span style="color: #4f97d7; font-weight: bold;">def</span> <span style="color: #bc6ec5; font-weight: bold;">featureNormalize</span>(X):
    (m,n) = X.shape
    <span style="color: #7590db;">X_norm</span> = X
    <span style="color: #7590db;">mu</span> = np.zeros(n);
    <span style="color: #7590db;">sigma</span> = np.zeros(n);
    <span style="color: #4f97d7; font-weight: bold;">for</span> i <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(n):
        <span style="color: #7590db;">mu</span>[i] = np.mean(X[:,i])
        <span style="color: #7590db;">sigma</span>[i] = np.std(X[:,i])
        <span style="color: #7590db;">X_norm</span>[:,i] = (X_norm[:,i]-mu[i])/sigma[i]
    <span style="color: #4f97d7; font-weight: bold;">return</span> X_norm
<span style="color: #4f97d7; font-weight: bold;">def</span> <span style="color: #bc6ec5; font-weight: bold;">computeCost</span>(X, y, theta):
    <span style="color: #4f97d7; font-weight: bold;">return</span> np.<span style="color: #4f97d7;">sum</span>((np.dot(X,theta) -y)**2)/(2*<span style="color: #4f97d7;">len</span>(y));

<span style="color: #4f97d7; font-weight: bold;">def</span> <span style="color: #bc6ec5; font-weight: bold;">gradientDescent</span>(X, y, theta, alpha, num_iters):
    <span style="color: #7590db;">m</span> = <span style="color: #4f97d7;">len</span>(y)
    <span style="color: #7590db;">J_history</span> = np.zeros(num_iters);
    <span style="color: #7590db;">theta_len</span> = <span style="color: #4f97d7;">len</span>(theta);
    <span style="color: #4f97d7; font-weight: bold;">for</span> num_iter <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(num_iters):
        <span style="color: #7590db;">theta</span> = theta - (alpha/m)*np.dot(X.T,(np.dot(X,theta).reshape(-1)-y))
        <span style="color: #7590db;">J_history</span>[num_iter] = computeCost(X, y, theta)
    <span style="color: #4f97d7; font-weight: bold;">return</span> theta, J_history

<span style="color: #7590db;">alpha</span> = 0.0001
<span style="color: #7590db;">num_iters</span> = 400000
<span style="color: #7590db;">theta</span> = np.zeros(2+1)
<span style="color: #7590db;">theta</span>, <span style="color: #7590db;">J_history</span> = gradientDescent(X, y, theta, alpha, num_iters)
<span style="color: #4f97d7; font-weight: bold;">print</span>(<span style="color: #2d9574;">"Greadient decent: the weight is {}, and the intercept is {}"</span>.<span style="color: #4f97d7;">format</span>(theta[:-1],theta[-1]))
<span style="color: #4f97d7; font-weight: bold;">print</span>(<span style="color: #2d9574;">"the predect of 3 ele is {}"</span>.<span style="color: #4f97d7;">format</span>(np.dot(np.c_[np.arange(testsize), np.arange(testsize),np.ones(testsize)], theta)))
</pre>
</div>
</div>
</div>

<div id="outline-container-orgdfe2b9a" class="outline-3">
<h3 id="orgdfe2b9a">linear regression</h3>
</div>

<div id="outline-container-org5b7b751" class="outline-3">
<h3 id="org5b7b751">Support Vector Machine</h3>
<div class="outline-text-3" id="text-org5b7b751">
</div>
<div id="outline-container-orga1db531" class="outline-4">
<h4 id="orga1db531">without soft margin</h4>
<div class="outline-text-4" id="text-orga1db531">
<p>
对于点的划分，由decision theory:
\[\vec{w}\vec{u} +c \ge 0\]
距此线一个单位对点标注
\[\vec{w}{x_{+}}+b \ge 1\]
then y = 
\[\vec{w}{x_{-}}+b \le 1\]
then y = -1
So,
\[y(\vec{w}x+b) -1 \ge 0\]
最大化标+点和标-点的距离：
\[D_{max} = (x_{+}-x_{1})\frac{\vec{w}}{||w||} = \frac{2}{||w||}\]
等价于最小化\(\frac{1}{2}||w||^{2}\), 再加上约束条件
\[L= \frac{1}{2}||w||^{2} -\sum
\alpha_{i}[y_{i}(\vec{w}\vec{x}+b)-1]\]
设L对w和b的偏导为0，\(\vec{w} = \sum \alpha_{i}x_{i}y_{i}\),\(\sum
\alpha_{i}y_{i}=0\).
再代回L，\[L=\sum \alpha_{i} - \frac{1}{2} \sum \sum \alpha_{i}
\alpha_{j} y_{i} y_{j}(x_{i}x_{j})\]
</p>
</div>
</div>

<div id="outline-container-org6ea42f0" class="outline-4">
<h4 id="org6ea42f0">with soft margin</h4>
<div class="outline-text-4" id="text-org6ea42f0">
<p>
对于不能绝对线性分割的，可以允许某些点进入空白分割区域（从-1到1的区域）
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">slack variable</td>
<td class="org-left">&epsilon;</td>
<td class="org-left">t<sub>n</sub> y(x<sub>n</sub>) &ge; 1-&epsilon;<sub>n</sub></td>
<td class="org-left">&epsilon; &gt; 0</td>
</tr>

<tr>
<td class="org-left">Controls trade-off between slack and margin</td>
<td class="org-left">C</td>
<td class="org-left">C= &infin;, if misclassified</td>
<td class="org-left">C &sum; &epsilon;<sub>n</sub></td>
</tr>
</tbody>
</table>

<p>
this L satisfied the KKT condition, and can be solved.
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">good classified</td>
<td class="org-left">a = 0</td>
<td class="org-left">&epsilon; = 0</td>
<td class="org-left">C = 0</td>
</tr>

<tr>
<td class="org-left">on the margin</td>
<td class="org-left">a &lt; C</td>
<td class="org-left">&epsilon; = 0</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">violate the margin</td>
<td class="org-left">a = C</td>
<td class="org-left">&epsilon; &gt; 0</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">misclassified</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&epsilon; &gt; 1</td>
<td class="org-left">C = &infin;</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>

<div id="outline-container-orgdde39c7" class="outline-3">
<h3 id="orgdde39c7">Neural network</h3>
<div class="outline-text-3" id="text-orgdde39c7">
</div>
<div id="outline-container-org2abab32" class="outline-4">
<h4 id="org2abab32">Backpropagation</h4>
<div class="outline-text-4" id="text-org2abab32">
</div>
<ul class="org-ul">
<li><a id="org92259d5"></a>感知机<br />
<div class="outline-text-5" id="text-org92259d5">
<p>
对x的向后更正，\(x^{'}= x - \eta \cdot \frac{dy}{dx}\).
对于感知机的传递功能，\(y = w^{T}x + b\).
由于感知机没有激活函数，所以直接对\[\mathcal{L} = \frac{1}{n}
\sum^{n}_{i=1}(w\cdot x^{i} +b -y^{i})^{2}\].
\[ \frac{\partial \mathcal{L}}{\partial w} = \frac{2}{n}
\sum^{n}_{i=1}(wx^{i}+b-y^{i})x^{i}\]
\[ \frac{\partial \mathcal{L}}{\partial b}= \frac{2}{n}\sum^{n}_{i=1}(wx^{i}+b -y^{i})\]
</p>
</div>
</li>

<li><a id="org28a98cd"></a>多层神经网络<br />
<div class="outline-text-5" id="text-org28a98cd">
<p>
而对于多层神经网络，\(z = w^{T}x + b\), \(\frac{\partial z}{\partial w} =x\),  \(\frac{\partial z}{\partial b} = 1\).
每层之间具有激活函数, \(\sigma(z) = \frac{1}{1-e^{-z}}\),\(\frac{\partial \sigma(x)}{\partial x} = \sigma (1-\sigma)\).
损失函数, \(\mathcal{L} = \frac{1}{2}(\sigma - y^{i})^{2}\), \[\frac{\partial \mathcal{L}}{\partial \sigma} = (\sigma -y^{i})\]
</p>


<p>
\[\frac{\partial \mathcal{L}}{\partial w} = \frac{\partial
\mathcal{L}}{\partial \sigma} \cdot \frac{\partial \sigma }{\partial z} \cdot
\frac{\partial z}{\partial w}\]
</p>


<p>
\(\frac{\partial \mathcal{L}}{\partial w} = (\sigma -
y)\sigma(1-\sigma) \cdot x\)
</p>

<p>
\[\frac{\partial \mathcal{L}}{\partial b} = \frac{\partial
\mathcal{L}}{\partial \sigma} \cdot \frac{\partial \sigma }{\partial z} \cdot
\frac{\partial z}{\partial b}\]
</p>

<p>
\(\frac{\partial  \mathcal{L}}{\partial b} = (\sigma -
y)\sigma(1-\sigma)\)
</p>

<p>
如果对于多层神经网络，则需要逐层计算，其中\(\frac{\partial
\mathcal{L}}{\partial w}\) 中的w就是相应层的权重，由最后的
L逐步回推到w。
</p>
</div>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgcb8456b" class="outline-3">
<h3 id="orgcb8456b">Conversational neural Network</h3>
</div>

<div id="outline-container-org7983b86" class="outline-3">
<h3 id="org7983b86">linear Discriminate Analysis</h3>
<div class="outline-text-3" id="text-org7983b86">
</div>
<div id="outline-container-org4e387fa" class="outline-4">
<h4 id="org4e387fa">Fisher's linear discriminant</h4>
<div class="outline-text-4" id="text-org4e387fa">
<p>
输入为j=0，1类样本，每类分别 \(N_{j}\) 个样本
\(\mu_j = \frac{1}{N_{j}} \sum x\) \(x \in N_{j}\)
\(\Sigma_{j} = \sum(x-\mu_{j})(x-\mu_{j})^{T}\), \(x \in N_{j}\)
</p>

<p>
\(argmax(J) = \frac{\omega^{T} (\mu_0-\mu_1)(\mu_0-\mu_1)^T
\omega}{\omega^T(\Sigma_0+\Sigma_1)\omega } =  \frac{\omega^{T} S_{b}
\omega}{\omega^T S_{w} \omega }\)
</p>
</div>
</div>
<div id="outline-container-orga61f55c" class="outline-4">
<h4 id="orga61f55c">Fisher's linear discriminant with Kernel method</h4>
<div class="outline-text-4" id="text-orga61f55c">
<p>
\[ J(w) = \frac{(m_{2}-m_{1})^{2}}{s_{1}^{2} + s_{2}^{2}} = 
  \frac{w^{T}(m_{2}-m_{1})^{T}(m_{2}-m_{1}) w}{ w^{T}(s_{1}^{2} +
  s_{2}^{2})w}\]
</p>

<p>
\[ w = \sum^{L}_{k=1} \alpha_{k} \phi(x_{k}) \]
</p>

<p>
\[ m = \frac{1}{L_{i}} \sum^{Li}_{n=1}\phi(x_{n}^{i})\]
</p>

<p>
\[ w^{T} m_{i} = \alpha^{T}M_{i}\]
</p>

<p>
\[ M_{i} = \frac{1}{L_{i}}\sum^{L}_{k=1}\sum^{L_{i}}_{n=1}
k(x_{k},x_{n}^{i})\]
</p>

<p>
Numerator:\[w^{t}S_{B}w = \alpha^{T}M\alpha\]
Denominator:
\[ w^{T}S_{w}w = \alpha^{T} N \alpha\]
\[ N = \sum_{i=1,2}K_{i}(I-1/L)K_{i}^{T} \]
\[ (K_{i})_{n,m} = k(x_{n}, x_{m}^{i})\]
</p>
</div>
</div>

<div id="outline-container-org29ea8e0" class="outline-4">
<h4 id="org29ea8e0">Probabilistic Generative Model</h4>
<div class="outline-text-4" id="text-org29ea8e0">
<p>
用贝叶斯定理求出每个可能的概率，再取最大的值
</p>
<pre class="example">
one two class case
</pre>
<p>
\[ P(C_{1}|x) = \frac{P(C_{1}|x)P(C_{1})}{P(C_{1}|x)P(C_{1}) +
P(C_{2}|x)P(C_{2})} = \frac{1}{1+exp(log
\frac{P(C_{1}|x)P(C_{1})}{P(C_{2}|x)P(C_{2})} )}\]
即可以 Logistic sigmoid 函数求解
</p>

<pre class="example">
multi class case
</pre>

<p>
\[P(C_{k}|x) = \frac{P(x|C_{k})P(C_{k})}{\sum_{j} P(x|C_{j})P(C_{j})}\]
</p>

<p>
即可以用 Softmax funtion 来求解
</p>
</div>
</div>

<div id="outline-container-orge2e4a8e" class="outline-4">
<h4 id="orge2e4a8e">Probabilistic Discriminant Model</h4>
<div class="outline-text-4" id="text-orge2e4a8e">
<p>
Better predictive performance if assumptions about class-conditional
distributions not correct.
</p>

<p>
和 generative model 一样求解，同样也有二分和多分类，但是该类问题设为
logical regression, See logical regression
</p>
</div>
</div>
</div>

<div id="outline-container-org1c7b934" class="outline-3">
<h3 id="org1c7b934">logistic regression</h3>
<div class="outline-text-3" id="text-org1c7b934">
<p>
用 logical sigmoid function 来作二分类判断，检验概率是否过半
</p>
</div>
</div>

<div id="outline-container-orgd1a04b7" class="outline-3">
<h3 id="orgd1a04b7">Principe Component Analysis</h3>
<div class="outline-text-3" id="text-orgd1a04b7">
</div>
<div id="outline-container-orgb018716" class="outline-4">
<h4 id="orgb018716">PCA Algorithms</h4>
<div class="outline-text-4" id="text-orgb018716">
<p>
将原来的数据坐标进行线性组合，组成新的坐标基底，让数据在新基底
上投影最小化，以去除，压缩该些维度
</p>
<ol class="org-ol">
<li>将数据中心化</li>
<li>求出数据在所有特性的协方差矩阵</li>
<li>如果矩阵是方阵，则可以直接特征值分解</li>
<li>如果矩阵不是方阵，则先乘以转置，再特征值分解，注意此时求得特征值要开方</li>
<li>如果不是方阵，也可以直接奇异值分解</li>
<li>取出前面的需要的维度，多余的被压缩了</li>
</ol>
</div>
</div>
<div id="outline-container-org634ea2f" class="outline-4">
<h4 id="org634ea2f">Probabilistic generative model for PCA</h4>
<div class="outline-text-4" id="text-org634ea2f">
<p>
State the probabilistic generative model underlying Probabilistic PCA
with a K-dimensional latent space and observations \(x\in R^{D}\) . Define
all three random variables and their distribution.
</p>

<p>
Hidden Variable z in K-dimension from probabilistic generative PCA:
we can transfer z into standard gaussian distribution,
\[p(\vec{z}) = N(0, I), \vec{z} \in R^{K}, \vec{z} \sim N(0, I)\]
</p>

<p>
observation variable x in D-dimension giving z:
\[p(\vec{x}|\vec{z}) = N(\vec{W}\vec{z} + u, \sigma^{2}I), \vec{x} \in
R^{D}\]
\[\vec{x} = Wz + u + \epsilon, \epsilon \sim N(0, \sigma^{2}I)\]
</p>

<p>
So, \(p(x) = \int p(x|z)p(z)dz\)
\[E(x) = E(Z + u + \epsilon) = u\]
\[Cove[x] = E[(Wz + u + \epsilon)(Wz + u + \epsilon)^{T}]
= E(W^{T}W) + E(\epsilon \epsilon^{T}) = WW^{T} + \sigma^{2}I\]
</p>

<p>
\[ x \sim N(u, Cov[x])\]
</p>
</div>
</div>
</div>


<div id="outline-container-org0de2bf5" class="outline-3">
<h3 id="org0de2bf5">K-Means</h3>
<div class="outline-text-3" id="text-org0de2bf5">
<p>
输入样本集 D: \(x_{1}, x_{1}, x_{2},,,x_{m}\)
聚类数 k, 
最大迭代数 N,
期望输出: \(C_{1}, C_{2},,,C_{k}\)
</p>

<p>
随机初始化k个聚类中心，并作不同类别的标记
for i= 1,2,..N:
    初始化所有C
    计算每个点到每个中心的距离，并被最小距离的聚类中心标记
    对于所有相同标记的聚类更新中心，再重复上一步骤，直到没有变化为止
</p>


<div class="org-src-container">
<pre class="src src-python"><span style="color: #4f97d7; font-weight: bold;">import</span> random
<span style="color: #4f97d7; font-weight: bold;">import</span> numpy <span style="color: #4f97d7; font-weight: bold;">as</span> np
<span style="color: #4f97d7; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #4f97d7; font-weight: bold;">as</span> plt

<span style="color: #7590db;">b</span> = []
<span style="color: #4f97d7; font-weight: bold;">for</span> i <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(100):
    <span style="color: #7590db;">a</span> = np.array(<span style="color: #4f97d7;">list</span>([(20,50),(30,10),(60,30)]))
    <span style="color: #4f97d7; font-weight: bold;">for</span> j <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(a.shape[0]):
        <span style="color: #4f97d7; font-weight: bold;">for</span> k <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(a.shape[1]):
            a[j][k] += random.randint(0,30)
            b.append(a[j])

<span style="color: #7590db;">b</span> = np.array(b)
plt.plot(b[:,0], b[:,1], <span style="color: #2d9574;">'ro'</span>)
plt.title(<span style="color: #2d9574;">"toy data"</span>)
plt.show()


<span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">sklearn API</span>
<span style="color: #4f97d7; font-weight: bold;">from</span> sklearn.cluster <span style="color: #4f97d7; font-weight: bold;">import</span> KMeans
<span style="color: #7590db;">y_pred</span> = KMeans(n_clusters=3, random_state=9).fit_predict(b)
plt.scatter(b[:, 0], b[:, 1], c=y_pred)
plt.title(<span style="color: #2d9574;">"toy data with sklearn API"</span>)
plt.show()

<span style="color: #2aa1ae; background-color: #292e34;"># </span><span style="color: #2aa1ae; background-color: #292e34;">manual</span>
<span style="color: #4f97d7; font-weight: bold;">def</span> <span style="color: #bc6ec5; font-weight: bold;">findClosestCentroids</span>(X, centroids):
    <span style="color: #7590db;">distance</span> = np.zeros((<span style="color: #4f97d7;">len</span>(X),<span style="color: #4f97d7;">len</span>(centroids)))
    <span style="color: #4f97d7; font-weight: bold;">for</span> i <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(<span style="color: #4f97d7;">len</span>(X)):
        <span style="color: #4f97d7; font-weight: bold;">for</span> j <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(<span style="color: #4f97d7;">len</span>(centroids)):
            <span style="color: #7590db;">distance</span>[i,j] = np.linalg.norm(X[i,:]-centroids[j,:])
    <span style="color: #4f97d7; font-weight: bold;">return</span> np.argmin(distance,axis=1)

<span style="color: #4f97d7; font-weight: bold;">def</span> <span style="color: #bc6ec5; font-weight: bold;">computeCentroids</span>(X, idx, K):
    <span style="color: #7590db;">centroids</span> = np.zeros((K,X.shape[1]))
    <span style="color: #4f97d7; font-weight: bold;">for</span> i <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(K):
        <span style="color: #7590db;">centroids</span>[i,:] = np.mean(X[idx == i],axis = 0)
    <span style="color: #4f97d7; font-weight: bold;">return</span> centroids


<span style="color: #4f97d7; font-weight: bold;">def</span> <span style="color: #bc6ec5; font-weight: bold;">runkMeans</span>(X,K,max_iters):
    <span style="color: #7590db;">indexs</span> = np.random.choice(np.array(<span style="color: #4f97d7;">range</span>(<span style="color: #4f97d7;">len</span>(X))), K,replace=<span style="color: #a45bad;">False</span>)
    <span style="color: #7590db;">centroids</span> = X[indexs]
    <span style="color: #4f97d7; font-weight: bold;">for</span> max_iter <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(max_iters):
        <span style="color: #7590db;">idx</span> = findClosestCentroids(X, centroids)
        <span style="color: #7590db;">centroids</span> = computeCentroids(X, idx, K)
        <span style="color: #7590db;">colors</span> = [<span style="color: #2d9574;">''</span>,<span style="color: #2d9574;">''</span>,<span style="color: #2d9574;">''</span>]
        <span style="color: #4f97d7; font-weight: bold;">for</span> i <span style="color: #4f97d7; font-weight: bold;">in</span> <span style="color: #4f97d7;">range</span>(K):
            plt.scatter(X[idx==i, 0], X[idx==i, 1])
        plt.scatter(centroids[:, 0], centroids[:, 1], c=<span style="color: #2d9574;">'r'</span>)
        plt.title(<span style="color: #2d9574;">"toy data with manual {} time"</span>.<span style="color: #4f97d7;">format</span>(max_iter))
        plt.show()
<span style="color: #7590db;">K</span> = 3
<span style="color: #7590db;">max_iters</span> = 3
runkMeans(b,K,max_iters)

</pre>
</div>
</div>
</div>

<div id="outline-container-org89538e5" class="outline-3">
<h3 id="org89538e5">EM algorithms</h3>
<div class="outline-text-3" id="text-org89538e5">
<p>
E step: compute responsibilites \(\gamma_{nk}\) given current \(\pi_{k},
\mu_{k}, \Sigma_{k}\)
\[ \gamma_{nk} = \frac{\pi_{k} N(x|\mu_{k}, \Sigma_{k}}{ 
\sum_{k=1}^{K}\pi_{k}N(x|\mu_{k},\Sigma_{k})}\]
</p>

<p>
M step: update  \(\pi_{k},\mu_{k}, \Sigma_{k}\) given \(\gamma_{nk}\).
according to the derivative of \(log p(x|\pi, \mu, \Sigma) =
\sum^{N}_{n=1}log \sum^{K}_{k=1} \pi N(x_{k}|\mu_{k}, \Sigma_{k})\)
with respect to the \(\pi_{k},\mu_{k}, \Sigma_{k}\),
</p>

<p>
cluster means: \[\mu_{k} = \frac{1}{N_{k}} \sum^{N}_{n=1} \gamma_{nk} x_{n}\]
</p>

<p>
cluster covariances: \[\Sigma_{k} =
\frac{1}{N_{k}}\sum^{N}_{n=1}\gamma_{nk}(x_{n}-\mu_{k})(x_{n}-\mu_{k})^{T}\]
</p>

<p>
cluster priors:\[\pi_{k} = \frac{N_{k}}{N}\]
</p>
</div>
</div>
</div>


<div id="outline-container-org5e35e58" class="outline-2">
<h2 id="org5e35e58">link</h2>
<div class="outline-text-2" id="text-org5e35e58">
<p>
机器学习代码和课件可在此下 <sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
<a href="https://jia666-my.sharepoint.com/:f:/g/personal/hmp121_xkx_me/EsaG70R5NXdMgC7V1aYwyQcB5C-8nFiSYTWG3Uur9ZvbEA?e=1D2uPf">https://jia666-my.sharepoint.com/:f:/g/personal/hmp121_xkx_me/EsaG70R5NXdMgC7V1aYwyQcB5C-8nFiSYTWG3Uur9ZvbEA?e=1D2uPf</a>
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: sx</p>
<p class="date">Created: 2020-07-31 Fr 12:57</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
