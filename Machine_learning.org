#+TITLE: Machine Learning
#+OPTIONS: num:nil
#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup
#+STARTUP:  content
* Math for ML
** Bayes's Rule
if x and y are independent:
$$ p(x,y) =  p(y)p(x) = p(x) p(y) = p(y,x) $$ 

#+begin_src 
条件概率：
条件概率 = 联合概率/边缘概率
先验概率和后验概率都是条件概率，但是条件已知是先验
#+end_src
$$ P(y|x) = \frac{ P(x,y)}{P(x)}$$
#+begin_src 
全概率公式
#+end_src
$$ p(y) = \sum_{i=1}^{n} p(y,x_{i}) $$
#+begin_src 
贝叶斯公式
#+end_src

$$ P(AB)=P(BA) $$
$$ P(y|x)P(x) = P(x|y)P(y)$$
$$ P(y|x) = \frac{ P(x|y) P(y)}{P(x)}$$

#+begin_src 
贝叶斯公式 + 全概率公式 + 条件概率 
#+end_src

\begin{eqnarray*}
P(A|B) &= \frac{P(B|A)P(A)}{P(B)} \\
       &= \frac{P(B|A)P(A)}{\sum_{i=1}^{n} p(B,A_{i})} \\
       &= \frac{P(B|A)P(A)}{\sum_{i=1}^{n} P(B|A_{i})P(A)} \\
\end{eqnarray*}

** Kovarianz Matrix
seeing link : [[https://scofild429.github.io/subjects/statistik.html][Statistik web]]
org link : [[~/Dropbox/subjects/statistik.org][Startistik org]]

for i = {1...n}, $x_{i}$ is a random variable, which belong to
Gaussian distribution

set 
 $$ X = \left( \begin{aligned}  x_{1} \\ x_{2}\\ . \\. \\x_{n}  \end{aligned}\right) $$

$$ \bar{X} = \left( \begin{aligned}  \bar{x}_{1}
\\ \bar{x}_{2}\\ . \\. \\ \bar{x}_{n}  \end{aligned} \right) $$

co-variance matrix $\Sigma = E [(X-\bar{X})(X-\bar{X})^{T} ]$

\begin{equation}
\Sigma = 
  \left(
  \begin{array}{c}
          x_{1}-\bar{x}_{1} \\
          x_{2}-\bar{x}_{2} \\
          x_{3}-\bar{x}_{3} \\
          ..                \\
          x_{n}-\bar{x}_{n} 
 \end{array}
 \right)
  \left(
  \begin{array}{ccccc}
          x_{1}-\bar{x}_{1} &
          x_{2}-\bar{x}_{2} &
          x_{3}-\bar{x}_{3} &
          ..                &
          x_{n}-\bar{x}_{n} 
  \end{array}
  \right)
\end{equation}
对角线上是对应元素的方差，其他是相对于两个元素的协方差

** Regularization
$$ Loss = \frac{1}{2}\sum^{N}_{i=1}(y_{i}-w^{t}\phi(x_{i}))^{2}+\frac{\lambda}{2}\sum^{M}_{j=1}|w_{j}|^{q} $$

|--------------------------------------------+---------------------------------------------------------|
| N                                          | example number                                          |
| M                                          | Eigenschaften Number, Diemension number                 |
| L0                                         | 控制网络中的非零权重                                    |
| L1                                         | 网络中的所有元素的绝对值之和,促使网络生成更多的稀疏矩阵 |
| L2                                         | 网络中的所有元素平方和,促使网络生成小比重的权值         |
| w                                          | w_{1}, w_{2}                                            |
| q=1                                        | l1 regularization                                       |
| q=2                                        | l2 regularization                                       |
| \lambda                                    | learning rate                                           |
| \sum^{N}_{i=1}(y_{i}-w^{t}\phi(x_{i}))^{2} | 依据w的同心圆                                           |
| \sum^{M}_{j=1}w_{j}^{q}                    | q=1, 菱形， q=2, 圆形                                   |
|                                            |                                                         |
|                                            |                                                         |

Loss 要最小，part1 刚好和 part2 相接，l1会在坐标轴上，所以如果有较小分
量，会被直接设为0

** Error Bias-Variance-trade-off
#+begin_src
Error = Bias + Variance + noise
#+end_src

|----------+------+----------+--------------------------+---|
| Bias     | 偏差 | 欠拟合   | 发挥，观测等主观因素影响 |   |
| Variance | 方差 | 过过拟合 | 稳定性，模型的构建决定   |   |
| noise    | 噪音 |          | 统难度                   |   |
|----------+------+----------+--------------------------+---|

** Multi variable Gaussian distribution
seeing the link 知乎  [[https://zhuanlan.zhihu.com/p/58987388][zhihu link]]

$$
{\displaystyle f_{\mathbf {X} }(x_{1},\ldots ,x_{k})={\frac {\exp
\left(-{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm
{T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu
}})\right)}{\sqrt {(2\pi )^{k}|{\boldsymbol {\Sigma
}}|}}}}
$$

#+begin_src python

def gaussian(x,mean,cov):
    dim = np.shape(cov)[0] #维度
    #之所以加入单位矩阵是为了防止行列式为0的情况
    covdet = np.linalg.det(cov+np.eye(dim)*0.01) #协方差矩阵的行列式
    covinv = np.linalg.inv(cov+np.eye(dim)*0.01) #协方差矩阵的逆
    xdiff = x - mean
    #概率密度
    prob = 1.0/np.power(2*np.pi,1.0*dim/2)/np.sqrt(np.abs(covdet))*np.exp(-1.0/2*np.dot(np.dot(xdiff,covinv),xdiff))
    return prob

#+end_src

** Mahalanobis distance 

$$ \Delta = \left(-{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm
{T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu
}})\right)
$$
$$ \Sigma = \sum U \Lambda U^{T} $$
$$ \Sigma^{-1} = \sum U \Lambda^{-1} U^{T} $$

$$ \Delta = -{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm
{T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu
}}) = -{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm
{T} }  U \Lambda^{-1} U^{T}({\mathbf {x} }-{\boldsymbol {\mu
}})
$$
马氏距离所使用的变换 : $$ Z = U^{T}(X - \mu) $$,


$$ D = \sqrt{ZZ^{T}} $$
关于新的坐标，U 是变换的旋转，$\Lambda$ 是基底的延伸，$(x-\mu)$ 是在其
上的投影，此后，在新坐标上，即为多变量，标准，不相关高斯分布

** K-fold Cross Validation
|---------+---------------------------------------------|
| N       | total examples                              |
| K       | number of sub-fold                          |
| m       | number of each sub-fold                     |
| big K   | small bias, with over fitting, big variance |
| small K | big bias, without fitting, low variance     |
|---------+---------------------------------------------|

** precision  recall

|-----+---------------+-------------------------------------------------+-------------------------------------------------|
| TPR | recall 查全 R | predict positive 测正 &  实正 actually positive | predict negative 测反 & 实正  actually positive |
| FPR |               | predict positive 测正 &  实反 actually negative | predict negative  测反 & 实反 actually negative |
|-----+---------------+-------------------------------------------------+-------------------------------------------------|
|     |               | precision 查准 P                                |                                                 |

R-P Couve : R P
ROC couve : FPR TPR

$$ F_{1} = \frac{1}{2} \cdot (\frac{1}{P}+\frac{1}{R})$$
$$ F_{\beta} = \frac{1}{\beta^{2}} \cdot
(\frac{\beta^{2}}{P}+\frac{1}{R})$$

关联性属性 ： 高中低 （3,2,1）
非关联性属性： 猪狗羊 （(1,0,0), (0,1,0),(0,0,1)）


** Gini
吉尼系数
$$ Gini(D) = \sum^{|y|}_{k=1}\sum_{k^{'}\not = k} p_{k}p_{k^{'}} = 1- \sum^{|y|}_{k=1}p_{k}^{2}$$


* activation function
** no activation
输出为实数空间或某个区间， 连续变化。直接有输出值和真实值比较
** Sigmoid
$$ Sigmoid(x) = \frac{1}{1+e^{-x}}$$

导数：$$ \sigma'(x) = \sigma(x)(1-\sigma(x))$$

** ReLU
$$ Relu(x) = 
\begin{cases}
x&  x >=0 \\
0&  x < 0
\end{cases}$$

$$ Relu(x) = max(0,x)$$

** LeakyReLU

$$ LeakyReLU(x) = \begin{cases}
x& x >=0 \\
px& x <0, 0<p<<1
\end{cases}$$

** Tanh
$$ tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$
$$ tanh(x) = 2.sigmoid(2x)-1$$
导数：$$\tanh'(x) = 1- \tanh^{2}(x)$$

** MSE
$$ \mathcal{L} = MSE(y, o) =
\frac{1}{d_{out}}\sum_{i=1}^{d_{out}}(y_{i}-o^{i})^{2}$$
导数 ：$$ \frac{\partial \mathcal{L}}{\partial o_{i}}= (o_{i}-y_{i})$$

** inf entropy 
$$ H(p) = -\sum_{i}P(i)\log_{2}^{P_{i}}$$
** softmax
$$ p_{z_{i}} = \frac{e^{z_{i}}}{\sum_{j}e^{z_{j}}}$$
所有种类的概率之和为1
导数:
$$ \frac{ \partial p_{z_{i}}}{\partial z_{j}} = 
\begin{cases}
p_{i}(1-p_{j}) &  if i =j \\
-p_{i}p_{j}    & if \neq j
\end{cases}$$

** cross entropy
在计算交叉熵时， 一般是和 softmax 函数一起使用的
$$ H(p||q) = -\sum_{i} p(i) \log_{2}^{q_{i}}$$
$$H(p||q) = H(p) + D_{KL}(p||q)$$

for One-hot coding
$$ H(p||q) = D_{KL}(p||q) = \sum_{i}y_{i}log(\frac{y_{j}}{o_{j}}) = 
1 \cdot \log\frac{1}{o_{i}} + \sum_{j!=i}0 \cdot \log \frac{0}{o_{j}}
= -\log o_{i}$$
o_i 为1 时，预测正确，交叉熵为0。
导数：
$$ \mathcal{L} = -\sum_{k}y_{k}\log(p_{k})$$
$$\begin{aligned}
 \frac{\partial \mathcal{L}}{\partial z_{i}} & =
-\sum_{k} y_{k} \frac{\partial \log(p_{k})}{\partial z_{i}} \\
&= -\sum_{k} y_{k} \frac{\partial \log(p_{k})}{\partial p_{k}} \cdot
\frac{\partial p_{k}}{\partial z_{i}} \\
&= -\sum_{k} y_{k} \frac{1}{\partial p_{k}} \cdot
\frac{\partial p_{k}}{\partial z_{i}} \\
\end{aligned}
$$

用上面 softmax 的导数结果，分为k=i 和k!=i两种情况
$$ \frac{\partial \mathcal{L}}{z_{i}}=p_{i}-y_{i}$$


* Algorithms
** Ordinary Least Squares(OLS)
*** 正规化方程
正则化方程的推导，用高斯分布的多变量分布的Maxisum likelihood,能一起求得对weight和bias值 : 

但是要添加一列1到 train 和 test，至于在前面还是后面有点怪异。

目前认为，在后面的话，多变量和参数可以按需求访问

Loss function:
$$ J = \frac{1}{2m}\sum(h_{\theta}(x_{i})-y_{i})^{2}$$
$$ \sigma = \frac{1}{2m}(X \theta -y)^{T} (X \theta -y)$$
对$\theta$ 求导，并令其为0，
$$\theta = (X^{T}X)^{-1}X^{T}y $$
但是要求$X^{T}X$ 必须可逆。

*** 正则化正规化方程
$$w = (\Phi^{T}\Phi + \lambda I)^{-1} \Phi^{T}y $$


#+begin_src python :results output
  import numpy as np
  import random
  from sklearn import linear_model
  testsize = 5

  x = np.array([a for a in range(100)])
  onesx = np.ones(x.shape)
  X = np.c_[x,onesx]
  y = np.array([a*5 + 20 + random.randint(0,3) for a in range(100)])
  print("the X shape is {}, and y shape is {}".format(X.shape, y.shape))

  # weight = np.dot(np.dot(np.linalg.pinv(np.dot(X.T, X)), X.T), y)
  weight = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

  print("OLS : the weight is{}, and the bais is {} ".format(weight[:-1], weight[-1]))
  print("the predect of 5 ele is {}".format(np.dot(np.c_[np.arange(testsize),np.ones(testsize)], weight)))


#+end_src

#+RESULTS:
: the X shape is (100, 2), and y shape is (100,)
: OLS : the weight is[5.00063606], and the bais is 21.508514851485195 
: the predect of 5 ele is [21.50851485 26.50915092 31.50978698 36.51042304 41.51105911]

#+begin_src 
也可以是对变量 with multi variables
#+end_src

#+begin_src python :results output
  import numpy as np
  import random
  from sklearn import linear_model
  testsize = 5

  x = np.array([a for a in range(100)])
  onesx = np.ones(x.shape)
  X = np.c_[onesx, x, 2*x]
  y = np.array([a*5 + 20 + random.randint(0,3) for a in range(100)])
  print("the X shape is {}, and y shape is {}".format(X.shape, y.shape))

  # ordinary  least squares (正规化方法）
  weight = np.dot(np.dot(np.linalg.pinv(np.dot(X.T, X)), X.T), y)
  print("OLS : the weight is{}, and the bais is {} ".format(weight[:-1], weight[-1]))
  print("the predect of 5 ele is {}".format(np.dot(np.c_[np.arange(testsize), np.arange(testsize),np.ones(testsize)], weight)))


#+end_src

#+RESULTS:
: the X shape is (100, 3), and y shape is (100,)
: OLS : the weight is[21.35089109  0.9999964 ], and the bais is 1.999992799280042 
: the predect of 5 ele is [ 1.9999928  24.35088029 46.70176778 69.05265527 91.40354275]

** 朴素贝叶斯 
假设各个属性完全独立
要判断某示例的分类，分别计算出该示例在每个分类中的概论乘以该类别中该示
例的各种属性出现的条件概论， 谁大选谁。（注意样本不足引起的某属性的条
件为零）

** Decision List

(f1,v1),(f2,v2)....(fr,vr)
fi is a term in CNF, vi belongs {0,1}, and the last term fr is always
true. and each term can be viewed as if else extended. if fi is
matched, so vi is its value.

#+BEGIN_SRC 
for 0<k<n, k-CNF and k-DNF are proper 


#+END_SRC

** Decision tree
*** 方法描述和变量
在训练集内以最大信息增益来确定划分属性，在各个子区内再重复剩下的属性

信息熵
$$ Ent(D)=-\sum^{y}_{k=1}p_{k}log_{2}^{p_{k}}$$

信息熵增益
$$ Gain(D,a) = Ent(D)-\sum^{V}_{v=1}\frac{|D^{v}|}{|D|}Ent(D^{v}) $$

吉尼系数
$$ Gini(D) = \sum^{|y|}_{k=1}\sum_{k^{'}\not = k} p_{k}p_{k^{'}} = 1- \sum^{|y|}_{k=1}p_{k}^{2}$$

*** 预剪枝和后剪枝
在利用训练集的最大信息增益确定划分属性后，用验证集来检验划分，如果验证
集的信息熵增加，（泛化结果不好)否定此次划分，设为叶节点

后剪枝是在这个树完成后，用验证集去检验每一个内节点，从下到上，如果去掉
该划分有更小的信息熵，则废除该划分。

*** 属性有连续值和缺失值
连续值离散化：排列该属性的所有取值n个，在n-1个区间中去中间值为离散值，
遍历所有离散值，找到最大信息增益的离散值，作二分。

缺失值，取出该属性的非缺失子集，再配以相应的比率计算信息增益，处理和以
前一样。如果选出的划分属性有缺失值，则给划分不作用到缺失样本，复制到每
个划分子集
*** 多变量决策
每个划分属性是多个属性（变量）的线性组合

** Bagging
多次放回抽样，用不同抽样的数据集在多棵树上并行计算，
|-----------+---------------+--------------------------|
| More Tree | Bias remained | Variance reduce to limit |
所以刚开始选择偏差小，方差大的强模型

** Random Forest
Random Forest = Decision Tree + Bagging + random Eigenschaften

|------------------+-------------|
| More deeper      | Bias reduce |
| More Eigenschaft | Bias reduce |
|------------------+-------------|
** Boosting
固定数据集，在多个串行的模型上顺序计算，模型间强相关，防止过拟合，用弱
相关模型

** gradient decent
当数据点很多是，正则化方法计算量将非常大，此时较多使用梯度下降

#+begin_src 
sklearn API
#+end_src

#+begin_src python :results output

  import numpy as np
  import random
  from sklearn import linear_model
  testsize = 5

  x = np.array([a for a in range(100)])
  onesx = np.ones(x.shape)
  X = np.c_[x, 2*x, onesx]
  y = np.array([a*5 + 20 + random.randint(0,3) for a in range(100)])
  print("the X shape is {}, and y shape is {}".format(X.shape, y.shape))

  # Sklearn API
  reg = linear_model.LinearRegression()
  model = reg.fit(X,y)
  print("Sklearn: the weith is {}, and the intercept is {}".format(model.coef_[:-1] ,model.intercept_))
  print("the predect of 3 ele is {}".format(model.predict(np.c_[np.arange(testsize), np.arange(testsize),np.ones(testsize)])))


  # manual 
  def featureNormalize(X):
      (m,n) = X.shape
      X_norm = X
      mu = np.zeros(n);
      sigma = np.zeros(n);
      for i in range(n):
          mu[i] = np.mean(X[:,i])
          sigma[i] = np.std(X[:,i])
          X_norm[:,i] = (X_norm[:,i]-mu[i])/sigma[i]
      return X_norm
  def computeCost(X, y, theta):
      return np.sum((np.dot(X,theta) -y)**2)/(2*len(y));

  def gradientDescent(X, y, theta, alpha, num_iters):
      m = len(y)
      J_history = np.zeros(num_iters);
      theta_len = len(theta);
      for num_iter in range(num_iters):
          theta = theta - (alpha/m)*np.dot(X.T,(np.dot(X,theta).reshape(-1)-y))
          J_history[num_iter] = computeCost(X, y, theta)
      return theta, J_history

  alpha = 0.0001
  num_iters = 400000
  theta = np.zeros(2+1)
  theta, J_history = gradientDescent(X, y, theta, alpha, num_iters)
  print("Greadient decent: the weight is {}, and the intercept is {}".format(theta[:-1],theta[-1]))
  print("the predect of 3 ele is {}".format(np.dot(np.c_[np.arange(testsize), np.arange(testsize),np.ones(testsize)], theta)))
#+end_src

#+RESULTS:
: the X shape is (100, 3), and y shape is (100,)
: Sklearn: the weith is [0.99925113 1.99850225], and the intercept is 21.64534653465344
: the predect of 3 ele is [21.64534653 24.64309991 27.64085329 30.63860666 33.63636004]
: Greadient decent: the weight is [0.99925367 1.99850734], and the intercept is 21.64450170230179
: the predect of 3 ele is [21.6445017  24.64226272 27.64002374 30.63778475 33.63554577]

** linear regression

** Support Vector Machine
*** without soft margin
对于点的划分，由decision theory:
$$\vec{w}\vec{u} +c \ge 0$$
距此线一个单位对点标注
$$\vec{w}{x_{+}}+b \ge 1$$
then y = 
$$\vec{w}{x_{-}}+b \le 1$$
then y = -1
So,
$$y(\vec{w}x+b) -1 \ge 0$$
最大化标+点和标-点的距离：
$$D_{max} = (x_{+}-x_{1})\frac{\vec{w}}{||w||} = \frac{2}{||w||}$$
等价于最小化$\frac{1}{2}||w||^{2}$, 再加上约束条件
$$L= \frac{1}{2}||w||^{2} -\sum
\alpha_{i}[y_{i}(\vec{w}\vec{x}+b)-1]$$
设L对w和b的偏导为0，$\vec{w} = \sum \alpha_{i}x_{i}y_{i}$,$\sum
\alpha_{i}y_{i}=0$.
再代回L，$$L=\sum \alpha_{i} - \frac{1}{2} \sum \sum \alpha_{i}
\alpha_{j} y_{i} y_{j}(x_{i}x_{j})$$

*** with soft margin
对于不能绝对线性分割的，可以允许某些点进入空白分割区域（从-1到1的区域）
| slack variable                              | \epsilon | t_n y(x_n) \ge 1-\epsilon_n | \epsilon > 0      |
| Controls trade-off between slack and margin | C        | C= \infty, if misclassified    | C \sum \epsilon_n |

this L satisfied the KKT condition, and can be solved.
| good classified    | a = 0 | \epsilon = 0 | C = 0      |
| on the margin      | a < C | \epsilon = 0 |            |
| violate the margin | a = C | \epsilon > 0 |            |
| misclassified      |       | \epsilon > 1 | C = \infty |

** Neural network
*** Backpropagation
**** 感知机
对x的向后更正，$x^{'}= x - \eta \cdot \frac{dy}{dx}$.
对于感知机的传递功能，$y = w^{T}x + b$.
由于感知机没有激活函数，所以直接对$$\mathcal{L} = \frac{1}{n}
\sum^{n}_{i=1}(w\cdot x^{i} +b -y^{i})^{2}$$.
$$ \frac{\partial \mathcal{L}}{\partial w} = \frac{2}{n}
\sum^{n}_{i=1}(wx^{i}+b-y^{i})x^{i}$$
$$ \frac{\partial \mathcal{L}}{\partial b}= \frac{2}{n}\sum^{n}_{i=1}(wx^{i}+b -y^{i})$$

**** 多层神经网络

而对于多层神经网络，$z = w^{T}x + b$, $\frac{\partial z}{\partial w} =x$,  $\frac{\partial z}{\partial b} = 1$.
每层之间具有激活函数, $\sigma(z) = \frac{1}{1-e^{-z}}$,$\frac{\partial \sigma(x)}{\partial x} = \sigma (1-\sigma)$.
损失函数, $\mathcal{L} = \frac{1}{2}(\sigma - y^{i})^{2}$, $$\frac{\partial \mathcal{L}}{\partial \sigma} = (\sigma -y^{i})$$


$$\frac{\partial \mathcal{L}}{\partial w} = \frac{\partial
\mathcal{L}}{\partial \sigma} \cdot \frac{\partial \sigma }{\partial z} \cdot
\frac{\partial z}{\partial w}$$


$\frac{\partial \mathcal{L}}{\partial w} = (\sigma -
y)\sigma(1-\sigma) \cdot x$

$$\frac{\partial \mathcal{L}}{\partial b} = \frac{\partial
\mathcal{L}}{\partial \sigma} \cdot \frac{\partial \sigma }{\partial z} \cdot
\frac{\partial z}{\partial b}$$

$\frac{\partial  \mathcal{L}}{\partial b} = (\sigma -
y)\sigma(1-\sigma)$

如果对于多层神经网络，则需要逐层计算，其中$\frac{\partial
\mathcal{L}}{\partial w}$ 中的w就是相应层的权重，由最后的
L逐步回推到w。

** Conversational neural Network

** linear Discriminate Analysis
*** Fisher's linear discriminant
输入为j=0，1类样本，每类分别 $N_{j}$ 个样本
$\mu_j = \frac{1}{N_{j}} \sum x$ $x \in N_{j}$
$\Sigma_{j} = \sum(x-\mu_{j})(x-\mu_{j})^{T}$, $x \in N_{j}$

$argmax(J) = \frac{\omega^{T} (\mu_0-\mu_1)(\mu_0-\mu_1)^T
\omega}{\omega^T(\Sigma_0+\Sigma_1)\omega } =  \frac{\omega^{T} S_{b}
\omega}{\omega^T S_{w} \omega }$
*** Fisher's linear discriminant with Kernel method
$$ J(w) = \frac{(m_{2}-m_{1})^{2}}{s_{1}^{2} + s_{2}^{2}} = 
  \frac{w^{T}(m_{2}-m_{1})^{T}(m_{2}-m_{1}) w}{ w^{T}(s_{1}^{2} +
  s_{2}^{2})w}$$

$$ w = \sum^{L}_{k=1} \alpha_{k} \phi(x_{k}) $$

$$ m = \frac{1}{L_{i}} \sum^{Li}_{n=1}\phi(x_{n}^{i})$$

$$ w^{T} m_{i} = \alpha^{T}M_{i}$$

$$ M_{i} = \frac{1}{L_{i}}\sum^{L}_{k=1}\sum^{L_{i}}_{n=1}
k(x_{k},x_{n}^{i})$$

Numerator:$$w^{t}S_{B}w = \alpha^{T}M\alpha$$
Denominator:
$$ w^{T}S_{w}w = \alpha^{T} N \alpha$$
$$ N = \sum_{i=1,2}K_{i}(I-1/L)K_{i}^{T} $$
$$ (K_{i})_{n,m} = k(x_{n}, x_{m}^{i})$$

*** Probabilistic Generative Model
用贝叶斯定理求出每个可能的概率，再取最大的值
#+begin_src 
one two class case
#+end_src
$$ P(C_{1}|x) = \frac{P(C_{1}|x)P(C_{1})}{P(C_{1}|x)P(C_{1}) +
P(C_{2}|x)P(C_{2})} = \frac{1}{1+exp(log
\frac{P(C_{1}|x)P(C_{1})}{P(C_{2}|x)P(C_{2})} )}$$
即可以 Logistic sigmoid 函数求解

#+begin_src 
multi class case
#+end_src

$$P(C_{k}|x) = \frac{P(x|C_{k})P(C_{k})}{\sum_{j} P(x|C_{j})P(C_{j})}$$

即可以用 Softmax funtion 来求解

*** Probabilistic Discriminant Model
Better predictive performance if assumptions about class-conditional
distributions not correct.

和 generative model 一样求解，同样也有二分和多分类，但是该类问题设为
logical regression, See logical regression

** logistic regression
用 logical sigmoid function 来作二分类判断，检验概率是否过半

** Principe Component Analysis
*** PCA Algorithms
将原来的数据坐标进行线性组合，组成新的坐标基底，让数据在新基底
上投影最小化，以去除，压缩该些维度
1. 将数据中心化
2. 求出数据在所有特性的协方差矩阵
3. 如果矩阵是方阵，则可以直接特征值分解
4. 如果矩阵不是方阵，则先乘以转置，再特征值分解，注意此时求得特征值要开方
5. 如果不是方阵，也可以直接奇异值分解
6. 取出前面的需要的维度，多余的被压缩了
*** Probabilistic generative model for PCA
State the probabilistic generative model underlying Probabilistic PCA
with a K-dimensional latent space and observations $x\in R^{D}$ . Define
all three random variables and their distribution.

Hidden Variable z in K-dimension from probabilistic generative PCA:
we can transfer z into standard gaussian distribution,
$$p(\vec{z}) = N(0, I), \vec{z} \in R^{K}, \vec{z} \sim N(0, I)$$

observation variable x in D-dimension giving z:
$$p(\vec{x}|\vec{z}) = N(\vec{W}\vec{z} + u, \sigma^{2}I), \vec{x} \in
R^{D}$$
$$\vec{x} = Wz + u + \epsilon, \epsilon \sim N(0, \sigma^{2}I)$$

So, $p(x) = \int p(x|z)p(z)dz$
$$E(x) = E(Z + u + \epsilon) = u$$
$$Cove[x] = E[(Wz + u + \epsilon)(Wz + u + \epsilon)^{T}]
= E(W^{T}W) + E(\epsilon \epsilon^{T}) = WW^{T} + \sigma^{2}I$$

$$ x \sim N(u, Cov[x])$$

** K-Means
输入样本集 D: $x_{1}, x_{1}, x_{2},,,x_{m}$
聚类数 k, 
最大迭代数 N,
期望输出: $C_{1}, C_{2},,,C_{k}$

随机初始化k个聚类中心，并作不同类别的标记
for i= 1,2,..N:
    初始化所有C
    计算每个点到每个中心的距离，并被最小距离的聚类中心标记
    对于所有相同标记的聚类更新中心，再重复上一步骤，直到没有变化为止
    

#+begin_src python :results output
  import random
  import numpy as np
  import matplotlib.pyplot as plt

  b = []
  for i in range(100):
      a = np.array(list([(20,50),(30,10),(60,30)]))
      for j in range(a.shape[0]):
          for k in range(a.shape[1]):
              a[j][k] += random.randint(0,30)
              b.append(a[j])

  b = np.array(b)
  plt.plot(b[:,0], b[:,1], 'ro')
  plt.title("toy data")
  plt.show()


  # sklearn API
  from sklearn.cluster import KMeans
  y_pred = KMeans(n_clusters=3, random_state=9).fit_predict(b)
  plt.scatter(b[:, 0], b[:, 1], c=y_pred)
  plt.title("toy data with sklearn API")
  plt.show()

  # manual
  def findClosestCentroids(X, centroids):
      distance = np.zeros((len(X),len(centroids)))
      for i in range(len(X)):
          for j in range(len(centroids)):
              distance[i,j] = np.linalg.norm(X[i,:]-centroids[j,:])
      return np.argmin(distance,axis=1)

  def computeCentroids(X, idx, K):
      centroids = np.zeros((K,X.shape[1]))
      for i in range(K):
          centroids[i,:] = np.mean(X[idx == i],axis = 0)
      return centroids


  def runkMeans(X,K,max_iters):
      indexs = np.random.choice(np.array(range(len(X))), K,replace=False)
      centroids = X[indexs]
      for max_iter in range(max_iters):
          idx = findClosestCentroids(X, centroids)
          centroids = computeCentroids(X, idx, K)
          colors = ['','','']
          for i in range(K):
              plt.scatter(X[idx==i, 0], X[idx==i, 1])
          plt.scatter(centroids[:, 0], centroids[:, 1], c='r')
          plt.title("toy data with manual {} time".format(max_iter))
          plt.show()
  K = 3
  max_iters = 3
  runkMeans(b,K,max_iters)

#+end_src

#+RESULTS:

** EM algorithms
E step: compute responsibilites $\gamma_{nk}$ given current $\pi_{k},
\mu_{k}, \Sigma_{k}$
$$ \gamma_{nk} = \frac{\pi_{k} N(x|\mu_{k}, \Sigma_{k}}{ 
\sum_{k=1}^{K}\pi_{k}N(x|\mu_{k},\Sigma_{k})}$$

M step: update  $\pi_{k},\mu_{k}, \Sigma_{k}$ given $\gamma_{nk}$.
according to the derivative of $log p(x|\pi, \mu, \Sigma) =
\sum^{N}_{n=1}log \sum^{K}_{k=1} \pi N(x_{k}|\mu_{k}, \Sigma_{k})$
with respect to the $\pi_{k},\mu_{k}, \Sigma_{k}$,

cluster means: $$\mu_{k} = \frac{1}{N_{k}} \sum^{N}_{n=1} \gamma_{nk} x_{n}$$

cluster covariances: $$\Sigma_{k} =
\frac{1}{N_{k}}\sum^{N}_{n=1}\gamma_{nk}(x_{n}-\mu_{k})(x_{n}-\mu_{k})^{T}$$

cluster priors:$$\pi_{k} = \frac{N_{k}}{N}$$


* link
机器学习代码和课件可在此下 [fn:xiaoxiong]



[fn:xiaoxiong]https://jia666-my.sharepoint.com/:f:/g/personal/hmp121_xkx_me/EsaG70R5NXdMgC7V1aYwyQcB5C-8nFiSYTWG3Uur9ZvbEA?e=1D2uPf
