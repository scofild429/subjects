#+TITLE:  python
#+OPTIONS: num:nil
#+SETUPFILE: /home/silin/.emacs.d/org-html-themes/org/theme-bigblow.setup
#+STARTUP: overview
* Fluent_Python
** map and for
#+BEGIN_SRC python :results output
list1 = [ord(x) for x in 'ABC']
print(list1)

list2 = map(ord, 'ABC')
print(list2)

#+END_SRC

#+RESULTS:
: [65, 66, 67]
: [65, 66, 67]

** multiply
#+BEGIN_SRC python :results output
print('A'*3)
print(['A']*3)
print([['A']]*3)

#+END_SRC

#+RESULTS:
: AAA
: ['A', 'A', 'A']
: [['A'], ['A'], ['A']]

* Python Program
** useful expression
conditional express
#+begin_src python :results output
condition = True
x = 1 if condition else 0
print(x)

#+end_src

#+RESULTS:
: 1

Large Number  Formation
#+begin_src python :results output

num1 = 100_000_000_000
num2 = 10_000_000
print(f'{num1 + num2 :,}')

#+end_src

#+RESULTS:
: 100,010,000,000


open with
#+begin_src python :results output
with open('pycharm_keybounding.org','r') as f:
    file= f.read()
words = file.split(' ')


#+end_src

#+RESULTS:

enumerate
#+begin_src python :results output
names = ['one', 'two', 'three', 'four']
for index, name in enumerate(names, start=1):
    print(index,name)

#+end_src

#+RESULTS:
: 1 one
: 2 two
: 3 three
: 4 four


zip
#+begin_src python :results output
  names = ['one', 'two', 'three', 'four']
  hero = ['ni', 'wo', 'ta', 'bu']

  for name, h in zip(names, hero):
      print(f'{name} is actually {h}')
#+end_src

#+RESULTS:
: one is actually ni
: two is actually wo
: three is actually ta
: four is actually bu


unpacking tuple
#+begin_src python :results output
a, b, *_ = (1,2,3, 4,5)
a, b, *f = (1,2,3, 4,5)
c, d, _ = (1,2,3)
print(a)
print(b)

#+end_src

#+RESULTS:
: 1
: 2


setattr, 

getattr


#+begin_src python
from getpass import getpass
password = getpass('Password:')
#+end_src


python -m model
直接执行model模块
** generator

use yield stead of return

#+begin_src python :results output
  def func (lst):
      lt = []
      for i in lst:
          lt.append( i*i)
      return lt
  num = func([ x for x in range(8)])
  print(num)
  num1 = [ x*x for x in range(8)]
  print(num1)


  #generator
  def funcg (lst):
      for i in lst:
          yield (i*i)

  numg = funcg([ x for x in range(8)])
  print(numg)
  numg1 = ( x*x for x in range(8))
  print(numg1)
        


#+end_src

#+RESULTS:
: [0, 1, 4, 9, 16, 25, 36, 49]
: [0, 1, 4, 9, 16, 25, 36, 49]
: <generator object funcg at 0x7f6b248da4d0>
: <generator object <genexpr> at 0x7f6b248dac50>
** List

#+BEGIN_SRC python
words = ['big','cat','dog']
for a in words:
    words.insert(0, 'wo')
###### this can not be execute, otherweise it will dead,rekursive of position 0
#+END_SRC
#+BEGIN_SRC python
words = ['big','cat','dog']
for a in words[:]:
    words.insert(0, 'wo')
#+END_SRC

#+RESULTS:
: None

** list creation

#+begin_src ipython :results output

# two fast way to create list
a0 = [1]*10
print(a0)
a1 = [1 for x in range(10)]
print(a1)

# every subarray is conneted
ak0 = [[]]*10
print(ak0)
ak0[2].append(3)
print(ak0)

# every subarray is disconneted
ak1 = [[] for x in range(10)]
print(ak1)
ak1[2].append(3)
print(ak1)

#+end_src

** Break Points

#+BEGIN_SRC 
else sentence after for: after for sentence else can be used

def fun(argument, *argument, **argument)
fun(single_value, tuple, dictionary)
#+END_SRC

 lambda with funtion
#+BEGIN_SRC python :results output
def fun(n):
    return lambda x: x+n
print(fun(3)(4))
#+END_SRC

#+RESULTS:
: 7




#+BEGIN_SRC 
nonlocal将变量的赋值,且将此值向外层作用域扩展一个范

global 将变量的赋值到程序的全局作用域
#+END_SRC

#+BEGIN_SRC 

 _ value: 在交互模式下，上一次打印出来的表达式被赋值给变量 _

 import test.py file as a model
 import test
 so all funtions and parameters can be called as in a test class
#+END_SRC

#+BEGIN_SRC 

python -i test.py 
after execute test.py file, terminal will get into python console, 
and all funtions and parameters can just be called.
#+END_SRC
** Tkinter Vairlaation
*** buttom
#+BEGIN_SRC python
  import tkinter as tk

  window = tk.Tk()
  window.title('my window')
  window.geometry('200x200')

  var = tk.StringVar()
  l = tk.Label(window, bg='yellow', width=20, text='empty')
  l.pack()

  def print_selection():
      l.config(text='you have selected ' + var.get())

  r1 = tk.Radiobutton(window, text='Option A',
                      variable=var, value='A',
                      command=print_selection)
  r1.pack()
  r2 = tk.Radiobutton(window, text='Option B',
                      variable=var, value='B',
                      command=print_selection)
  r2.pack()
  r3 = tk.Radiobutton(window, text='Option C',
                      variable=var, value='C',
                      command=print_selection)
  r3.pack()


  window.mainloop()


#+END_SRC
*** checkbutton
#+BEGIN_SRC python
  import tkinter as tk

  window = tk.Tk()
  window.title('my window')
  window.geometry('200x200')

  l = tk.Label(window, bg='yellow', width=20, text='empty')
  l.pack()

  def print_selection():
      if (var1.get() == 1) & (var2.get() == 0):
          l.config(text='I love only Python ')
      elif (var1.get() == 0) & (var2.get() == 1):
          l.config(text='I love only C++')
      elif (var1.get() == 0) & (var2.get() == 0):
          l.config(text='I do not love either')
      else:
          l.config(text='I love both')

  var1 = tk.IntVar()
  var2 = tk.IntVar()
  c1 = tk.Checkbutton(window, text='Python', variable=var1, onvalue=1, offvalue=0,
                      command=print_selection)
  c2 = tk.Checkbutton(window, text='C++', variable=var2, onvalue=1, offvalue=0,
                      command=print_selection)
  c1.pack()
  c2.pack()


  window.mainloop()


#+END_SRC
*** menubar
#+BEGIN_SRC python
  import tkinter as tk

  window = tk.Tk()
  window.title('my window')
  window.geometry('200x200')

  l = tk.Label(window, text='', bg='yellow')
  l.pack()
  counter = 0
  def do_job():
      global counter
      l.config(text='do '+ str(counter))
      counter+=1

  menubar = tk.Menu(window)
  filemenu = tk.Menu(menubar, tearoff=0)
  menubar.add_cascade(label='File', menu=filemenu)
  filemenu.add_command(label='New', command=do_job)
  filemenu.add_command(label='Open', command=do_job)
  filemenu.add_command(label='Save', command=do_job)
  filemenu.add_separator()
  filemenu.add_command(label='Exit', command=window.quit)

  editmenu = tk.Menu(menubar, tearoff=0)
  menubar.add_cascade(label='Edit', menu=editmenu)
  editmenu.add_command(label='Cut', command=do_job)
  editmenu.add_command(label='Copy', command=do_job)
  editmenu.add_command(label='Paste', command=do_job)

  submenu = tk.Menu(filemenu)
  filemenu.add_cascade(label='Import', menu=submenu, underline=0)
  submenu.add_command(label="Submenu1", command=do_job)

  window.config(menu=menubar)

  window.mainloop()


#+END_SRC
*** canvas (cannt load img)
#+BEGIN_SRC python
  import tkinter as tk

  window = tk.Tk()
  window.title('my window')
  window.geometry('200x200')

  canvas = tk.Canvas(window, bg='blue', height=100, width=200)
  image_file = tk.PhotoImage(file='ins.gif')
  image = canvas.create_image(10, 10, anchor='nw', image=image_file)
  x0, y0, x1, y1= 50, 50, 80, 80
  line = canvas.create_line(x0, y0, x1, y1)
  oval = canvas.create_oval(x0, y0, x1, y1, fill='red')
  arc = canvas.create_arc(x0+30, y0+30, x1+30, y1+30, start=0, extent=180)
  rect = canvas.create_rectangle(100, 30, 100+20, 30+20)
  canvas.pack()

  def moveit():
      canvas.move(rect, 0, 2)

  b = tk.Button(window, text='move', command=moveit).pack()


  window.mainloop()

#+END_SRC
*** frame


#+BEGIN_SRC python

  import tkinter as tk

  window = tk.Tk()
  window.title('my window')
  window.geometry('200x200')
  tk.Label(window, text='on the window').pack()

  frm = tk.Frame(window)
  frm.pack()
  frm_l = tk.Frame(frm, )
  frm_r = tk.Frame(frm)
  frm_l.pack(side='left')
  frm_r.pack(side='right')

  tk.Label(frm_l, text='on the frm_l1').pack()
  tk.Label(frm_l, text='on the frm_l2').pack()
  tk.Label(frm_r, text='on the frm_r1').pack()
  window.mainloop()

#+END_SRC
*** list
#+BEGIN_SRC python


  import tkinter as tk

  window = tk.Tk()
  window.title('my window')
  window.geometry('200x200')

  var1 = tk.StringVar()
  l = tk.Label(window, bg='yellow', width=4, textvariable=var1)
  l.pack()

  def print_selection():
      value = lb.get(lb.curselection())
      var1.set(value)

  b1 = tk.Button(window, text='print selection', width=15,
                height=2, command=print_selection)
  b1.pack()

  var2 = tk.StringVar()
  var2.set((11,22,33,44))
  lb = tk.Listbox(window, listvariable=var2)
  list_items = [1,2,3,4]
  for item in list_items:
      lb.insert('end', item)
  lb.insert(1, 'first')
  lb.insert(2, 'second')
  lb.delete(2)
  lb.pack()

  window.mainloop()


#+END_SRC
*** scale
#+BEGIN_SRC python

  import tkinter as tk

  window = tk.Tk()
  window.title('my window')
  window.geometry('200x200')

  l = tk.Label(window, bg='yellow', width=20, text='empty')
  l.pack()

  def print_selection(v):
      l.config(text='you have selected ' + v)

  s = tk.Scale(window, label='try me', from_=5, to=11, orient=tk.HORIZONTAL,
               length=200, showvalue=0, tickinterval=2, resolution=0.01, command=print_selection)
  s.pack()

  window.mainloop()


#+END_SRC
** decorator
1, @后没有参数，则被修饰函数名被传到修饰函数作参数
2, @后有参数，在该参数被传到修饰函数作参数，而被修饰函数可在其定义内被
接收
#+BEGIN_SRC python :results output :session
  def log(func):
      def wrapper(*args, **kw):
          print('call function %s():' % func.__name__)
          return func(*args, **kw)
      return wrapper

  def loog(text):
      def decorator(fun):
          def wrapper(*args, **kw):
              print('kkcall function %s with %s():' % (fun.__name__, text))
              return fun(*args, **kw)
          return wrapper
      return decorator



  @loog('exit')
  def now():
      print("now to start")

  now()
#+END_SRC

#+RESULTS:
: kkcall function now with exit():
: now to start

** functools
#+BEGIN_SRC python :results output
import functools
int2 = functools.partial(int, base=3)
print(int2('1211'))

#+END_SRC

#+RESULTS:
: 49

** Polymorphism
#+BEGIN_SRC python :results output :session
  class Animal(object):
      def __init__(self):
          self.name ='Animal name'

      def run(self):
          print('Animal is running')

  class Dog(Animal):
      def __init__(self):
          self.name ='Dog name'
      def run(self):
          print('Dog is running')

  class Cat(Animal):
      def __init__(self):
          self.name ='Cat name'
      def run(self):
          print('Cat is running')

  class Tortoise(Animal):
      def __init__(self):
          self.name ='Tortoise name'
      def run(self):
          print('Tortoise is running slowly')
            
  # Polymorphism, all Class or instance will be checked the best passing
  # mothode or character
  def run_twice(a):
      a.run()

  def name(b):
      print(b.name)

  dog = Dog()
  cat = Cat()

  
  run_twice(Animal())
  run_twice(Dog())
  run_twice(Cat())
  run_twice(dog)
  
  name(Animal())
  name(Dog())
  name(dog)

#+END_SRC

#+RESULTS:
: Animal is running
: Animal is running
: Dog is running
: Cat is running
: Dog is running
: Animal name
: Dog name
: Dog name

** generator
#+BEGIN_SRC python :results output :session
  def something():
      result = []
      for _ in range(10):
          result.append(2)
      return result
  print(something())


  def iter_some():
      x = 0
      for _ in range(10):
          yield x
          x += 1
  a = iter_some()
  print(next(a))
  print(next(a))
  print(next(a))

#+END_SRC

#+RESULTS:
: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
: 0
: 1
: 2

#+BEGIN_SRC python :results output :session

  def fib ():
      a = b = 1
      yield a
      yield b
      while True:
          a, b = b, a+b
          yield b
        

  for var in fib():
      if var > 100:
          break
      print(var)
    
            

#+END_SRC

#+RESULTS:
#+begin_example
1
1
2
3
5
8
13
21
34
55
89
#+end_example

** pdb
1. python -m pdb test.py
step by step with n,

2. import pdb 
pdb.set_trace() 设置断点

** Decorator
decorator just rewrite the function, using call function as argument
#+begin_src python :results output
  def decorator_function(original_function):
      def wrapper_function(*arg, **kwargs):
          print("using wrapper, and  the name is {}, and the age".format(original_function.__name__))
          return original_function(*arg, **kwargs)
      return wrapper_function

  class decorator_class(object):
      def __init__(self, original_function):
          self.original_function = original_function
      def __call__(self, *arg, **kwargs):
          print("using the decorator class")
          return self.original_function(*arg, **kwargs)

  print("////////function decor without augurment//////////////////////")
  def display():
      print("display this")
  display = decorator_function(display)
  display()
  print("////////decorator decor without augurment//////////////////////")
  @decorator_function
  def display1():
      print("display1 this")
  display1()
  print("////////function decor with augurment//////////////////////")
  def display_info(name, age):
      print("display the name is {}, and the age is {}".format(name, age))
  display_info = decorator_function(display_info)
  display_info("Xiang", 21)
  print("////////decorator decor with augurment//////////////////////")
  @decorator_function
  def display_info1(name, age):
      print("display the name is {}, and the age is {}".format(name, age))
  display_info1("Xiang", 21)
  print("/////////class decorator decor/////////////////////")
  @decorator_class
  def display_info_class(name, age):
      print("display the name is {}, and the age is {}".format(name, age))
  display_info_class("Xiang", 21)
  print("////////class function decor//////////////////////")
  def display_info_class(name, age):
      print("display the name is {}, and the age is {}".format(name, age))
  display_info_class1 = decorator_class(display_info_class)
  display_info_class1("Xiang", 21)



#+end_src

#+RESULTS:
#+begin_example
////////function decor without augurment//////////////////////
using wrapper, and  the name is display, and the age
display this
////////decorator decor without augurment//////////////////////
using wrapper, and  the name is display1, and the age
display1 this
////////function decor with augurment//////////////////////
using wrapper, and  the name is display_info, and the age
display the name is Xiang, and the age is 21
////////decorator decor with augurment//////////////////////
using wrapper, and  the name is display_info1, and the age
display the name is Xiang, and the age is 21
/////////class decorator decor/////////////////////
using the decorator class
display the name is Xiang, and the age is 21
////////class function decor//////////////////////
using the decorator class
display the name is Xiang, and the age is 21
#+end_example

** with * **
#+begin_src ipython :results output :exports both
  def function_with_one_star(*d):
      print(d)

  def function_with_two_stars(**d):
      print(d)

  function_with_one_star(1,2,3)

  function_with_two_stars(a=1, b=2, c=3)


#+end_src

#+RESULTS:
: (1, 2, 3)
: {'a': 1, 'b': 2, 'c': 3}

* Python small Program
** turtle movement
#+BEGIN_SRC python

  import turtle
  turtle.setup(650, 350, 200, 200)
  turtle.penup()
  turtle.fd(-250)
  turtle.pendown()
  turtle.pensize(25)
  turtle.pencolor("purple")
  turtle.seth(-40)
  for i in range(4):
      turtle.circle(40, 80)
      turtle.circle(-40, 80)
  turtle.circle(40, 80/2)
  turtle.fd(40)
  turtle.circle(16, 180)
  turtle.fd(40*2/3)
  turtle.done()


#+END_SRC

#+RESULTS:

** tempertur converse
#+BEGIN_SRC python
  #TempConver.py
  TempStr = input('请输入带有符号的温度值:')
  if TempStr[-1] in ['F', 'f']:
      C = (eval(TempStr[0:-1]) - 32)/1.8
      print('转换后的温度值为{:.2f}C'.format(C))
  elif TempStr[-1] in ['C', 'c']:
      F = 1.8*eval(TempStr[0:-1]) + 32
      print('转换后的温度值为{:.2f}F'.format(F))
  else:
      print('输入有误')


#+END_SRC
** print current time
#+BEGIN_SRC python :results output
  import turtle, time
  def drawGap():
      turtle.penup()
      turtle.fd(5)
  def drawLine(draw):
      drawGap()
      turtle.pendown() if draw else turtle.penup()
      turtle.fd(40)
      drawGap()
      turtle.right(90)
  def drawDigit(digit):
      drawLine(True) if digit in [2,3,4,5,6,8,9] else drawLine(False)
      drawLine(True) if digit in [0,1,3,4,5,6,7,8,9] else drawLine(False)
      drawLine(True) if digit in [0,2,3,5,6,8,9] else drawLine(False)
      drawLine(True) if digit in [0,2,6,8] else drawLine(False)
      turtle.left(90)
      drawLine(True) if digit in [0,4,5,6,8,9] else drawLine(False)
      drawLine(True) if digit in [0,2,3,5,6,7,8,9] else drawLine(False)
      drawLine(True) if digit in [0,1,2,3,4,7,8,9] else drawLine(False)
      turtle.left(180)
      turtle.penup()
      turtle.fd(20)
  # def drawDate(date):
  #     for i in date:
  #         drawDigit(eval(i))
  # def main():
  def drawDate(date):
      turtle.pencolor("red")
      for i in date:
          if i == '-':
              turtle.write('年', font=("Arial", 18, "normal"))
              turtle.pencolor("green")
              turtle.fd(40)
          elif i == '=':
              turtle.write('月', font=("Arial", 18, "normal"))
              turtle.pencolor("blue")
              turtle.fd(40)
          elif i == '+':
              turtle.write('日', font=("Arial", 18, "normal"))
          else:
              drawDigit(eval(i))
  def main():
      turtle.setup(800,350,200,200)
      turtle.penup()
      turtle.fd(-300)
      turtle.pensize(5)
      # drawDate('20181010')
      drawDate(time.strftime("%Y-%m=%d+",time.gmtime()))
      turtle.hideturtle()
      turtle.done()
  main()

#+END_SRC

#+RESULTS:

* data analysis
** statistical mothode
#+BEGIN_SRC python :results output
  from scipy import stats
  from scipy.stats import norm
  import numpy as np
  import scipy as sp
  print(sp.stats.t.ppf(0.95,6))
  print(norm.cdf([-1,0,1]))
  print(norm.cdf(np.array([-1,0,1])))
  print(norm.mean(), norm.std(), norm.var() )
  print(norm.pdf(0))
  print(norm.cdf(1.96))
  print(norm.ppf(0.975))
  print(norm.cdf(1))
  print(norm.ppf(0.841344746090))
  print(norm.sf(1-norm.cdf(1)))
  print(norm.ppf(0.9))
  print(stats.t.ppf(0.975,3))
  print(stats.t.ppf(0.975,3))

#+END_SRC

#+RESULTS:
#+begin_example
1.9431802803927816
[0.15865525 0.5        0.84134475]
[0.15865525 0.5        0.84134475]
0.0 1.0 1.0
0.3989422804014327
0.9750021048517795
1.959963984540054
0.8413447460685429
1.0000000000886762
0.4369702468636344
1.2815515655446004
3.182446305284263
3.182446305284263
#+end_example

** confidence level interval determinate
#+BEGIN_SRC python
  import numpy as np
  import scipy as sp
  import scipy.stats

  b = [8*x**0 for x in range(200)] + np.random.normal(0, 0.05, (200))


  def t_stastik(data, confidence):
      m, se = np.mean(data), sp.stats.sem(data)
      h = se*sp.stats.t.isf((1-confidence)/2. , df = (len(data)-1) )
      return m, m-h, m+h
  print(" For given data sete we have their mean  with 95% confidence level of region :",t_stastik(b,0.95))

  def mean_confidence_interval(data, confidence):
      m, se = np.mean(data), sp.stats.sem(data)
      h = se*sp.stats.t.ppf((1+confidence)/2.,len(data)-1)
      return m, m-h, m+h
  print('For data the mean  can also  be calcaleted as at 95% confidence level is :', mean_confidence_interval(b, 0.95))


#+END_SRC
** a complete ploted distribution of confidence level on t mode
#+BEGIN_SRC python
  import numpy as np
#  import seaborn as sns
  from scipy import stats
  import matplotlib.pyplot as plt

  np.random.seed(3)
  MU = 64
  sigma = 5
  size = 10
  heights = np.random.normal(MU, sigma,size)
  print("accoding to the mean and deviation we have a example of 10 rondom number : ", heights)

  mean_heights = np.mean(heights)
  deviation_heights = np.std(heights)
  SE = np.std(heights)/np.sqrt(size)

  print('99% confidence interval is :', stats.t.interval(0.99, df = size-1 , loc = mean_heights, scale=SE))
  print('90% confidence interval is :', stats.t.interval(0.90, df = size-1 , loc = mean_heights, scale=SE))
  print('80% confidence interval is :', stats.t.interval(0.80, df = size-1 , loc = mean_heights, scale=SE)) 

#+END_SRC
** a complete ploted distribution
#+BEGIN_SRC python
  import numpy as np

  sample_size = 1000
  heights = np.random.normal(MU, sigma, sample_size)
  SE = np.std(heights)/np.sqrt(sample_size)
  (l,u) = stats.norm.interval(0.95, loc = np.mean(heights), scale = SE)
  print(l,u)
  plt.hist(heights, bins = 20)
  y_height = 5
  plt.plot([l,u], [y_height, y_height], '_', color='r')
  plt.plot(np.mean(heights), y_height, 'o', color= 'b')
  plt.show()

#+END_SRC

#+RESULTS:

** a complete ploted distribution on between region

#+BEGIN_SRC python
  x = np.linspace(-5,5,100)
  y = stats.norm.pdf(x,0,1)
  plt.plot(x,y)
  plt.vlines(-1.96,0,1,colors='r',linestyles='dashed')
  plt.vlines(1.96,0,1,colors='r',linestyles='dashed')
  fill_x = np.linspace(-1.96,1.96,500)
  fill_y = stats.norm.pdf(fill_x, 0,1)
  plt.fill_between(fill_x,fill_y)
  plt.show()

#+END_SRC
** a example from internet
#+BEGIN_SRC python
  import pandas as pd
  from scipy import stats as ss
  data_url = "https://raw.githubusercontent.com/alstat/Analysis-with-Programming/master/2014/Python/Numerical-Descriptions-of-the-Data/data.csv"
  df = pd.read_csv(data_url)
  print(df.describe())
  import matplotlib.pyplot as plt
  pd.options.display.mpl_style = 'default' 
  plt.show(df.plot(kind = 'box'))

#+END_SRC
** 1 2 3 order and gauss fitting
#+BEGIN_SRC python
  import numpy as np
  import matplotlib.pyplot as plt
  from scipy import optimize
  from scipy.optimize import curve_fit


  def f_1_degree(x,A,B):
      return A*x + B

  def f_2_degree(x,A,B,C):
      return A*x**2 + B*x + C

  def f_3_degree(x,A,B,C,D):
      return A*x**3 + B*x**2 + C*x + D


  def f_gauss(x,A,B,sigma):
      return A*np.exp(-(x-B)**2/(2*sigma**2))

  def plot_figure():
      plt.figure()

      x0 = [1,2,3,4,5]
      y0 = [1,3,8,18,36]

      #plot original data
      plt.scatter(x0,y0,25,"red")

      # plot f1
      params_1, pcovariance_1 = optimize.curve_fit(f_1_degree,x0,y0)
      ##########
      params_f_1, pcovariance_f_1 = curve_fit(f_1_degree,x0,y0)
      x1 = np.arange(0,6,0.01)
      y1 = params_1[0]*x1+params_1[1]
      plt.plot(x1,y1,"blue")
      print("The liear fitting for date is : y = ",params_1[1],"*x + ",params_1[0])
      print("The params uncertainies are:")
      print("a =", params_1[0], "+/-", round(pcovariance_1[0,0]**0.5,3))
      print("b =", params_1[1], "+/-", round(pcovariance_1[1,1]**0.5,3))


      #plot f2
      params_2, pcovariance_2 = curve_fit(f_2_degree,x0,y0)
      x2 = np.arange(0,6,0.01)
      y2 = params_2[0]*x1**2+params_2[1]*x1 + params_2[2]
      plt.plot(x2,y2,"green")
      print("The second order curve fitting for date is : y = " ,params_2[2],"*x² + " ,params_2[1],"*x + ",params_2[0])
      print("The params uncertainies are:")
      print("a =", params_2[0], "+/-", round(pcovariance_2[0,0]**0.5,3))
      print("a =", params_2[0], "+/-", round(pcovariance_2[0,0]**0.5,3))
      print("b =", params_2[1], "+/-", round(pcovariance_2[1,1]**0.5,3))
      print("c =", params_2[2], "+/-", round(pcovariance_2[2,2]**0.5,3))

      #plot f3
      params_3, pcovariance_3 = curve_fit(f_3_degree,x0,y0)
      x3 = np.arange(0,6,0.01)
      y3 = params_3[0]*x1**3+params_3[1]*x1**2 + params_3[2]*x1 + params_3[3]
      plt.plot(x3,y3,"purple")
      print("The second order curve fitting for date is:y =",params_3[3],"*x³+",params_2[2],"*x² + " ,params_2[1],"*x + ",params_2[0])
      print("The params uncertainies are:")
      print("a =", params_3[0], "+/-", round(pcovariance_3[0,0]**0.5,3))
      print("b =", params_3[1], "+/-", round(pcovariance_3[1,1]**0.5,3))
      print("c =", params_3[2], "+/-", round(pcovariance_3[2,2]**0.5,3))
      print("d =", params_3[3], "+/-", round(pcovariance_3[3,3]**0.5,3))

      #plot gauss
      params_gauss, pcovariance_gauss = curve_fit(f_gauss,x0,y0)
      xgauss = np.arange(0,6,0.01)
      ygauss = params_gauss[0]*np.exp(-(xgauss-params_gauss[1])**2/(2*params_gauss[2]**2))
      plt.plot(xgauss,ygauss,"black")
      print("The gauss function curve fitting for date is : y = ",params_gauss[2],"*exp{-(x-",params_gauss[1],")²/(2*sigma²) +",params_gauss[0])
      print("The params uncertainies are:")
      print("a =", params_gauss[0], "+/-", round(pcovariance_gauss[0,0]**0.5,3))
      print("mean =", params_gauss[1], "+/-", round(pcovariance_gauss[1,1]**0.5,3))
      print("std =", params_gauss[2], "+/-", round(pcovariance_gauss[2,2]**0.5,3))
 
    

      plt.title("plot for different fittign")
      plt.xlabel("x")
      plt.ylabel("y")
      plt.show()
      return

  plot_figure()


#+END_SRC
** linear fitting
#+BEGIN_SRC python
  # matplotlib inline
  import matplotlib.pyplot as plt;
  import numpy as np;
  from scipy import integrate
  from scipy.optimize import curve_fit
  import math

  #1. x axis coordinnat for 10 points data
  xmin=0.01; xmax=2; pts = 10;
  xx = np.linspace(xmin, xmax, pts);

  #2. y axis coordinnat for 10 points data
  rho = np.sqrt(1/xx) + 0.5*np.exp(xx)*xx**2;

  #plot the original data
  plt.plot(xx, rho, 'bo', label='Original data')

  #3. x axis coordinnat for 200 points fitting
  x_fine = np.linspace(xmin, xmax, 200);

  #fiting
  params, cov = np.polyfit(xx, rho, 1, cov=True)

  #to reconstruct the linear function
  bestfit_rho = params[0]*x_fine + params[1]
  plt.plot(x_fine, bestfit_rho, 'r-', lw=2, label='One order of linear fit');

  print(params)



#+END_SRC
** linear fitting with ployfit

#+BEGIN_SRC python
  # matplotlib inline
  import matplotlib.pyplot as plt;
  import numpy as np;
  from scipy import integrate
  from scipy.optimize import curve_fit
  import math

  #1. x axis coordinnat for 10 points data
  xmin=0.01; xmax=2; pts = 10;
  xx = np.linspace(xmin, xmax, pts);

  #2. y axis coordinnat for 10 points data
  rho = np.sqrt(1/xx) + 0.5*np.exp(xx)*xx**2;

  #plot the original data
  plt.plot(xx, rho, 'bo', label='Original data')

  #3. x axis coordinnat for 200 points fitting
  x_fine = np.linspace(xmin, xmax, 200);

  #fiting  it can be any order 
  params, cov = np.polyfit(xx, rho, 4, cov=True);
  p = np.poly1d(params)
  plt.plot(x_fine, p(x_fine), 'g-', lw=2, label='The Best poly1d fit');

  print(params)
  plt.xlabel('$x$');
  plt.ylabel(r'$\rho$');
  plt.legend(fontsize=13);
  plt.show()


#+END_SRC
* blockchain
** part1

#+BEGIN_SRC python
  import hashlib
  import json

  class Block():
      def __init__(self, nonce,tstamp, transcation, prevhash=''):
          self.nonce = nonce
          self.tstamp = tstamp
          self.transcation = transcation
          self.prevhash = prevhash
          self.hash =self.calcHash()
      def __str__(self):
          string = 'nonce:'+str(self.nonce)+'\n'
          string += 'tstamp:'+str(self.tstamp)+'\n'
          string += 'transcation:'+str(self.transcation)+'\n'
          string += 'prevhash:'+str(self.prevhash)+'\n'
          string += 'hash:'+str(self.hash)+'\n'
          return string
      def calcHash(self):
          block_string = json.dumps({'nonce':self.nonce, 'tstamp':self.tstamp, 'transcation':self.transcation, 'prevhash':self.prevhash}, sort_keys=True).encode()
          return hashlib.sha256(block_string).hexdigest()

  #     def printHash(self):
  #         print('prevhash', self.prevhash)
  #         print('hash', self.hash)
  # bblock = Block(1,'Fri Oct 26 00:51:10 2018', 100)
  # bblock.printHash()

  class Blockchain():
      def __init__(self):
          self.chain=[self.generateGensisBlock(),]
      def generateGensisBlock(self):
          return Block(0,'Fri Oct 26 21:15:12 2018','Gensis Block')
      def getLastBlock(self):
          return self.chain[-1]
      def addBlock(self,newBlock):
          newBlock.prevhash=self.getLastBlock().hash
          newBlock.hash=newBlock.calcHash()
          self.chain.append(newBlock)
      def isChainValid(self):
          for i in range(1,len(self.chain)):
              prevb = self.chain[i-1]
              currb = self.chain[i]
              if(currb.hash != currb.calcHash()):
                  print('invalid block')
                  return False
              if(currb.prevhash != prevb.hash):
                  print('invalid chain')
                  return False
          return True
            

  shangCoin = Blockchain()
  shangCoin.addBlock(Block(1,'Fri Oct 26 21:22:51 2018',100))
  shangCoin.addBlock(Block(2,'Fri Oct 26 21:23:28 2018',200))

  shangCoin.chain[1].transcation = 66
  print('only change transcation:'+ '\n'+str(shangCoin.chain[1].calcHash()) +'\n'+ str(shangCoin.chain[1].hash))
  #如果只是改了一个transaction的值，在执行calcHash之后的结果肯定不等于之前存在hash属性里的结果一致，会报错
  shangCoin.chain[1].hash =shangCoin.chain[1].calcHash()
  print('after change transcation, and hash all again:'+'\n'+str(shangCoin.chain[2].prevhash)+'\n'+str(shangCoin.chain[1].hash))
  #如果在改变transaction后，再hash整个块，则存在下一个块内的prevhash将不会等于这个块内hash变化后的值

  for b in shangCoin.chain:
      print(b)
  print(shangCoin.isChainValid())


#+END_SRC
** part2
#+BEGIN_SRC python
  import hashlib
  import json

  class Block():
      def __init__(self, nonce,tstamp, transcation, prevhash=''):
          self.nonce = nonce
          self.tstamp = tstamp
          self.transcation = transcation
          self.prevhash = prevhash
          self.hash =self.calcHash()
      def __str__(self):
          string = 'nonce:'+str(self.nonce)+'\n'
          string += 'tstamp:'+str(self.tstamp)+'\n'
          string += 'transcation:'+str(self.transcation)+'\n'
          string += 'prevhash:'+str(self.prevhash)+'\n'
          string += 'hash:'+str(self.hash)+'\n'
          return string
      def calcHash(self):
          block_string = json.dumps({'nonce':self.nonce, 'tstamp':self.tstamp, 'transcation':self.transcation, 'prevhash':self.prevhash}, sort_keys=True).encode()
          return hashlib.sha256(block_string).hexdigest()
      def mineBlock(self,difficult):
          while(self.hash[:difficult] != str('').zfill(difficult)):
              self.nonce += 1
              self.hash = self.calcHash()
          print('the mineBlock is:',self.hash)
  #     def printHash(self):
  #         print('prevhash', self.prevhash)
  #         print('hash', self.hash)
  # bblock = Block(1,'Fri Oct 26 00:51:10 2018', 100)
  # bblock.printHash()

  class Blockchain():
      def __init__(self):
          self.chain=[self.generateGensisBlock(),]
          self.difficult = 3
      def generateGensisBlock(self):
          return Block(0,'Fri Oct 26 21:15:12 2018','Gensis Block')
      def getLastBlock(self):
          return self.chain[-1]
      def addBlock(self,newBlock):
          newBlock.prevhash=self.getLastBlock().hash
          newBlock.hash=newBlock.calcHash()
          newBlock.mineBlock(self.difficult)
          self.chain.append(newBlock)
      def isChainValid(self):
          for i in range(1,len(self.chain)):
              prevb = self.chain[i-1]
              currb = self.chain[i]
              if(currb.hash != currb.calcHash()):
                  print('invalid block')
                  return False
              if(currb.prevhash != prevb.hash):
                  print('invalid chain')
                  return False
          return True
            

  shangCoin = Blockchain()
  print('Adding the first block')
  shangCoin.addBlock(Block(1,'Fri Oct 26 21:22:51 2018',100))
  print('Adding the second block')
  shangCoin.addBlock(Block(2,'Fri Oct 26 21:23:28 2018',200))

  # shangCoin.chain[1].transcation = 66
  # print('only change transcation:'+ '\n'+str(shangCoin.chain[1].calcHash()) +'\n'+ str(shangCoin.chain[1].hash))
  # #如果只是改了一个transaction的值，在执行calcHash之后的结果肯定不等于之前存在hash属性里的结果一致，会报错
  # shangCoin.chain[1].hash =shangCoin.chain[1].calcHash()
  # print('after change transcation, and hash all again:'+'\n'+str(shangCoin.chain[2].prevhash)+'\n'+str(shangCoin.chain[1].hash))
  # #如果在改变transaction后，再hash整个块，则存在下一个块内的prevhash将不会等于这个块内hash变化后的值

  for b in shangCoin.chain:
      print(b)
    
  if(shangCoin.isChainValid()):
    
      print('valid blockchain')
  else:
      print('hacked blockchain')
    


#+END_SRC
** part3
#+BEGIN_SRC python
  import hashlib
  import json
  from datetime import datetime 
  class Transaction():
      def __init__(self, from_address,to_address, amount):
          self.from_address = from_address
          self.to_address = to_address
          self.amount = amount

  class Block():
      def __init__(self,tstamp, transcationList, prevhash=''):
          self.nonce = 0
          self.tstamp = tstamp
          self.transcationList = transcationList
          self.prevhash = prevhash
          self.hash =self.calcHash()
      def __str__(self):
          string = 'nonce:'+str(self.nonce)+'\n'
          string += 'tstamp:'+str(self.tstamp)+'\n'
          string += 'transcation:'+str(self.transcation)+'\n'
          string += 'prevhash:'+str(self.prevhash)+'\n'
          string += 'hash:'+str(self.hash)+'\n'
          return string
      def calcHash(self):
          block_string = json.dumps({'nonce':self.nonce, 'tstamp':str(self.tstamp), 'transcation':self.transcationList[0].amount, 'prevhash':self.prevhash}, sort_keys=True).encode()
          return hashlib.sha256(block_string).hexdigest()
      def mineBlock(self,difficult):
          while(self.hash[:difficult] != str('').zfill(difficult)):
              self.nonce += 1
              self.hash = self.calcHash()
          print('the mineBlock is:',self.hash)
  #     def printHash(self):
  #         print('prevhash', self.prevhash)
  #         print('hash', self.hash)
  # bblock = Block(1,'Fri Oct 26 00:51:10 2018', 100)
  # bblock.printHash()

  class Blockchain():
      def __init__(self):
          self.chain=[self.generateGensisBlock(),]
          self.difficult = 3
          self.pendingTransactions = []
          self.mining_reward = 100
        
      def generateGensisBlock(self):
          return Block('Fri Oct 26 21:15:12 2018', [Transaction(None,None,0),])
    
      def getLastBlock(self):
          return self.chain[-1]
    
      # def addBlock(self,newBlock):
      #     newBlock.prevhash=self.getLastBlock().hash
      #     newBlock.hash=newBlock.calcHash()
      #     newBlock.mineBlock(self.difficult)
      #     self.chain.append(newBlock)
      def minePendingTranaction(self,mining_reward_address):
          block = Block(datetime.now(), self.pendingTransactions)
          block.mineBlock(self.difficult)
          print('Block is mined to get reward:',self.mining_reward)
          self.chain.append(block)
          self.pendingTransactions = [Transaction(None,mining_reward_address, self.mining_reward)]
        
      def createTransaction(self,T):
          self.pendingTransactions.append(T)

      def getBalence(self,address):
          balance = 0
          for b in self.chain:
              for t in b.transcationList:
                  if t.to_address==address:
                      balance += t.amount
                  if t.from_address==address:
                      balance -= t.amount
          return balance
    
      def isChainValid(self):
          for i in range(1,len(self.chain)):
              prevb = self.chain[i-1]
              currb = self.chain[i]
              if(currb.hash != currb.calcHash()):
                  print('invalid block')
                  return False
              if(currb.prevhash != prevb.hash):
                  print('invalid chain')
                  return False
          return True
            

  shangCoin = Blockchain()
  shangCoin.createTransaction(Transaction('address1', 'address2',100))
  shangCoin.createTransaction(Transaction('address2', 'address1',50))
  print('starting mining:')
  shangCoin.minePendingTranaction('shangaddress')
  print('shangCoin miner balance is:', shangCoin.getBalence('shangaddress'))

  shangCoin.createTransaction(Transaction('address1', 'address2',200))
  shangCoin.createTransaction(Transaction('address2', 'address1',150))
  print('starting mining again:')
  shangCoin.minePendingTranaction('shangaddress')
  print('shangCoin miner balance is:', shangCoin.getBalence('shangaddress'))


  # print('Adding the first block')
  # shangCoin.addBlock(Block(1,'Fri Oct 26 21:22:51 2018',100))
  # print('Adding the second block')
  # shangCoin.addBlock(Block(2,'Fri Oct 26 21:23:28 2018',200))

  # shangCoin.chain[1].transcation = 66
  # print('only change transcation:'+ '\n'+str(shangCoin.chain[1].calcHash()) +'\n'+ str(shangCoin.chain[1].hash))
  # #如果只是改了一个transaction的值，在执行calcHash之后的结果肯定不等于之前存在hash属性里的结果一致，会报错
  # shangCoin.chain[1].hash =shangCoin.chain[1].calcHash()
  # print('after change transcation, and hash all again:'+'\n'+str(shangCoin.chain[2].prevhash)+'\n'+str(shangCoin.chain[1].hash))
  # #如果在改变transaction后，再hash整个块，则存在下一个块内的prevhash将不会等于这个块内hash变化后的值
 
  # for b in shangCoin.chain:
  #     print(b)
    
  # if(shangCoin.isChainValid()):
    
  #     print('valid blockchain')
  # else:
  #     print('hacked blockchain')
    


#+END_SRC
** part4 (has error)
#+BEGIN_SRC python
  import hashlib
  import json
  from datetime import datetime 
  from flask import Flask
  from flask import jsonify
  from time import time

  class Block():
      def __init__(self, nonce, tstamp, transationList, prevhash='', hash =''):
          self.nonce = nonce
          self.tstamp = tstamp
          self.transationList = transationList
          self.prevhash = prevhash
          if hash == '':
              self.hash = self.calcHash
          else:
              self.hash =hash

      def toDict(self):
          return {'nonce':self.nonce, 'tstamp':str(self.tstamp), 'transation':self.transationList, 'prevhash':self.prevhash, 'hash':self.hash}
    
      def calcHash(self):
          block_string = json.dumps({'nonce':self.nonce, 'tstamp':str(self.tstamp), 'transation':self.transationList, 'prevhash':self.prevhash}, sort_keys=True).encode()
          return hashlib.sha256(block_string).hexdigest()
      def mineBlock(self, difficult):
          while(self.hash[:difficult] != str('').zfill(difficult)):
              self.nonce += 1
              self.hash = self.calcHash()
          print('the mineBlock is:',self.hash)

  class Blockchain():
      def __init__(self):
          self.chain=[]
          self.difficult=3
          self.pendingTransations = []
          self.mining_reward = 100
          self.generateGensisBlock()
      def generateGensisBlock(self):
          dect = {'nonce':0, 'tstamp':'Sun Oct 28 12:54:03 2018', 'transationList':[{'from_address':None, 'to_address':None, 'amount':0},],'hash':''}
          b = Block(**dect)
          self.chain.append(b.toDict())
    
      def getLastBlock(self):
          return Block(**self.chain[-1])

      def minePendingTransation(self, mining_reward_address):
          self.pendingTransations = [{'from_address':None,'to_address':mining_reward_address,'amount':self.mining_reward},]
          block = Block(0, str(datetime.now()),self.pendingTransations)
          block.prevhash = self.getLastBlock().hash
          block.mineBlock(self.difficult)
          print('Block is mined to get reward:',self.mining_reward)
          self.chain.append(block.toDict())
        
      def createTransation(self, from_address, to_address,amount):
          self.pendingTransations.append({'from_adress':from_address,'to_address':to_address, 'amount':amount})

      def getBalence(self,address):
          balance = 0
          for index in range(len(self.chain)):
              dictList=self.chain[index]['transationList']
              for dic in dictList:
                  if dic['to_address']==address:
                      balance += dic['amount']
                  if dic['from_address']==address:
                      balance -= dic['amount']
          return balance
    
      def isChainValid(self):
          for i in range(1,len(self.chain)):
              prevb =Block(**self.chain[i-1])
              currb =Block(**self.chain[i])
              if(currb.hash != currb.calcHash()):
                  print('invalid block')
                  return False
              if(currb.prevhash != prevb.hash):
                  print('invalid chain')
                  return False
          return True
            

  shangCoin = Blockchain()
  shangCoin.createTransation('address1', 'address2',100)
  shangCoin.createTransation('address2', 'address1',50)
  print('starting mining:')
  shangCoin.minePendingTransation("shangaddress")
  print('shangCoin miner balance is:', shangCoin.getBalence("shangaddress"))
  print(shangCoin.isChainValid())


  # print('Adding the first block')
  # shangCoin.addBlock(Block(1,'Fri Oct 26 21:22:51 2018',100))
  # print('Adding the second block')
  # shangCoin.addBlock(Block(2,'Fri Oct 26 21:23:28 2018',200))

  # shangCoin.chain[1].transcation = 66
  # print('only change transcation:'+ '\n'+str(shangCoin.chain[1].calcHash()) +'\n'+ str(shangCoin.chain[1].hash))
  # #如果只是改了一个transaction的值，在执行calcHash之后的结果肯定不等于之前存在hash属性里的结果一致，会报错
  # shangCoin.chain[1].hash =shangCoin.chain[1].calcHash()
  # print('after change transcation, and hash all again:'+'\n'+str(shangCoin.chain[2].prevhash)+'\n'+str(shangCoin.chain[1].hash))
  # #如果在改变transaction后，再hash整个块，则存在下一个块内的prevhash将不会等于这个块内hash变化后的值
 
  # for b in shangCoin.chain:
  #     print(b)
    
  # if(shangCoin.isChainValid()):
    
  #     print('valid blockchain')
  # else:
  #     print('hacked blockchain')
    


#+END_SRC

* Computer Version
** opencv
[[https://anaconda.org/conda-forge/opencv][conda link]]
conda install -c conda-forge opencv

** Ubungs 01
<<computer version ubung 01>>
Solution erkälerung
Mediuem 
under Quantil 25%
ober Quantil  75%

Entopie
$$ E = \sum h_{rel}(w) log^{\frac{1}{h_{rel}}(w)} = - \sum p log^{p}$$

Anisotorpie 

Paar-Grauwertmatrix

* python automotion
** system manipulation 
*** os.getcwd()
#+begin_src ipython :results output :exports both
import os
print(os.getcwd())
#+end_src

#+RESULTS:
: /home/silin/Dropbox/subjects

*** os.chdir() 
#+begin_src ipython  :results output :exports both
import os
os.chdir("../../Schreibtisch")
print(os.getcwd())
#+end_src

#+RESULTS:
: /home/silin/Schreibtisch

*** os.listdir()
#+begin_src ipython  :results output :exports both
print(os.listdir())
#+end_src

#+RESULTS:
: []

* PyTorch
** Basic
*** cpu with cuda.gpu 
#+begin_src ipython :results output

  import torch
  import time
  print(torch.__version__)
  print(torch.cuda.is_available)

  a = torch.randn(10000, 1000)
  b = torch.randn(1000, 2000)

  t0 = time.time()
  c = torch.matmul(a, b)
  t1 = time.time()
  print(a.device, t1-t0, c.norm(2))

  device = torch.device('cuda')
  a = a.to(device)
  b = b.to(device)

  t0 = time.time()
  c = torch.matmul(a, b)
  t2 = time.time()
  print(a.device, t2-t0, c.norm(2))


  t0 = time.time()
  c = torch.matmul(a, b)
  t2 = time.time()
  print(a.device, t2-t0, c.norm(2))
#+end_src

#+RESULTS:
: 1.7.0+cu101
: <function is_available at 0x7f58ba0e9d90>
: cpu 0.22681307792663574 tensor(141040.8438)
: cuda:0 0.004456043243408203 tensor(141442.0938, device='cuda:0')
: cuda:0 0.00037932395935058594 tensor(141442.0938, device='cuda:0')

*** auto Derivative

#+begin_src ipython :results output
  import torch
  from torch import autograd

  x = torch.tensor(1.)
  a = torch.tensor(1. , requires_grad=True)
  b = torch.tensor(2. , requires_grad=True)
  c = torch.tensor(3. , requires_grad=True)

  y = a**2*x + b*x + c

  print('before:', a.grad, b.grad, c.grad)
  grad = autograd.grad(y, [a,b,c])
  print('after:', grad[0], grad[1], grad[2])

#+end_src

#+RESULTS:
: before: None None None
: after: tensor(2.) tensor(1.) tensor(1.)

*** small example
#+begin_src ipython :results output
  import torch

  x = torch.ones(2,2,requires_grad=True)
  y = x +2
  z = y*y*3
  out = z.mean()
  out.backward()
  print(x.grad)

#+end_src

#+RESULTS:
: tensor([[4.5000, 4.5000],
:         [4.5000, 4.5000]])


\begin{equation}
x = 
  \left(
  \begin{array}{cc}
          1  & 1  \\
          1  & 1 \\
  \end{array}
  \right)
\end{equation}



\begin{equation}
y =    2 + x =
  \left(
  \begin{array}{cc}
          3  & 3  \\
          3  & 3 \\
  \end{array}
  \right)
\end{equation}


\begin{equation}
z =   3*y*y =
  \left(
  \begin{array}{cc}
          27  & 27  \\
          27  & 27 \\
  \end{array}
  \right)
\end{equation}

$$ out = \frac{1}{4}\sum_{i=1,j=1}^{i=2, j=2}3y_{ij}y_{ij} =\frac{1}{4}\sum_{i=1,j=1}^{i=2, j=2}3(2+x_{ij})(2+x_{ij}) $$

x.grad is to say :
$$ \frac{d(out_{ij})}{d(x_{ij})}  = 3(2+4)/4 = 4.5$$ 








** Function
*** x = x.new_ones(4,3)

** Iterator DataLoader

here, THE i is the number of iterations, each iteration has 20 iter
#+begin_src ipython :results output
  import torch
  import numpy as np

  i = np.array([a for a in range(100)])
  i = torch.from_numpy(i)

  test = torch.utils.data.DataLoader(i, batch_size=20,shuffle=False)
  testiter = iter(test)

  for k, data in enumerate(testiter):
      print(k)


#+end_src

#+RESULTS:
: 0
: 1
: 2
: 3
: 4

** torch.max
the return is a namedtuple
dim = 0, 1, maximum of rows or columns
#+begin_src ipython :results output
import torch
i = torch.randn(3,4)
print(torch.max(i, 0))
print(torch.max(i, 1))

#+end_src

#+RESULTS:
: torch.return_types.max(
: values=tensor([-0.1560,  0.1561,  0.9781,  0.4501]),
: indices=tensor([1, 0, 1, 0]))
: torch.return_types.max(
: values=tensor([0.4904, 0.9781, 0.2857]),
: indices=tensor([2, 2, 2]))



** exsample 3 ploy
#+begin_src ipython :results output
  import torch
  import math


  class Polynomial3(torch.nn.Module):
      def __init__(self):
          """
          In the constructor we instantiate four parameters and assign them as
          member parameters.
          """
          super().__init__()
          self.a = torch.nn.Parameter(torch.randn(()))
          self.b = torch.nn.Parameter(torch.randn(()))
          self.c = torch.nn.Parameter(torch.randn(()))
          self.d = torch.nn.Parameter(torch.randn(()))

      def forward(self, x):
          """
          In the forward function we accept a Tensor of input data and we must return
          a Tensor of output data. We can use Modules defined in the constructor as
          well as arbitrary operators on Tensors.
          """
          return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3

      def string(self):
          """
          Just like any class in Python, you can also define custom method on PyTorch modules
          """
          return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'


  # Create Tensors to hold input and outputs.
  x = torch.linspace(-math.pi, math.pi, 2000)
  y = torch.sin(x)

  # Construct our model by instantiating the class defined above
  model = Polynomial3()

  # Construct our loss function and an Optimizer. The call to model.parameters()
  # in the SGD constructor will contain the learnable parameters of the nn.Linear
  # module which is members of the model.
  criterion = torch.nn.MSELoss(reduction='sum')
  optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)
  for t in range(2000):
      # Forward pass: Compute predicted y by passing x to the model
      y_pred = model(x)

      # Compute and print loss
      loss = criterion(y_pred, y)
      if t % 100 == 99:
          print(t, loss.item())

      # Zero gradients, perform a backward pass, and update the weights.
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

  print(f'Result: {model.string()}')


#+end_src

* packages
** matplotlib.plot
*** two linear plot 
#+begin_src ipython :results output
  import numpy as np
  import random
  from matplotlib import pyplot as plt

  a = np.array([ x for x in range(10)])
  b = np.array([[random.randint(0,10) for x in range(10)],[random.randint(0,10) for x in range(10)]])
  plt.plot(a,b.T)
  plt.show()


#+end_src

#+RESULTS:

*** simplest linear plot
#+BEGIN_SRC ipython :results output
  import matplotlib.pyplot as plt
  plt.plot([1,2,3,4])
  plt.ylabel('some numbers')
  plt.show()


#+END_SRC

#+RESULTS:

*** simplest point plot
#+BEGIN_SRC ipython :results output
  import matplotlib.pyplot as plt
  plt.plot([1,2,3,4], [1,4,9,16], 'ro')
  plt.axis([0, 6, 0, 20])
  plt.show()


#+END_SRC

#+RESULTS:

*** simplest function plot
#+BEGIN_SRC ipython :results output
  import numpy as np
  import matplotlib.pyplot as plt

  # evenly sampled time at 200ms intervals
  t = np.arange(0., 5., 0.2)

  # red dashes, blue squares and green triangles
  plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')
  plt.show()


#+END_SRC
*** simplest subplot
#+BEGIN_SRC  ipython :results output
  import numpy as np
  import matplotlib.pyplot as plt

  def f(t):
      return np.exp(-t) * np.cos(2*np.pi*t)

  t1 = np.arange(0.0, 5.0, 0.1)
  t2 = np.arange(0.0, 5.0, 0.02)

  plt.figure(1)
  plt.subplot(211)
  plt.plot(t1, f(t1), 'bo', t2, f(t2), 'k')

  plt.subplot(212)
  plt.plot(t2, np.cos(2*np.pi*t2), 'r--')
  plt.show()


#+END_SRC
*** histogram plot with cusomised legend
#+BEGIN_SRC  ipython :results output
  import numpy as np
  import matplotlib.pyplot as plt

  # Fixing random state for reproducibility
  np.random.seed(19680801)

  mu, sigma = 100, 15
  x = mu + sigma * np.random.randn(10000)

  # the histogram of the data
  n, bins, patches = plt.hist(x, 50, normed=1, facecolor='g', alpha=0.75)


  plt.xlabel('Smarts')
  plt.ylabel('Probability')
  plt.title('Histogram of IQ')
  plt.text(60, .025, r'$\mu=100,\ \sigma=15$')
  plt.axis([40, 160, 0, 0.03])
  plt.grid(True)
  plt.show()


#+END_SRC
*** histogram plot 
#+BEGIN_SRC  ipython :results output
  import numpy as np
  import matplotlib.pyplot as plt

  # Fixing random state for reproducibility
  np.random.seed(19680801)

  mu, sigma = 100, 15
  x = mu + sigma * np.random.randn(100000)

  # the histogram of the data
  n, bins, patches = plt.hist(x, 50, facecolor='g')

  plt.xlabel('Smarts')
  plt.ylabel('Probability')
  plt.title('Histogram of IQ')
  #plt.text(60, .025, r'$\mu=100,\ \sigma=15$')
  #plt.axis([40, 160, 0, 0.03])
  #plt.grid(True)
  plt.show()



#+END_SRC
*** histogram y axis with log index
#+BEGIN_SRC  ipython :results output

  import numpy as np
  import matplotlib.pyplot as plt
  from matplotlib.ticker import NullFormatter  # useful for `logit` scale

  # Fixing random state for reproducibility
  np.random.seed(19680801)

  # make up some data in the interval ]0, 1[
  y = np.random.normal(loc=0.5, scale=0.4, size=1000)
  y = y[(y > 0) & (y < 1)]
  y.sort()
  x = np.arange(len(y))

  # plot with various axes scales
  plt.figure(1)

  # linear
  plt.subplot(221)
  plt.plot(x, y)
  plt.yscale('linear')
  plt.title('linear')
  plt.grid(True)


  # log
  plt.subplot(222)
  plt.plot(x, y)
  plt.yscale('log')
  plt.title('log')
  plt.grid(True)


  # symmetric log
  plt.subplot(223)
  plt.plot(x, y - y.mean())
  plt.yscale('symlog', linthreshy=0.01)
  plt.title('symlog')
  plt.grid(True)

  # logit
  plt.subplot(224)
  plt.plot(x, y)
  plt.yscale('logit')
  plt.title('logit')
  plt.grid(True)
  # Format the minor tick labels of the y-axis into empty strings with
  # `NullFormatter`, to avoid cumbering the axis with too many labels.
  plt.gca().yaxis.set_minor_formatter(NullFormatter())
  # Adjust the subplot layout, because the logit one may take more space
  # than usual, due to y-tick labels like "1 - 10^{-3}"
  plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25, wspace=0.35)

  plt.show()


#+END_SRC
*** a example of mean standrad deviation plotting 
#+BEGIN_SRC  ipython :results output
  import matplotlib.pyplot as plt;
  import numpy as np
  import scipy.stats
  import scipy as sp
  from scipy import integrate
  from scipy.optimize import curve_fit

  data = np.genfromtxt('Highz_SN_data.txt');

  redshift = np.array([data[:,2]])
  mu = np.array([data[:,3]])
  velocity = np.array([data[:,4]])
  redshift_mean = np.mean(redshift)
  mu_mean = np.mean(mu)
  velocity_mean = np.mean(velocity)
  redshift_std = np.std(redshift)
  mu_std = np.std(mu)
  velocity_std = np.std(velocity)
  para = ["redshift", "mu", "velocity"]
  x_pos = np.arange(len(para))
  ctes = [redshift_mean, mu_mean, velocity_mean]
  error = [redshift_std, mu_std, velocity_std]


  fig, ax = plt.subplots()
  ax.bar(x_pos, ctes, yerr=error, align='center', alpha=0.5, ecolor='black', capsize=10)
  ax.set_ylabel('Coefficient of Thermal Expansion ($\degree C^{-1}$)')
  ax.set_xticks(x_pos)
  ax.set_xticklabels(para)
  ax.set_title('Coefficent of Thermal Expansion (CTE) of Three Metals')
  ax.yaxis.grid(True)

  # Save the figure and show
  plt.tight_layout()
  plt.savefig('bar_plot_with_error_bars.png')
  plt.show()

#+END_SRC

** pandas 
*** plot.scatter()
#+begin_src python :results output
  import pandas as pd
  import numpy as np
  a = pd.DataFrame(np.random.rand(3,2))
  print(a)
  a.plot.scatter(x=0, y = 1)


#+end_src

#+RESULTS:
:           0         1
: 0  0.768941  0.501116
: 1  0.514047  0.526439
: 2  0.843669  0.523868

* Tensorflow 1
** Constand additional und multiply
*** addition
#+BEGIN_SRC python   :results output :session
  import tensorflow as tf
  a = tf.constant([1.0, 2.0])
  b = tf.constant([3.0, 4.0])

  a_m = tf.constant([[1.0, 2.0]])
  b_m = tf.constant([[3.0], [4.0]])

  result_add = a + b
  result_multpl = tf.matmul(a_m, b_m)

  with tf.Session() as sess:
      print (sess.run(result_add))
      print (sess.run(result_multpl))

  print (result_add)
  print (result_multpl)
#+END_SRC

*** multiply

#+BEGIN_SRC python   :results output :session

  import tensorflow as tf
  #创建常量
  m1= tf.constant([[3,3]])
  m2=tf.constant([[1],[2]])
  #相乘
  product = tf.matmul(m1,m2)
  print(product)

  #定义一个会话，启动默认图
  sess = tf.Session()
  #调用sess，执行乘法运算
  result = sess.run(product)
  print(result)
  #不要忘了关闭sess
  sess.close()

  with tf.Session() as sess:
      result = sess.run(product)
      print(result)
  #使用with不需要专门关闭sess


#+END_SRC

#+RESULTS:
: Tensor("MatMul:0", shape=(1, 1), dtype=int32)
: [[9]]
: [[9]]

** Variable
*** the first using of variable & subtract add 
#+BEGIN_SRC python :results output
  import tensorflow as tf
  x = tf.Variable([1,2])
  a = tf.constant([3,3])
  sub = tf.subtract(x,a)
  add = tf.add(x,sub)
# 对于变量，要初始化init
  init = tf.global_variables_initializer()
  with tf.Session() as sess:
      sess.run(init)
      print(sess.run(sub))
      print(sess.run(add))


#+END_SRC

#+RESULTS:
: [-2 -1]
: [-1  1]

*** assign a value for variable and update & assign 
#+BEGIN_SRC python :results output
  #............................．．．．．．变量计算
  #变量可以被起名，初始化为０
  import tensorflow as tf
  state = tf.Variable(0, name = 'counter')
  new_value = tf.add(state,1)
  #赋值功能assign
  update = tf.assign(state,new_value)
  init = tf.global_variables_initializer()
  with tf.Session() as sess:
      sess.run(init)
      print(state)
      for _ in range(5):
          sess.run(update)
          print(sess.run(state))
          print(sess.run(update))


#+END_SRC

#+RESULTS:
#+begin_example
<tf.Variable 'counter:0' shape=() dtype=int32_ref>
1
2
3
4
5
6
7
8
9
10
#+end_example

*** fetch and Feed &placeholder
#+BEGIN_SRC python :results output
  import tensorflow as tf
  # Fetch 在一个会话里执行多个op
  input1 = tf.constant(3.0)
  input2 = tf.constant(2.0)
  input3 = tf.constant(5.0)

  add = tf.add(input2,input3)
  mul = tf.multiply(input1,add)

  with tf.Session() as sess:
      result = sess.run([add,mul])  #有[]
      print(result)

  #------------------Feed
  #创建占位符
  input4= tf.placeholder(tf.float32)
  input5 = tf.placeholder(tf.float32)
  output = tf.multiply(input4, input5)
  with tf.Session() as sess:
      print(sess.run(output,feed_dict = {input4:[7.0],input5:[2.0]}))  #随后赋值是用字典的方式进行的feed_dict = {input4:[7.0],input5:[2.0]}, 数字还加了方括号．


#+END_SRC

#+RESULTS:
: [7.0, 21.0]
: [14.]

** Tuning
*** change learing rate
#+BEGIN_SRC python
  #coding:utf-8

  import tensorflow as tf
  LEARING_RATE_BASE = 0.1
  LEARING_RATE_DECAY = 0.99
  LEARING_RATE_STEP= 1

  global_step = tf.Variable(0,trainable = False)
  learning_rate = tf.train.exponential_decay(LEARING_RATE_BASE, global_step,
  LEARING_RATE_STEP, LEARING_RATE_DECAY, staircase = True)
  
  w = tf.Variable(tf.constant(5, dtype = tf.float32))
  loss = tf.square(w+1)

  train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step = global_step)

  with tf.Session() as sess:
      init_op = tf.global_variables_initializer()
      sess.run(init_op)
      for i in range(40):
          sess.run(train_step)
          learnin_rate_val = sess.run(learning_rate)
          global_step_val = sess.run(global_step)
          w_val = sess.run(w)
          loss_val = sess.run(loss)
          print(" After {} steps: global_step is {}, w is {}, learnin_rate is {}, loss is {}"
  .format(i, global_step_val, w_val, learnin_rate_val, loss_val))


#+END_SRC
*** learing rate for loss
#+BEGIN_SRC python :results output
  import tensorflow as tf
  w = tf.Variable(tf.constant(5,dtype=tf.float32))
  loss = tf.square(w+1)
  train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)

  with tf.Session() as sess:
      init_op=tf.global_variables_initializer()
      sess.run(init_op)
      for i in range(50):
          sess.run(train_step)
          w_var = sess.run(w)
          loss_val = sess.run(loss)
          print("After {} steps: w is {}, loss is {}".format(i,w_var,loss_val))
          #print("After %s steps: w is %f, loss is %f." %(i,w_var, loss_val))


#+END_SRC

** train examples
*** the first train case
#+BEGIN_SRC python :results output
  import tensorflow as tf
  import numpy as np
  import matplotlib.pyplot as plt
  #生成随机数据
  x_date = np.random.rand(100)
  y_date = x_date*0.4 + 2

  #构造线性模型
  b = tf.Variable(0.)
  k = tf.Variable(0.)
  y = k*x_date + b

  #构造二次代价函数
  loss = tf.reduce_mean(tf.square(y_date-y))
  #定义梯度下降的优化器
  optimizer = tf.train.GradientDescentOptimizer(0.2)
  #定义一个最小化代价函数
  train = optimizer.minimize(loss)
  init = tf.global_variables_initializer()
  with tf.Session() as sess:
      sess.run(init)
      for steps in range(201):
          sess.run(train)
          if steps%20 == 0:
              print(steps, sess.run([k,b]))
      prediction_value = sess.run(y)
      plt.figure()
      plt.scatter(x_date, y_date)
      plt.plot(x_date,prediction_value,'r-',lw=5)
      plt.show()



#+END_SRC

#+RESULTS:
#+begin_example
0 [0.44114023, 0.87820524]
20 [0.7041128, 1.841991]
40 [0.58822596, 1.9022033]
60 [0.5164994, 1.9394703]
80 [0.47210538, 1.9625362]
100 [0.44462854, 1.9768122]
120 [0.4276222, 1.9856484]
140 [0.4170963, 1.9911172]
160 [0.41058153, 1.9945022]
180 [0.40654916, 1.9965973]
200 [0.40405348, 1.9978939]
#+end_example

*** non linear regression case
#+BEGIN_SRC python
  #----------------------------非线性回归
  import tensorflow as tf
  import numpy as np
  import matplotlib.pyplot as plt

  #构造数据
  x_date = np.linspace (-0.5,0.5,100)[:,np.newaxis] #np.newaxis 功能同None，将行变列
  noise = np.random.normal(0,0.02,x_date.shape)
  y_date = np.square(x_date)+noise

  x = tf.placeholder(tf.float32,[None,1])
  y = tf.placeholder(tf.float32,[None,1])

  #构建神经网络
  Weight_L1 = tf.Variable(tf.random_normal([1,10]))
  Biase_L1 = tf.Variable(tf.zeros([1,10]))
  Wx_plus_b_L1 = tf.matmul(x,Weight_L1)+Biase_L1
  L1 = tf.nn.tanh(Wx_plus_b_L1)

  #定义输出层
  Weight_L2 = tf.Variable(tf.random_normal([10,1]))
  Biase_L2 = tf.Variable(tf.zeros([1,1]))
  Wx_plus_b_L2 = tf.matmul(L1,Weight_L2)+ Biase_L2
  prediction = tf.nn.tanh(Wx_plus_b_L2)

  #二次代价函数
  loss = tf.reduce_mean(tf.square(y-prediction))
  train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      for _ in range(2000):
          sess.run(train_step,feed_dict={x:x_date,y:y_date})

      #训练好后，用来做预测
      prediction_value = sess.run(prediction,feed_dict={x:x_date})
      plt.figure()
      plt.scatter(x_date, y_date)
      plt.plot(x_date,prediction_value,'r-',lw=5)
      plt.show()


#+END_SRC

#+RESULTS:
: None

*** the first train with data for accuary
#+BEGIN_SRC python
  import tensorflow as tf
  from tensorflow.examples.tutorials.mnist import input_data
  #载入数据集
  mnist= input_data.read_data_sets('MNIST_data', one_hot = True)

  #设定每个批次的大小
  batch_size = 100
  #计算总共的批次
  n_batch = mnist.train.num_examples // batch_size

  #参数统计
  def variable_summries(var):
      with tf.name_scope('summaries'):
          mean = tf.reduce_mean(var)
          tf.summary.scalar('mean',mean)
          with tf.name_scope('stddev'):
              stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))
          tf.summary.scalar('stddev',stddev)
          tf.summary.scalar('max',tf.reduce_max(var))
          tf.summary.scalar('min',tf.reduce_min(var))
          tf.summary.histogram('histogram',var)

  #命名空间
  with tf.name_scope('input'):
      x = tf.placeholder(tf.float32,[None,784])
      y = tf.placeholder(tf.float32,[None,10])

  with tf.name_scope('layers'):
      with tf.name_scope('wight'):
          W = tf.Variable(tf.truncated_normal([784,10]))
          variable_summries(W)
      with tf.name_scope('biases'):
          B = tf.Variable(tf.zeros([10])+0.1)
          variable_summries(B)
      with tf.name_scope('wx_plus_b'):
          wx_plus_b=tf.matmul(x,W)+B
      with tf.name_scope('softmax'):    
          prediction = tf.nn.tanh(wx_plus_b)

  #定义二次代价函数
  #loss = tf.reduce_mean(tf.square(y-prediction))
  #重新定义对数(交叉熵)
  with tf.name_scope('loss'):
      loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))
      tf.summary.scalar('loss',loss)
  #使用梯度下降法

  with tf.name_scope('train'):
      train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
  #    train_step = tf.train.AdamOptimizer(0.05).minimize(loss)
  #初始化
  init = tf.global_variables_initializer()

  #测试准确率
  with tf.name_scope('accuracy'):
      with tf.name_scope('correct_prediction'):
          correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))
      with tf.name_scope('accuracy'):
          accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
          tf.summary.scalar('accuracy',accuracy)

  #合并summary
  merged = tf.summary.merge_all()
          #训练开始
  with tf.Session() as sess:
      writer = tf.summary.FileWriter('pics/',sess.graph)
      sess.run(init)
      for epoch in range(51):
          for batch in range(n_batch):
              batch_xs, batch_ys = mnist.train.next_batch(batch_size)
              summary,_ = sess.run([merged,train_step], feed_dict={x:batch_xs, y:batch_ys})
  #            writer.add_summary(summary,batch)
          writer.add_summary(summary,epoch)
          acc = sess.run(accuracy,feed_dict={x:mnist.test.images, y:mnist.test.labels})
          print("准确率为:　在"+str(epoch)+"回,"+str(acc))



#+END_SRC

#+RESULTS:
: None

*** a simple CNN case
#+BEGIN_SRC python  :results output
  import tensorflow as tf
  import numpy as np
  BATCH_SIZE = 8
  seed = 23455

  rng = np.random.RandomState(seed)
  X = rng.rand(32, 2)
  Y = [[int(X0+X1 < 1)] for (X0, X1) in X]
  # print ("X is :", X)
  # print ("Y is :", Y)

  x = tf.placeholder(tf.float32, shape = (None, 2))
  y_ = tf.placeholder(tf.float32, shape = (None, 1))

  w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1))
  w = tf.Variable(tf.random_normal([3,3], stddev=1, seed=1))
  w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1))

  a = tf.matmul(x,w1)
  b = tf.matmul(a,w)
  y = tf.matmul(b,w2)

  loss = tf.reduce_mean(tf.square(y-y_))
  train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)

  with tf.Session() as sess:
      init_op = tf.global_variables_initializer()
      sess.run(init_op)
      print("w1 is :", sess.run(w1))
      print("w is :", sess.run(w))
      print("w2 is :", sess.run(w2))

      steps= 30000
      for i in range(steps):
          start = (i*BATCH_SIZE) % 32
          end = start + BATCH_SIZE
          sess.run(train_step, feed_dict={x:X[start:end], y_:Y[start:end]})
          if i % 5000 == 0:
              total_loss = sess.run(loss, feed_dict={x:X, y_:Y})
              print("After %d training steps, loss on all data is %g" %(i,total_loss))
      print("\n")
      print("w1 is :", sess.run(w1))
      print("w is :", sess.run(w))
      print("w2 is :", sess.run(w2))

#+END_SRC

#+RESULTS:
#+begin_example
w1 is : [[-0.8113182   1.4845988   0.06532937]
 [-2.4427042   0.0992484   0.5912243 ]]
w is : [[-0.8113182   1.4845988   0.06532937]
 [-2.4427042   0.0992484   0.5912243 ]
 [ 0.59282297 -2.1229296  -0.72289723]]
w2 is : [[-0.8113182 ]
 [ 1.4845988 ]
 [ 0.06532937]]
After 0 training steps, loss on all data is 27.0734
After 5000 training steps, loss on all data is 0.383562
After 10000 training steps, loss on all data is 0.383562
After 15000 training steps, loss on all data is 0.383562
After 20000 training steps, loss on all data is 0.383562
After 25000 training steps, loss on all data is 0.383561


w1 is : [[-0.84913164  1.3203508   0.219825  ]
 [-1.9427874   0.7993799  -0.2589098 ]]
w is : [[-0.31725118  0.802253    0.02743915]
 [-2.471998    0.16294315  0.5947651 ]
 [ 0.56031865 -2.0743563  -0.7197714 ]]
w2 is : [[-0.45712712]
 [ 0.67528236]
 [ 0.02388799]]
#+end_example

*** CNN train
#+BEGIN_SRC python
  import tensorflow as tf
  from tensorflow.examples.tutorials.mnist import input_data

  mnist=input_data.read_data_sets("MNIST_data",one_hot = True)

  batch_size = 100
  n_batch = mnist.train.num_examples // batch_size

  #初始化权值
  def weight_variable(shape):
      return tf.Variable(tf.truncated_normal(shape,stddev =0.01))

  #初始化偏置
  def bias_variable(shape):
      return tf.Variable(tf.constant(0.1,shape= shape))

  #定义卷积层
  def conv2d(x,W):
      return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding="SAME") 

  #池化层定义
  def max_pool_2x2(x):
      return tf.nn.max_pool(x,ksize =[1,2,2,1],strides=[1,2,2,1], padding = "SAME")

  x = tf.placeholder(tf.float32,[None, 784])
  y = tf.placeholder(tf.float32,[None,10])

  x_image = tf.reshape(x,[-1,28,28,1])

  #初始化第一个卷积层的权值和偏置，输入其要求的形状
  W_convl = weight_variable([5,5,1,32]) #5x5的采样窗口大小，１通道对黑白，３通道对彩色
  b_convl = bias_variable([32])

  #现在卷积
  h_conv1 = tf.nn.relu(conv2d(x_image,W_convl) +b_convl)
  #现在池化
  h_pool1 = max_pool_2x2(h_conv1) 

  ###定义第二个卷积层
  W_convl2 = weight_variable([5,5,32,64]) #5x5的采样窗口大小，１通道对黑白，３通道对彩色
  b_convl2 = bias_variable([64])

  #现在卷积
  h_conv2 = tf.nn.relu(conv2d(h_pool1 ,W_convl2) +b_convl2)
  #现在池化
  h_pool2 = max_pool_2x2(h_conv2)

  #池化后将结果扁平化处理，以便输入网络
  h_pool2_flat = tf.reshape(h_pool2, [-1,7*7*64])

  #建立第一个神经网络的全连接层，初始化其权重和偏置
  W_fcl = weight_variable([7*7*64, 100])
  b_fcl = bias_variable([100])

  #第一层的计算
  h_fcl = tf.nn.relu(tf.matmul(h_pool2_flat, W_fcl) + b_fcl)

  #dropout
  keep_prob = tf.placeholder(tf.float32)
  h_fcl_drop = tf.nn.dropout(h_fcl, keep_prob)

  #建立第二个神经层
  W_fc2 = weight_variable([100,10])
  b_fc2 = bias_variable([10])
  prediction = tf.nn.softmax(tf.matmul(h_fcl_drop, W_fc2)+b_fc2)

  #交叉熵
  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits=prediction))
  #优化
  train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

  #结果
  correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))
  #准确率
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      for epoch in range(21):
          for batch in range(n_batch):
              batch_xs, batch_ys = mnist.train.next_batch(batch_size)
              sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys, keep_prob:0.7})
          acc= sess.run(accuracy, feed_dict={x:mnist.test.images,y:mnist.test.labels, keep_prob:1.0})
          print("在第"+str(epoch)+"轮，准确率为"+str(acc))

#+END_SRC
*** classification
#+BEGIN_SRC python

  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  import tensorflow as tf
  from tensorflow.examples.tutorials.mnist import input_data
  # number 1 to 10 data
  mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

  def add_layer(inputs, in_size, out_size, activation_function=None,):
      # add one more layer and return the output of this layer
      Weights = tf.Variable(tf.random_normal([in_size, out_size]))
      biases = tf.Variable(tf.zeros([1, out_size]) + 0.1,)
      Wx_plus_b = tf.matmul(inputs, Weights) + biases
      if activation_function is None:
          outputs = Wx_plus_b
      else:
          outputs = activation_function(Wx_plus_b,)
      return outputs

  def compute_accuracy(v_xs, v_ys):
      global prediction
      y_pre = sess.run(prediction, feed_dict={xs: v_xs})
      correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
      result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys})
      return result

  # define placeholder for inputs to network
  xs = tf.placeholder(tf.float32, [None, 784]) # 28x28
  ys = tf.placeholder(tf.float32, [None, 10])

  # add output layer
  prediction = add_layer(xs, 784, 10,  activation_function=tf.nn.softmax)

  # the error between prediction and real data
  cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),
                                                reduction_indices=[1]))       # loss
  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

  sess = tf.Session()
  # important step
  # tf.initialize_all_variables() no long valid from
  # 2017-03-02 if using tensorflow >= 0.12
  if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:
      init = tf.initialize_all_variables()
  else:
      init = tf.global_variables_initializer()
  sess.run(init)

  for i in range(1000):
      batch_xs, batch_ys = mnist.train.next_batch(100)
      sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys})
      if i % 50 == 0:
          print(compute_accuracy(
              mnist.test.images, mnist.test.labels))


#+END_SRC
*** optimizer varialbe (with error)
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  import tensorflow as tf
  import numpy as np


  def add_layer(inputs, in_size, out_size, n_layer, activation_function=None):
      # add one more layer and return the output of this layer
      layer_name = 'layer%s' % n_layer
      with tf.name_scope(layer_name):
          with tf.name_scope('weights'):
              Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')
              tf.summary.histogram(layer_name + '/weights', Weights)
          with tf.name_scope('biases'):
              biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name='b')
              tf.summary.histogram(layer_name + '/biases', biases)
          with tf.name_scope('Wx_plus_b'):
              Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)
          if activation_function is None:
              outputs = Wx_plus_b
          else:
              outputs = activation_function(Wx_plus_b, )
          tf.summary.histogram(layer_name + '/outputs', outputs)
      return outputs


  # Make up some real data
  x_data = np.linspace(-1, 1, 300)[:, np.newaxis]
  noise = np.random.normal(0, 0.05, x_data.shape)
  y_data = np.square(x_data) - 0.5 + noise

  # define placeholder for inputs to network
  with tf.name_scope('inputs'):
      xs = tf.placeholder(tf.float32, [None, 1], name='x_input')
      ys = tf.placeholder(tf.float32, [None, 1], name='y_input')

  # add hidden layer
  l1 = add_layer(xs, 1, 10, n_layer=1, activation_function=tf.nn.relu)
  # add output layer
  prediction = add_layer(l1, 10, 1, n_layer=2, activation_function=None)

  # the error between prediciton and real data
  with tf.name_scope('loss'):
      loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),
                                          reduction_indices=[1]))
      tf.summary.scalar('loss', loss)

  with tf.name_scope('train'):
      train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

  sess = tf.Session()
  merged = tf.summary.merge_all()

  writer = tf.summary.FileWriter("logs/", sess.graph)

  init = tf.global_variables_initializer()
  sess.run(init)

  for i in range(1000):
      sess.run(train_step, feed_dict={xs: x_data, ys: y_data})
      if i % 50 == 0:
          result = sess.run(merged,
                            feed_dict={xs: x_data, ys: y_data})
          writer.add_summary(result, i)

  # direct to the local dir and run this in terminal:
  # $ tensorboard --logdir logs


#+END_SRC
*** dropout
#+BEGIN_SRC python
import tensorflow as tf
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data
#载入数据集
mnist= input_data.read_data_sets('MNIST_data', one_hot = True)

#设定每个批次的大小
batch_size = 100
#计算总共的批次
n_batch = mnist.train.num_examples//batch_size

x = tf.placeholder(tf.float32,[None,784])
y = tf.placeholder(tf.float32,[None,10])
keep_prob = tf.placeholder(tf.float32)
#构建神经网络
W = tf.Variable(tf.truncated_normal([784,2000], stddev = 0.1))
B = tf.Variable(tf.zeros([2000])+0.1)
p1 = tf.nn.softmax(tf.matmul(x,W)+B)
p1_dropout = tf.nn.dropout(p1,keep_prob)

W1 = tf.Variable(tf.truncated_normal([2000,2000]))
B1 = tf.Variable(tf.zeros([2000])+0.1)
p2 = tf.nn.softmax(tf.matmul(p1_dropout,W1)+B1)
p2_dropout = tf.nn.dropout(p2,keep_prob)

W2 = tf.Variable(tf.truncated_normal([2000,10]))
B2 = tf.Variable(tf.zeros([10])+0.1)
prediction = tf.nn.softmax(tf.matmul(p2_dropout,W2)+B2)

#定义二次代价函数
#loss = tf.reduce_mean(tf.square(y-prediction))
#重新定义对数(交叉熵)
loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))
#使用梯度下降法
train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)

#初始化
init = tf.global_variables_initializer()

#测试准确率
correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))

#训练开始
with tf.Session() as sess:
    sess.run(init)
    for epoch in range(21):
        for batch in range(n_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys,keep_prob:1.0})

        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images, y:mnist.test.labels, keep_prob:1.0})
        train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images, y:mnist.train.labels, keep_prob:1.0})
        print("准确率为:　在"+str(epoch)+"回,"+str(test_acc)+", 但是在训练集中为"+str(train_acc))


#+END_SRC
*** number identification
#+BEGIN_SRC python
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
#载入数据集
mnist= input_data.read_data_sets('MNIST_data', one_hot = True)

#设定每个批次的大小
batch_size = 100
#计算总共的批次
n_batch = mnist.train.num_examples // batch_size

x = tf.placeholder(tf.float32,[None,784])
y = tf.placeholder(tf.float32,[None,10])
keep_prob = tf.placeholder(tf.float32)
lr= tf.Variable(0.001,dtype=tf.float32)
#构建神经网络

# W = tf.Variable(tf.truncated_normal([784,10]))
# B = tf.Variable(tf.zeros([10])+0.1)
# prediction = tf.nn.softmax(tf.matmul(x,W)+B)

# W = tf.Variable(tf.zeros([784,10]))
# B = tf.Variable(tf.zeros([10]))
# prediction = tf.nn.softmax(tf.matmul(x,W)+B)
 
#构建神经网络
W = tf.Variable(tf.truncated_normal([784,500],stddev =0.1))
B = tf.Variable(tf.zeros([500])+0.1)
p1 = tf.nn.tanh(tf.matmul(x,W)+B)
p1_dropout = tf.nn.dropout(p1,keep_prob)

W1 = tf.Variable(tf.truncated_normal([500,200],stddev=0.1))
B1 = tf.Variable(tf.zeros([200])+0.1)
p2 = tf.nn.tanh(tf.matmul(p1_dropout,W1)+B1)
p2_dropout = tf.nn.dropout(p2,keep_prob)

W2 = tf.Variable(tf.truncated_normal([200,10],stddev = 0.1))
B2 = tf.Variable(tf.zeros([10])+0.1)
prediction = tf.nn.softmax(tf.matmul(p2_dropout,W2)+B2)



#定义二次代价函数
#loss = tf.reduce_mean(tf.square(y-prediction))
#重新定义对数(交叉熵)
loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))
#使用梯度下降法
#train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
train_step = tf.train.AdamOptimizer(lr).minimize(loss)
#初始化
init = tf.global_variables_initializer()

#测试准确率
correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))

# #训练开始
# with tf.Session() as sess:
#     sess.run(init)
#     for epoch in range(20):
#         for batch in range(n_batch):
#             batch_xs, batch_ys = mnist.train.next_batch(batch_size)
#             sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys})
#         acc = sess.run(accuracy,feed_dict={x:mnist.test.images, y:mnist.test.labels})
#         print("准确率为:　在"+str(epoch)+"回,"+str(acc))


#训练开始
with tf.Session() as sess:
    sess.run(init)
    for epoch in range(41):
        sess.run(tf.assign(lr,0.001*(0.95**epoch)))
        for batch in range(n_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys,keep_prob:1.0})
        learing_rate = sess.run(lr)
        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,
y:mnist.test.labels, keep_prob:1.0})
        train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images,
y:mnist.train.labels, keep_prob:1.0})
        print("准确率为:　在"+str(epoch)+"回,"+str(test_acc)+",但是在训练集中为"+str(train_acc)+"同时学习率为"+str(learing_rate))


#+END_SRC
*** with L regularizer 
#+BEGIN_SRC python

  import tensorflow as tf
  import numpy as np
  import matplotlib.pyplot as plt
  BATCH_SIZE = 30
  seed = 2
  rdm = np.random.RandomState(seed)
  X = rdm.randn(300,2)
  Y_ = [int(x0*x0 + x1*x1 < 2) for (x0,x1) in X]
  Y_c = [['red' if y else 'blue'] for y in Y_]
  X = np.vstack(X).reshape(-1,2)
  Y_ = np.vstack(Y_).reshape(-1,1)
  print(X)
  print(Y_)
  print(Y_c)
  plt.scatter(X[:,0],X[:,1], c = np.squeeze(Y_c))
  plt.show()

  def get_weight(shape, regularizer):
      w = tf.Variable(tf.random_normal(shape), dtype = tf.float32)
      tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))
      return w
  def get_bias(shape):
      b = tf.Variable(tf.constant(0.01, shape=shape))
      return b
  x = tf.placeholder(tf.float32, shape=(None, 2))
  y_ = tf.placeholder(tf.float32, shape = (None , 1))
  w1 = get_weight([2,11], 0.01)
  b1 = get_bias([11])
  y1 = tf.nn.relu(tf.matmul(x,w1)+b1)
  w2 = get_weight([11,1],0.01)
  b2 = get_bias([1])
  y = tf.matmul(y1,w2)+b2

  loss_mse = tf.reduce_mean(tf.square(y-y_))
  loss_total = loss_mse + tf.add_n(tf.get_collection('losses'))

  #########with Regularizer#########
  train_step_l = tf.train.AdamOptimizer(0.0001).minimize(loss_total)
  with tf.Session() as sess:
      init_op = tf.global_variables_initializer()
      sess.run(init_op)
      STEPS = 40000
      for i in range(STEPS):
          start = (i*BATCH_SIZE) % 300
          end = start + BATCH_SIZE
          sess.run(train_step_l, feed_dict = {x:X[start:end], y_:Y_[start:end]})
          if i % 2000 == 0:
              loss_total_v = sess.run(loss_total, feed_dict={x:X,y_:Y_})
              print('After %d steps, loss is: %f' %(i, loss_total_v))
      xx, yy = np.mgrid[-3:3:0.01, -3:3:0.01]
      grid = np.c_[xx.ravel(), yy.ravel()]
      probs = sess.run(y, feed_dict={x:grid})
      probs = probs.reshape(xx.shape)
      print ('w1 is \n:', sess.run(w1))
      print ('b1 is \n:', sess.run(b1))
      print ('w2 is \n:', sess.run(w2))
      print ('b2 is \n:', sess.run(b2))
  plt.scatter(X[:,0], X[:,1], c = np.squeeze(Y_c))
  plt.contour(xx, yy, probs, levels = [.5])
  plt.show()


  print('loss_mse_v * loss_total_v is :', loss_mse_v*loss_total_v)


#+END_SRC
*** without L regularizer
#+BEGIN_SRC python
  import tensorflow as tf
  import numpy as np
  import matplotlib.pyplot as plt
  BATCH_SIZE = 30
  seed = 2
  rdm = np.random.RandomState(seed)
  X = rdm.randn(300,2)
  Y_ = [int(x0*x0 + x1*x1 < 2) for (x0,x1) in X]
  Y_c = [['red' if y else 'blue'] for y in Y_]
  X = np.vstack(X).reshape(-1,2)
  Y_ = np.vstack(Y_).reshape(-1,1)
  print(X)
  print(Y_)
  print(Y_c)
  plt.scatter(X[:,0],X[:,1], c = np.squeeze(Y_c))
  plt.show()

  def get_weight(shape, regularizer):
      w = tf.Variable(tf.random_normal(shape), dtype = tf.float32)
      tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))
      return w
  def get_bias(shape):
      b = tf.Variable(tf.constant(0.01, shape=shape))
      return b
  x = tf.placeholder(tf.float32, shape=(None, 2))
  y_ = tf.placeholder(tf.float32, shape = (None , 1))
  w1 = get_weight([2,11], 0.01)
  b1 = get_bias([11])
  y1 = tf.nn.relu(tf.matmul(x,w1)+b1)
  w2 = get_weight([11,1],0.01)
  b2 = get_bias([1])
  y = tf.matmul(y1,w2)+b2

  loss_mse = tf.reduce_mean(tf.square(y-y_))
  loss_total = loss_mse + tf.add_n(tf.get_collection('losses'))
  #########without Regularizer#########
  train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_mse)
  with tf.Session() as sess:
      init_op = tf.global_variables_initializer()
      sess.run(init_op)
      STEPS = 40000
      for i in range(STEPS):
          start = (i*BATCH_SIZE) % 300
          end = start + BATCH_SIZE
          sess.run(train_step, feed_dict = {x:X[start:end], y_:Y_[start:end]})
          if i % 2000 == 0:
              loss_mse_v = sess.run(loss_mse, feed_dict={x:X,y_:Y_})
              print('After %d steps, loss is: %f' %(i, loss_mse_v))
      xx, yy = np.mgrid[-3:3:0.01, -3:3:0.01]
      grid = np.c_[xx.ravel(), yy.ravel()]
      probs = sess.run(y, feed_dict={x:grid})
      probs = probs.reshape(xx.shape)
      print ('w1 is \n:', sess.run(w1))
      print ('b1 is \n:', sess.run(b1))
      print ('w2 is \n:', sess.run(w2))
      print ('b2 is \n:', sess.run(b2))
  plt.scatter(X[:,0], X[:,1], c = np.squeeze(Y_c))
  plt.contour(xx, yy, probs, levels = [.5])
  plt.show()


#+END_SRC
*** exponen decay with L regularizer(this has error)
#+BEGIN_SRC python
  import numpy as np
  import matplotlib.pyplot as plt
  ######### import opt4_8_generateds
  ######### import opt4_8_forward


  try:
      import opt4_8_generateds
      import opt4_8_forward
  except:
      import pip 
      pip.main(['install','opt4_8_forward'])
      pip.main(['install','opt4_8_generateds'])
      import opt4_8_generateds
      import opt4_8_forward



  STEPS = 40000
  BATCH_SIZE = 30
  LEARNING_RATE_BASE = 0.001
  LEARNING_RATE_DECAY = 0.999
  REGULARIZER = 0.01

  def backward():
      x = tf.placeholder(tf.float32, shape = (None,2))
      y_ = tf.placeholder(tf.float32, shape = (None,1))

      x , Y_, Y_c = opt4_8_generateds.generateds()
      y = opt4_8_forward.forward(x,REGULARIZER)

      global_steps = tf.Variable(0, trainable= False)
      learning_rate = tf.train.exponential_decay(
          LEARNING_RATE_BASE,
          global_steps,
          300/BATCH_SIZE,
          LEARNING_RATE_DECAY,
          staircase = True)
      loss_mse = tf.reduce_mean(tf.square(y-y_))
      loss_total = loss_mse + tf.add_n(tf.get_collection('losses'))

      train_step = tf.train.AdamOptimizer(learning_rate).minimizer(loss_total)

      with tf.Session() as sess:
          init_op = tf.global_variables_initializer()
          sess.run(init_op)
          for i in range(STEPS):
              start = (i*BATCH_SIZE) % 300
              end = start + BATCH_SIZE
              sess.run(train_step, feed_dict = {x:X[start:end], y_:Y_[start:end]})
              if i % 2000 == 0:
                  loss_v = sess.run(loss_total, feed_dict={x:X,y_:Y_})
                  print('After %d steps, loss is: %f' %(i, loss_v))
          xx, yy = np.mgrid[-3:3:0.01, -3:3:0.01]
          grid = np.c_[xx.ravel(), yy.ravel()]
          probs = sess.run(y, feed_dict={x:grid})
          probs = probs.reshape(xx.shape)

      plt.scatter(X[:,0], X[:,1], c = np.squeeze(Y_c))
      plt.contour(xx, yy, probs, levels = [.5])
      plt.show()

  if __name__ == '__main__':
      backward()


#+END_SRC
*** read date for variables
#+BEGIN_SRC python :results output
  from tensorflow.examples.tutorials.mnist import input_data
  mnist = input_data.read_data_sets('./data', one_hot = True)

  # print(mnist.train.labels[0])
  # print(mnist.train.images[0])

  BATCH_SIZE = 200
  xs, ys = mnist.train.next_batch(BATCH_SIZE)
  print("xs shape :", xs.shape)
  print("ys shape :", ys.shape)


#+END_SRC

#+RESULTS:
: Extracting ./data/train-images-idx3-ubyte.gz
: Extracting ./data/train-labels-idx1-ubyte.gz
: Extracting ./data/t10k-images-idx3-ubyte.gz
: Extracting ./data/t10k-labels-idx1-ubyte.gz
: xs shape : (200, 784)
: ys shape : (200, 10)




#+startup: content
** keras
*** classification
#+BEGIN_SRC python


  # please note, all tutorial code are running under python3.5.
  # If you use the version like python2.7, please modify the code accordingly

  # 5 - Classifier example

  import numpy as np
  np.random.seed(1337)  # for reproducibility
  from keras.datasets import mnist
  from keras.utils import np_utils
  from keras.models import Sequential
  from keras.layers import Dense, Activation
  from keras.optimizers import RMSprop

  # download the mnist to the path '~/.keras/datasets/' if it is the first time to be called
  # X shape (60,000 28x28), y shape (10,000, )
  (X_train, y_train), (X_test, y_test) = mnist.load_data()

  # data pre-processing
  X_train = X_train.reshape(X_train.shape[0], -1) / 255.   # normalize
  X_test = X_test.reshape(X_test.shape[0], -1) / 255.      # normalize
  y_train = np_utils.to_categorical(y_train, num_classes=10)
  y_test = np_utils.to_categorical(y_test, num_classes=10)

  # Another way to build your neural net
  model = Sequential([
      Dense(32, input_dim=784),
      Activation('relu'),
      Dense(10),
      Activation('softmax'),
  ])

  # Another way to define your optimizer
  rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)

  # We add metrics to get more results you want to see
  model.compile(optimizer=rmsprop,
                loss='categorical_crossentropy',
                metrics=['accuracy'])

  print('Training ------------')
  # Another way to train the model
  model.fit(X_train, y_train, epochs=2, batch_size=32)

  print('\nTesting ------------')
  # Evaluate the model with the metrics we defined earlier
  loss, accuracy = model.evaluate(X_test, y_test)

  print('test loss: ', loss)
  print('test accuracy: ', accuracy)

#+END_SRC
*** regression
#+BEGIN_SRC python
  # # please note, all tutorial code are running under python3.5.
  # # If you use the version like python2.7, please modify the code accordingly

  # # 4 - Regressor example

  import numpy as np
  np.random.seed(1337)  # for reproducibility
  from keras.models import Sequential
  from keras.layers import Dense
  import matplotlib.pyplot as plt

  # create some data
  X = np.linspace(-1, 1, 200)
  np.random.shuffle(X)    # randomize the data
  Y = 0.5 * X + 2 + np.random.normal(0, 0.05, (200 ))
  # plot data
  plt.scatter(X, Y)
  plt.show()

  X_train, Y_train = X[:160], Y[:160]     # first 160 data points
  X_test, Y_test = X[160:], Y[160:]       # last 40 data points

  # build a neural network from the 1st layer to the last layer
  model = Sequential()

  model.add(Dense(units=1, input_dim=1)) 

  # choose loss function and optimizing method
  model.compile(loss='mse', optimizer='sgd')

  # training
  print('Training -----------')
  for step in range(301):
      cost = model.train_on_batch(X_train, Y_train)
      if step % 100 == 0:
          print('train cost: ', cost)

  # test
  print('\nTesting ------------')
  cost = model.evaluate(X_test, Y_test, batch_size=40)
  print('test cost:', cost)
  W, b = model.layers[0].get_weights()
  print('Weights=', W, '\nbiases=', b)

  # plotting the prediction
  Y_pred = model.predict(X_test)
  plt.scatter(X_test, Y_test)
  plt.plot(X_test, Y_pred)
  plt.show()
#+END_SRC
** sklearn
*** decisiontree
#+BEGIN_SRC python
  from sklearn.feature_extraction import DictVectorizer
  import csv
  from sklearn import preprocessing
  from sklearn import tree
  from sklearn.externals.six import StringIO
  import numpy as np
  import pandas as pd
  from pylab import *

  allElectronicsData = open("computer.csv")
  reader = csv.reader(allElectronicsData)
  headers = next(reader)
  print(headers)
  print(reader)

  featureList = []
  labelList = []
  for row in reader:
      labelList.append(row[len(row)-1])
      rowDict = {}
      for i in range(1,len(row)-1):
          rowDict[headers[i]] = row[i]
      featureList.append(rowDict)
  print(featureList)

  vec = DictVectorizer()
  dummyX = vec.fit_transform(featureList).toarray()
  print("dummyX: " + str(dummyX))
  print(vec.get_feature_names())

  print("labeList" + str(labelList))
  lb = preprocessing.LabelBinarizer()
  dummyY = lb.fit_transform(labelList)
  print("dummyY:" + str(dummyY))

  clf = tree.DecisionTreeClassifier(criterion= 'entropy')
  clf = clf.fit(dummyX,dummyY)
  print("clf :" + str(clf))

  # save as dot
  with open("output.dot","w") as f:
      f = tree.export_graphviz(clf, feature_names= vec.get_feature_names(), out_file = f)
    
  #in terminnal gives the flowwing comands

  #    dot -Tpdf -O output.dot
  #    xdg-open output.pdf

#+END_SRC
*** cross validation 1
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn.datasets import load_iris
  from sklearn.cross_validation import train_test_split
  from sklearn.neighbors import KNeighborsClassifier

  iris = load_iris()
  X = iris.data
  y = iris.target

  # test train split #
  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)
  knn = KNeighborsClassifier(n_neighbors=5)
  knn.fit(X_train, y_train)
  y_pred = knn.predict(X_test)
  print(knn.score(X_test, y_test))

  # this is cross_val_score #
  from sklearn.cross_validation import cross_val_score
  knn = KNeighborsClassifier(n_neighbors=5)
  scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')
  print(scores)

  # this is how to use cross_val_score to choose model and configs #
  from sklearn.cross_validation import cross_val_score
  import matplotlib.pyplot as plt
  k_range = range(1, 31)
  k_scores = []
  for k in k_range:
      knn = KNeighborsClassifier(n_neighbors=k)
      loss = -cross_val_score(knn, X, y, cv=10, scoring='mean_squared_error') # for regression
  ##    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy') # for classification
      k_scores.append(loss.mean())

  plt.plot(k_range, k_scores)
  plt.xlabel('Value of K for KNN')
  plt.ylabel('Cross-Validated Accuracy')
  plt.show()


#+END_SRC

*** cross validation 2
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn.learning_curve import  learning_curve
  from sklearn.datasets import load_digits
  from sklearn.svm import SVC
  import matplotlib.pyplot as plt
  import numpy as np

  digits = load_digits()
  X = digits.data
  y = digits.target
  train_sizes, train_loss, test_loss= learning_curve(
          SVC(gamma=0.01), X, y, cv=10, scoring='mean_squared_error',
          train_sizes=[0.1, 0.25, 0.5, 0.75, 1])
  train_loss_mean = -np.mean(train_loss, axis=1)
  test_loss_mean = -np.mean(test_loss, axis=1)

  plt.plot(train_sizes, train_loss_mean, 'o-', color="r",
               label="Training")
  plt.plot(train_sizes, test_loss_mean, 'o-', color="g",
               label="Cross-validation")

  plt.xlabel("Training examples")
  plt.ylabel("Loss")
  plt.legend(loc="best")
  plt.show()


#+END_SRC

*** corss validation 3
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn.learning_curve import  validation_curve
  from sklearn.datasets import load_digits
  from sklearn.svm import SVC
  import matplotlib.pyplot as plt
  import numpy as np

  digits = load_digits()
  X = digits.data
  y = digits.target
  param_range = np.logspace(-6, -2.3, 5)
  train_loss, test_loss = validation_curve(
          SVC(), X, y, param_name='gamma', param_range=param_range, cv=10,
          scoring='mean_squared_error')
  train_loss_mean = -np.mean(train_loss, axis=1)
  test_loss_mean = -np.mean(test_loss, axis=1)

  plt.plot(param_range, train_loss_mean, 'o-', color="r",
               label="Training")
  plt.plot(param_range, test_loss_mean, 'o-', color="g",
               label="Cross-validation")

  plt.xlabel("gamma")
  plt.ylabel("Loss")
  plt.legend(loc="best")
  plt.show()


#+END_SRC
*** data application
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn import datasets
  from sklearn.linear_model import LinearRegression
  import matplotlib.pyplot as plt

  loaded_data = datasets.load_boston()
  data_X = loaded_data.data
  data_y = loaded_data.target

  model = LinearRegression()
  model.fit(data_X, data_y)

  print(model.predict(data_X[:4, :]))
  print(data_y[:4])

  X, y = datasets.make_regression(n_samples=100, n_features=1, n_targets=1, noise=1)
  plt.scatter(X, y)
  plt.show()


#+END_SRC
*** eigenschaft function
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn import datasets
  from sklearn.linear_model import LinearRegression

  loaded_data = datasets.load_boston()
  data_X = loaded_data.data
  data_y = loaded_data.target

  model = LinearRegression()
  model.fit(data_X, data_y)

  print(model.predict(data_X[:4, :]))
  print(model.coef_)
  print(model.intercept_)
  print(model.get_params())
  print(model.score(data_X, data_y)) # R^2 coefficient of determination


#+END_SRC
*** iris
#+BEGIN_SRC python
  # View more python learning tutorial on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn import datasets
  from sklearn.model_selection import train_test_split
  from sklearn.neighbors import KNeighborsClassifier

  iris = datasets.load_iris()
  iris_X = iris.data
  iris_y = iris.target

  ##print(iris_X[:2, :])
  ##print(iris_y)

  X_train, X_test, y_train, y_test = train_test_split(
      iris_X, iris_y, test_size=0.3)

  ##print(y_train)

  knn = KNeighborsClassifier()
  knn.fit(X_train, y_train)
  print(knn.predict(X_test))
  print(y_test)


#+END_SRC
*** normnalization
#+BEGIN_SRC python
# View more python learning tutorial on my Youtube and Youku channel!!!

# Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
# Youku video tutorial: http://i.youku.com/pythontutorial

"""
Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
"""
from __future__ import print_function
from sklearn import preprocessing
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets.samples_generator import make_classification
from sklearn.svm import SVC
import matplotlib.pyplot as plt

a = np.array([[10, 2.7, 3.6],
                     [-100, 5, -2],
                     [120, 20, 40]], dtype=np.float64)
print(a)
print(preprocessing.scale(a))

X, y = make_classification(n_samples=300, n_features=2 , n_redundant=0, n_informative=2, random_state=22, n_clusters_per_class=1, scale=100)
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.show()
X = preprocessing.scale(X)    # normalization step
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)
clf = SVC()
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))


#+END_SRC
*** save load
#+BEGIN_SRC python
  # View more python tutorials on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  """
  Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
  """
  from __future__ import print_function
  from sklearn import svm
  from sklearn import datasets

  clf = svm.SVC()
  iris = datasets.load_iris()
  X, y = iris.data, iris.target
  clf.fit(X, y)

  # # method 1: pickle
  # import pickle
  # # save
  # with open('save/clf.pickle', 'wb') as f:
  #     pickle.dump(clf, f)
  # # restore
  # with open('save/clf.pickle', 'rb') as f:
  #    clf2 = pickle.load(f)
  #    print(clf2.predict(X[0:1]))

  # method 2: joblib
  from sklearn.externals import joblib
  # Save
  joblib.dump(clf, 'save/clf.pkl')
  # restore
  clf3 = joblib.load('save/clf.pkl')
  print(clf3.predict(X[0:1]))


#+END_SRC
** theano 
*** basic
#+BEGIN_SRC python :results output
  # View more python tutorials on my Youtube and Youku channel!!!

  # Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
  # Youku video tutorial: http://i.youku.com/pythontutorial

  # 4 - basic usage
  #   from __future__ import print_function
  import numpy as np
  import theano.tensor as T
  from theano import function

  # basic
  x = T.dscalar('x')
  y = T.dscalar('y')
  z = x+y     # define the actual function in here
  f = function([x, y], z)  # the inputs are in [], and the output in the "z"

  print(f(2,3))  # only give the inputs "x and y" for this function, then it will calculate the output "z"

  # # to pretty-print the function
  from theano import pp
  print(pp(z))

  # # how about matrix
  x = T.dmatrix('x')
  y = T.dmatrix('y')
  z = x + y
  f = function([x, y], z)
  print(f(np.arange(12).reshape((3,4)), 10*np.ones((3,4))))


#+END_SRC

#+RESULTS:
: 5.0
: (x + y)
: [[10. 11. 12. 13.]
:  [14. 15. 16. 17.]
:  [18. 19. 20. 21.]]

* Tensorflow 2 
** Install and Check GPU
*** install  driver 418-server, cuda 10.1, cudnn 7.6.1 
1. nouveau to backlist: add following to /etc/modprobe.d/blacklist.conf
   blacklist nouveau
   options nouveau modeset=0

2. sudo update-initramfs -u   
   
3. sudo apt update
   ubuntu-drivers devices
   sudo apt install nvidia-drivers-418-server
   sudo reboot

   then I will be able to check :   nvidia-smi


4. CUDA
   go to [[https://developer.nvidia.com/cuda-toolkit-archive]]
   select CUDA Toolkit 10.1 update 2 to install
   unmark the Drivers in  CUDA installation


5. emacs ~/.bashrc
   export PATH="/usr/local/cuda-10.1/bin:$PATH"
   export LD_LIBRARY_PATH="/usr/lcoal/cuda-10.1/lib64:$LD_LIBRARY_PATH"
   source ~/.bashrc

6. check CUDA
   cd /usr/local/cuda-10.1/samples/1_Utilities/deviceQuery
   sudo make
   ./deviceQuery

   OK if Result = PASS
   or nvcc -V
   
6. cuDNN
   go to [[https://developer.nvidia.com/rdp/cudnn-download]]
   at least select 7.6.1 for CUDA 10.1
   sudo cp cuda/include/cudnn.h /usr/local/cuda/include/
   sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/
   sudo chmod a+r /usr/local/cuda/include/cudnn.h 

7. sudo apt remove nvidia-*
   reboot

8. download the nvidia driver source file  at
   [[https://www.nvidia.com/Download/index.aspx?lang=en-us]]

9. cd ~/Downloads
   chmod +x ~/Downloads/NVIDIA-Linux-x86_64-450.80.02.run
   sudo ./NVI...

10. sudo apt install python3-pip
    sudo apt install ipython3
    pip3 install pip
    pip3 install tensorflow-gpu
    ipython3
    import tensorflow as tf
    print(tf.test.is_gpu_available())

*** fixup driver 418-server
sometime can this be, the communication with nvidia driver is failed,
to fix it, apt the driver 418-driver from apt again
*** check

#+BEGIN_SRC ipython :results output
  import tensorflow as tf
  if (tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)==True):
      print("using gpu")
#+END_SRC

#+RESULTS:
: WARNING:tensorflow:From <ipython-input-3-77a0022b7707>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
: Instructions for updating:
: Use `tf.config.list_physical_devices('GPU')` instead.
: using gpu



#+begin_src ipython  :results output
  import sys
  print(sys.version)
#+end_src

#+RESULTS:
: 2.7.17 (default, Sep 30 2020, 13:38:04) 
: [GCC 7.5.0]







** Tensorflow foundation
*** 数值类型 
|--------+------+---------+-----------------------+----------------------|
| type   | diam | shape   | example               | function             |
| Scalar |    0 | []      | []                    | acc                  |
| Vector |    1 | [n,]    | [1.0]                 | bias (b)             |
| Matrix |    2 | [n, m]  | [[1,2],[3,4]]         | weight (W)           |
| Tensor |   >=3 | [n,m,p] | [[[1],[1]],[[2],[2]]] | input ( [n, h, w, 3] |
|--------+------+---------+-----------------------+----------------------|

#+begin_src python

a = tf.constant(example)
tf.constant()  # 功能类似于tf.convert_to_tensor()
b = tf.constant('Hello Deep learning')
c = tf.constant(True)
dtype=tf.int16, int32, int64,tf.float16, tf.float32, tf.float64
tf.cast(a, tf.float64)
tf.Variable(a)    #可以添加a的可训练属性
tf.zeros([n,m])  tf.zeros_like(a)  == tf.zeros(a.shape)
tf.ones([n,m])   tf.ones_like(a)   == tf.ones(a.shape)
tf.fill([2,2],10)
tf.random.normal([2,2]) == tf.random.normal([2,2], mean=0, stddev=1)
tf.random.uniform(shape, minval=0,maxval=10,dtype=tf.float32)
tf.range(10) == tf.range(0,10, delta = 2)

#+end_src

#+RESULTS:

*** Reference(,) and Segment(:)
#+begin_src python
x = tf.random.normal([4,32,32,3])
x[2][1][0][1]  == x[2,1,0,1]

[start:end:step] for each dimension.
x[1:3:2, 1:4:2, 2:4:2, 1:3:2]

#+end_src

*** 改变视图
x = tf.random.normal([4,32,32,3]) 的数据是整体贮存的，可以合法的
reshape. 从后往前合并，拆分。

**** 增加维度
增加前置维度
tf.expand_dims(x, axis=0)
增加后置维度
tf.expand_dims(x, axis=-1)
也可以是其他值，表示在相应的位置增加一个维度

**** 删除维度
删除前置
tf.squeeze(x, axis=0)
后置和其他位置一样，删除dia=1的维度

*** 交换维度
这会改变数据的贮存顺序
x = tf.random.normal([2,32,32,3])
x = tf.transpose(x, perm=[0,3,1,2])
以前的维度下表变换为perm
 
*** Broadcasting
*** tile
x = tf.random.normal([4,32,32,3])
y = tf.tile(x,[2,3,3,1]) 对应维度各复制成原来的2，3，3，1倍。

*** Concatenate & Stack & Split & unstack
tf.concat([a,b],axis=0) 除了axis=0外的所有维度都应该一样
tf.stack([a,b],axis=0) a,b的所有维度应该都一样，插入的位置和
expand_dims() 遵守相同规则
tf.split(x, num_or_size_splits=10, axis=0) 拆分的维度不消失
tf.unstack(x, axis=0) 拆为步长为1           拆分的维度消失

*** Statistik

L1 Norm  $||x_{1}|| = \sum_{i}|x_{i}|$
tf.norm(x, ord=1)
L2 Norm  $||x_{2}|| = \sqrt{\sum_{i}|x_{i}|^2 }$
tf.norm(x,ord=2)

tf.reduce_max(x, axis=0)
tf.reduce_min()
tf.reduce_mean()
tf.reduce_sum()
不指明axis则是对全局求解
tf.argmax(x, axis) 
tf.argmin(x,axis) axis 轴的极值坐标

#+begin_src python

out = tf.random.normal([100,10])
out = tf.nn.softmax(out, axis=1)
pred = tf.argmax(out, axis=1)
y = tf.random.uniform([100],dtype=tf.int64,maxval=10)
out = tf.equal(pred, y)
out = tf.cast(out, dtype=tf.float32)
cor = tf.reduce_sum(out)
#+end_src

*** Padding
#+begin_src 
x = tf.pad(x,[[0,2]]) []内是padding 填充的方案，每个[]表示一个维度
#+end_src


*** advance manipulation
*** tf.minimum
tf.minimum(x, a) 最小a
tf.maximum(x, b) 最大b
tf.minimum(tf.maximum(x,2),7) == tf.clip_by_value(x,2,7)

*** tf.gather
tf.gather(x, [0,2,4,5,7],axis =1) 抽取在axis=1上的[0,2,4,5,7]坐标的组
成数据，并可以重新定义组成数据的顺序
*** tf.gather_nd
tf.gather_nd(x,[[1,1],[2,2],[3,3]]) 
后面根的[]说明了所有成员要操作的维度，
第一成员的第一维坐标为1，第二维坐标为1..
所有成员组成List
*** tf.boolean_mask
tf.boolean_mask(x, mask=[True, False, True, False],axis =0)
在axis=0的轴上，只有mask成员是True才会被选中，mask 长度等于axis=0 轴的
长度。
*** tf.where
#+begin_src python
a = tf.ones([3,3])
b = tf.zeros([3,3])
cond = tf.constant([True,False,False],[False, True, True],[False, False, False])
c = tf.where(cond, a,b)
#+end_src
tf.where(cond) 返回所有值为True元素的下标
#+begin_src  python
x = tf.random.normal([3.3])
mask = x>0
ind = tf.where(mask)
a = tf.gather_nd(x,ind)
#+end_src
*** tf.scatter_nd
在一个长度为8的空白向量（全为0）里，将updates按照indices的位置写入
#+begin_src python
indices = tf.constant([[4],[3],[2],[1]])
updates = tf.constant([3,1,0,2])
tf.scatter_nd(indices, updates, [8])

#+end_src

*** tf.meshgrid
#+begin_src python
x = tf.linspace(-8., 8, 100)   #-8后面的. 不能省略
y = tf.linspace(-8., 8, 100)
x.shape = 100
x, y = tf.meshgrid(x,y)
x.shape = [100,100]
ax.contour3D(x.numpy(), y.numpy(), z.numpy(), 50)

#+end_src

** chapter 01 code

一个很简单的例子，用tf来求某个函数的导数
#+BEGIN_SRC python :results output
import tensorflow as tf
a = tf.constant(1.)
b = tf.constant(2.)
c = tf.constant(3.)
w = tf.constant(4.)

with tf.GradientTape() as tape:
    tape.watch([w])
    y = a*w**2 + b*w + c
[dy_dw] = tape.gradient(y, [w])
print(dy_dw)

#+END_SRC


检测cpu和gpu运行时的时间对比
#+begin_src python :results output 
  import tensorflow as tf
  import timeit

  n = 10000000
  with tf.device('/cpu:0'):
      cpu_a = tf.random.normal([1, n])
      cpu_b = tf.random.normal([n, 1])

  with tf.device('/gpu:0'):
      gpu_a = tf.random.normal([1, n])
      gpu_b = tf.random.normal([n, 1])

  def cpu_run():
      with tf.device('/cpu:0'):
          c = tf.matmul(cpu_a, cpu_b)
      return c

  def gpu_run():
      with tf.device('/gpu:0'):
          c = tf.matmul(gpu_a, cpu_b)
      return c

  cpu_time = timeit.timeit(cpu_run, number=10)
  gpu_time = timeit.timeit(cpu_run, number=10)
  print('run time: ', cpu_time, gpu_time)



#+end_src

#+RESULTS:
: run time:  0.0974298170003749 0.09593267899981583

不用tensorflow的API，使用纯函数来实现神经网络训练的例子
#+begin_src python :results output
import numpy as np
data = []
for i in range(100):
    x = np.random.uniform(-10., 10)
    y = 1.477*x + 0.089 + np.random.normal(0., 0.01)
    data.append([x, y])
data = np.array(data)


def mse(b, w, points):
    totalError = 0
    for i in range(0, len(points)):
        x = points[i, 0]
        y = points[i, 1]
        totalError += (y-(w*x+b))**2
    return totalError/float(len(points))


def step_gradient(b_current, w_current, points, lr):
    b_gradient = 0
    w_gradient = 0
    M = float(len(points))
    for i in range(0, len(points)):
        x = points[i, 0]
        y = points[i, 1]
        b_gradient += (2/M)*((w_current*x+b_current)-y)
        w_gradient += (2/M)*x*((w_current*x + b_current)-y)
    new_b = b_current - (lr*b_gradient)
    new_w = w_current - (lr*w_gradient)
    return [new_b, new_w]


def gradient_descent(points, staring_b, staring_w, lr, num_iterations):
    b = staring_b
    w = staring_w
    for step in range(num_iterations):
        b, w = step_gradient(b, w, np.array(points), lr)
        loss = mse(b, w, points)
        if step % 5000  == 0:
            print(f"iterations:{step}, loss :{loss}, w:{w}, b:{b}")
    return [b, w]


def main():
    lr = 0.001
    initial_b = 0
    initial_w = 0
    num_iterations = 100000
    [b, w] = gradient_descent(data, initial_b, initial_w, lr, num_iterations)
    loss = mse(b, w, data)
    print(f"Final loss :{loss}, w:{w}, b:{b}")


# if __name__ ==' __main__':
main()


#+end_src

#+RESULTS:
#+begin_example
iterations:0, loss :70.10079414302263, w:0.11067024890672035, b:0.003952550528332437
iterations:5000, loss :0.00010199717063390048, w:1.4769704836166537, b:0.08946956663994972
iterations:10000, loss :0.00010199716394347099, w:1.4769703908814773, b:0.08947221136809669
iterations:15000, loss :0.00010199716394347127, w:1.4769703908749494, b:0.08947221155430594
iterations:20000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:25000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:30000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:35000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:40000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:45000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:50000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:55000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:60000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:65000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:70000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:75000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:80000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:85000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:90000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
iterations:95000, loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
Final loss :0.0001019971639434711, w:1.476970390874949, b:0.08947221155431519
#+end_example

** MNIST dataset

#+begin_src python :results output :session MNIST_image
  import tensorflow as tf
  from tensorflow import keras
  from tensorflow.keras import layers, optimizers, datasets
  w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))
  b1 = tf.Variable(tf.zeros([256]))
  w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))
  b2 = tf.Variable(tf.zeros([128]))
  w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))
  b3 = tf.Variable(tf.zeros([10]))
  (x,y),(x_val, y_val)=datasets.mnist.load_data()
  print('x:',x.shape, 'y:', y.shape,'x test:', x_val.shape, 'y test:', y_val)
  def preprocess(x, y):
      x = tf.cast(x, dtype = tf.float32)/255.
      x = tf.reshape(x, [-1,28*28])
      y = tf.cast(y,dtype=tf.int32)
      y = tf.one_hot(y, depth=10)
      return x,y
  train_db = tf.data.Dataset.from_tensor_slices((x,y))  #构建Dataset 对象
  train_db = train_db.shuffle(10000)                    # 打散样本顺序
  train_db = train_db.batch(128)                        #批训练
  train_db = train_db.map(preprocess)
  test_db = tf.data.Dataset.from_tensor_slices((x_val, y_val))
  test_db = test_db.shuffle(1000)
  test_db = test_db.batch(128)
  test_db = test_db.map(preprocess)
  lr = 0.001
  for epoch in range(8):
      for step, (x,y) in enumerate(train_db):
          with tf.GradientTape() as tape:
              h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])
              h1 = tf.nn.relu(h1)
              h2 = h1@w2 + b2
              h2 = tf.nn.relu(h2)
              out = h2@w3 + b3
              loss = tf.square(y - out)
              loss = tf.reduce_mean(loss)
              grads = tape.gradient(loss, [w1,b1,w2,b2,w3,b3])
              w1.assign_sub(lr *grads[0])
              b1.assign_sub(lr * grads[1])
              w2.assign_sub(lr *grads[2])
              b2.assign_sub(lr * grads[3])
              w3.assign_sub(lr *grads[4])
              b3.assign_sub(lr * grads[5])
  for x, y in test_db:
      h1 = x@w1 + b1
      h1 = tf.nn.relu(h1)
      h2 = h1@w2 + b2
      h2 = tf.nn.relu(h2)
      out = h2@w3 + b3
      pred = tf.argmax(out,axis=1)
      y = tf.argmax(y, axis=1)
      correct = tf.equal(pred, y)
      total_correct +=tf.reduce_sum(tf.cast(correct,dty=tf.int32)).numpy()



#+end_src

** Make_moons

all import 
#+begin_src python  :session make_moons
  import seaborn as sns
  import matplotlib.pyplot as plt
  #+end_src
#+RESULTS:


generate the data
#+begin_src python  :results output :session make_moons  
  from sklearn.datasets import make_moons
  from sklearn.model_selection import train_test_split
  N_samples = 2000
  Test_size = 0.3
  X, y = make_moons(n_samples = N_samples, noise = 0.2, random_state=100)
  X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = Test_size, random_state = 42)
  print(X.shape, y.shape)
  def make_plot(X, y, plot_name, file_name=None, XX=None, YY=None, preds=None,dark=False):
      if(dark):
          plt.style.use('dark_background')
      else:
          sns.set_style("whitegrid")
      plt.figure(figsize=(16,12))
      axes = plt.gca()
      axes.set(xlabel="$x_1$", ylabel="$x_2$")
      plt.title(plot_name,fontsize=30)
      plt.subplots_adjust(left=0.20)
      plt.subplots_adjust(right=0.8)
      if(XX is not None and YY is not None and preds is not None):
          plt.contourf(XX,YY,preds.reshape(XX.shape), 25, alpha=1,cmap = cm.Spectral)
          plt.contour(XX,YY, preds.reshape(XX.shape), levels=[.5],cmap="Greys", vmin=0,vmax=0.6)
          plt.scatter(X[:,0],X[:,1],c=y.ravel(), s=40, cmap=plt.cm.Spectral,edgecolors='none')
          plt.savefig('data.svg')
          plt.close()
  make_plot(X,y,None,"Classification Dataset Visualization")
#+end_src
#+RESULTS:
: (2000, 2) (2000,)


generate the  signal Layer class
#+begin_src python :session make_moons
  class Layer:
      def __init__(self, n_input, n_neurons,activation=None, weight=None,bias=None):
          self.weight = weight if weight is not None else np.random.randn(n_input,n_neurons)*np.sqrt(1/n_neurons)
          self.bias = bias if bias is not None else np.random.rand(n_neurons)*0.1
          self.activation = activation
          self.last_activation = None
          self.error = None
          self.delta = None
      def activate(self,x):
          r = np.dot(x, self.weight)+self.bias
          self.last_activation = self._apply_activation(r)
          return self.last_activation
      def _apply_activation(self, r):
          if self.activation is None:
              return r
          elif self.activation == 'relu':
              return np.maximum(r,0)
          elif self.activation == 'tanh':
              return np.tanh(r)
          elif self.activation == 'sigmoid':
              return 1/(1+np.exp(-r))
          return r
      def apply_activation_derivative(self, r):
          if self.activation is None:
              return np.ones_like(r)
          elif self.activation == 'relu':
              grad = np.array(r, copy=True)
              grad[r>0] = 1.
              grad[r<=0] =0.
              return grad
          elif self.activation == 'tanh':
              return 1-r**2
          elif self.activation == 'sigmoid':
              return r*(1-r)
          return r
            
#+end_src
#+RESULTS:
| 2000 | 2 |

generate the multi Layers Class NeuralNetwork
#+begin_src python :session make_moons
  class NeuralNetwork:
      def __init__(self):
          self._layers = []
      def add_layer(self, layer):
          self._layers.append(layer)
      def feed_forward(self, X):
          for layer in self._layers:
              X = layer.activate(X)
          return X

      def backpropagation(self, X, y,learning_rate):
          output = self.feed_forward(X)
          for i in reversed(range(len(self._layers))):
              layer = self._layers[i]
              if layer == self._layers[-1]:
                  layer.error = y-output
                  layer.delta = layer.error*layer.apply_activation_derivative(output)
              else:
                  next_layer = self._layers[i+1]
                  layer.error = np.dot(next_layer.weights, next_layer.delta)
                  layer.delta = layer.error*layer.apply_activation_derivative(layer.last_activation)

          for i in range(len(self._layers)):
              layer = self._layers[i]
              o_i = np.atleast_2d(X if i == 0 else  self._layers[i-1].last_activation)
              layer.weights += layer.delta*o_i.T*learning_rate


      def train(self, X_train, X_test, y_train, y_test, learning_rate, max_epochs):
          y_onehot = np.zeros((y_train.shape[0],2))
          y_onehot[np.arange(y_train.shape[0]),y_train] =1
          mses = []
          for i in range(max_epochs):
              for j in range(len(X_train)):
                  self.backpropagation(X_train[j], y_onehot[j], learning_rate)
              if i%10 == 0:
                  mse = np.mean(np.square(y_onehot - self.feed_forward(X_train)))
                  mses.apply(mse)
                  print('Epoch : #%s, MSE: %f' %(i, float(mse)))
                  print('Accuracy: %.2f%%' %(self.accuracy(self.predict(X_test),y_test.flatten())*100))
          return mses


#+end_src
#+RESULTS:

#+begin_src  python :session make_moons
nn = Neuralnetwork()
nn.add_layer(Layer (2, 25, 'sigmoid'))
nn.add_layer(Layer(25, 50, 'sigmoid'))
nn.add_layer(Layer(50, 25, 'sigmoid'))
nn.add_layer(Layer(25, 2, 'sigmoid'))
nn.backpropagation(X_train,y_train,0.001)
nn.train(X_train, X_test, y_train, y_test, 0.001,20)


#+end_src
#+RESULTS:

different Layers
#+begin_src python :session make_moons
  for n in range(5):
      model = Sequential()
      model.add(Dense(8,input_dim=2,activation='relu'))
      for _ in range(n):
          model.add(Dense(32,activation='relu'))
      model.add(Dense(1,activation='sigmoid'))
      model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
      history = model.fit(X_train,y_train,epochs = 20, verbose=1)
      preds = model.predict_classes(np.c_[XX.ravel(), YY_ravel()])
      title = "网络层数({})".format(n)
      file = "网络容量 %f.png" %(2+n*1)
      make_plot(X_train,y_train, title,file,XX,YY,preds)
    

#+end_src

#+RESULTS:

** Keras
*** tf.keras 

#+begin_src python :results output
  import tensorflow as tf
  from tensorflow import keras
  from tensorflow.keras import layers

  x = tf.constant([2., 1., 0.1])
  print(tf.keras.layers.Softmax(axis=-1)(x))
  print(tf.nn.softmax(x))

#+end_src

#+RESULTS:
: tf.Tensor([0.6590012  0.24243298 0.09856589], shape=(3,), dtype=float32)
: tf.Tensor([0.6590012  0.24243298 0.09856589], shape=(3,), dtype=float32)

tf.keras.Model 是tf.keras.Sequential的父类(网络类）
Sequential类还有方法：
Sequential.compile()
Sequential.fit()
Sequential.predict()

tf.keras.layers.Layer 是网络层类的父类（网络层类）

*** 模型的保存和加载
**** 保存数值
Sequential.save_weights('weights.ckpt')
重建了一样的网络后，重新加载
Sequential.load_weights('weights.ckpt')
**** 保存框架
tf.keras.Sequential.save('model.h5')
不需要重构模型，可以直接生成保存的模型
network = tf.keras.models.load_model('model.h5')
**** 跨系统平台保存恢复
tf.saved_model.save(network, 'model-savedmodel')
复制，分发该文件后，在跨平台上复现
network = tf.saved_model.load('model-savedmodel')
*** self-def 
自定义网络层类，继承Layer
自定义网络类，继承Model

** Dropout
tf.nn.dropout(x, rate = 0.5)
model.add(layers.Dropout(rate=0.5))
** Data Augmentation
*** resize
tf.image.resize(x,[244,244])
*** rote
tf.image.rot90(x,1) k为1时，代表一个90度的g逆时针旋转
*** flip
tf.image.random_flip_left_right(x)
tf.image.random_flip_up_down(x)
*** crop
先放大，再剪裁
tf.image.resize(x,[244,244])
tf.image.random_crop(x,[224,224,3])

* IDE
** emacs
** Pycharm

|---------+-----------------------------------|
| M-1     | open projects                     |
| C-M-c   | collapse 折叠代码                 |
| C-M-e   | expand  打开代码                  |
| C-M-s   | open Settings                     |
| C-S-+   | increasing fonts                  |
| C-S+-   | decreasing fonts                  |
| C-S-0   | reset fonts                       |
| S-Esc   | close                             |
| S-tab   | open in Emacs                     |
| S-Enter | execute/run                       |
| C-S-i   | go to define file                 |
| C-M-l   | show line                         |
| C-F12   | close main bar                    |
| C-M-x   | exit                              |
| C-A-[]  | change in between opened projects |
| C-ins   | in project add a new file         |
| C-c C-p | execute python file               |
| C-c C-o | close (python) tab                |
| f11     | Full screen                       |
| A-.     | open the relate source file       |
|---------+-----------------------------------|

** jupyter
#+begin_src sh
sudo apt install jupyter
sudo apt install jupyter lab
pip install jupyterthemes
jt -t monokai -f fira -fs 13 -nf ptsans -nfs 11 -N -kl -cursw 5 -cursc r -cellw 95% -T

#+end_src
Helf -> edit shortcut
|------------+------------------------------|
| j          | next cell                    |
| k          | previous cell                |
| f          | pull the cell to the top     |
| i          | intercept cell               |
| c          | clear results                |
| space      | next site                    |
| ctrl space | previous site                |
| ctrl enter | execute and to the next cell |
| C-M-m      | close mainbar                |
| C-M-n      | close toolbar                |
|------------+------------------------------|

* Machine Learning
** Math for ML
*** Matrix
**** Conjugate transpose Matrix
$$ A^{*} = (\bar{A})^{T} = \bar{A^{T}}$$
共轭转置矩阵, 先共轭再转置，还是先转置再共轭都可以。
**** Normal Matrix
$$ A^{*}A = A A^{*}$$
正定矩阵, 是转置和本身满足交换律
$$ A = URDU^{-1} $$
可酉变换
**** Unitary Matrix
$$ A^{*}A = A A^{*} = I $$
 酉矩阵，是正定矩阵, 即转置和本身满足交换律，而且等于 I 

**** orthogonal matrix
实数酉矩阵

$$
{\displaystyle Q^{T}=Q^{-1}\Leftrightarrow Q^{T}Q=QQ^{T}=I}
$$
得该矩阵的转置矩阵为其逆矩阵

**** Hermitian matrix
$$ A = A^{H}$$, $a_{i,j} = \bar{a_{j,i}}$
共轭对称
埃尔米特矩阵，厄米特矩阵，厄米矩阵,所有元素对称出共轭
*** Eigenwertzerlegung eigende decompostion

对于矩阵求特征值特征向量，特征值分解，但是要求必须是方阵，如果不是，先
要转换： A = a*a.T


$$ A=UBU^T $$ 

#+begin_src python :results output
  import numpy as np
  a = np.mat([[1,2,3,4],[1,1,1,1]])
  A = a*a.T
  B, U = np.linalg.eig(A)
  print("eigenvalue of A : ")
  print(B)
  print("eigenvalue of a :(should be equal to the following) ")
  print(np.sqrt(B))
  print("eigenvactor : ")
  print(U)

#+end_src

#+RESULTS:
: eigenvalue of A : 
: [33.40121947  0.59878053]
: eigenvalue of a : 
: [5.77937881 0.77380911]
: eigenvactor : 
: [[ 0.94673755 -0.32200622]
:  [ 0.32200622  0.94673755]]

*** SVD Singular value decompostion

但是对于一般矩阵，不是方阵，可以奇异值分解：
$$ a = UBV^T , a^{t} = VBU^{T} $$
$$ A = aa^{T} = UB^{2}U^{T}$$
$$ A^{'}=a^{T}a=VB^{2}V^{T}$$

#+begin_src python :results output
  import numpy as np
  a = np.mat([[1,2,3,4],[1,1,1,1]])
  U, B, Vt = np.linalg.svd(a)
  print("left eigenvalue : ")
  print(U)
  print("eigenvactor of a : ")
  print(B)
  print("right eigenvalue : ")
  print(Vt)
#+end_src

#+RESULTS:
#+begin_example
left eigenvalue : 
[[-0.94673755 -0.32200622]
 [-0.32200622  0.94673755]]
eigenvactor of a : 
[5.77937881 0.77380911]
right eigenvalue : 
[[-0.21952944 -0.38334247 -0.5471555  -0.71096853]
 [ 0.80734554  0.3912142  -0.02491713 -0.44104847]
 [-0.40008743  0.25463292  0.69099646 -0.54554195]
 [-0.37407225  0.79697056 -0.47172438  0.04882607]]
#+end_example



*** Bayes's Rule
if x and y are independent:
$$ p(x,y) =  p(y)p(x) = p(x) p(y) = p(y,x) $$ 

#+begin_src 
条件概率：
条件概率 = 联合概率/边缘概率
先验概率和后验概率都是条件概率，但是条件已知是先验
#+end_src
$$ P(y|x) = \frac{ P(x,y)}{P(x)}$$
#+begin_src 
全概率公式
#+end_src
$$ p(y) = \sum_{i=1}^{n} p(y,x_{i}) $$
#+begin_src 
贝叶斯公式
#+end_src

$$ P(AB)=P(BA) $$
$$ P(y|x)P(x) = P(x|y)P(y)$$
$$ P(y|x) = \frac{ P(x|y) P(y)}{P(x)}$$

#+begin_src 
贝叶斯公式 + 全概率公式 + 条件概率 
#+end_src

\begin{eqnarray*}
P(A|B) &= \frac{P(B|A)P(A)}{P(B)} \\
       &= \frac{P(B|A)P(A)}{\sum_{i=1}^{n} p(B,A_{i})} \\
       &= \frac{P(B|A)P(A)}{\sum_{i=1}^{n} P(B|A_{i})P(A)} \\
\end{eqnarray*}

*** Kovarianz Matrix

for i = {1...n}, $x_{i}$ is a random variable, which belong to
Gaussian distribution

set 
 $$ X = \left( \begin{aligned}  x_{1} \\ x_{2}\\ . \\. \\x_{n}  \end{aligned}\right) $$

$$ \bar{X} = \left( \begin{aligned}  \bar{x}_{1}
\\ \bar{x}_{2}\\ . \\. \\ \bar{x}_{n}  \end{aligned} \right) $$

co-variance matrix $\Sigma = E [(X-\bar{X})(X-\bar{X})^{T} ]$

\begin{equation}
\Sigma = 
  \left(
  \begin{array}{c}
          x_{1}-\bar{x}_{1} \\
          x_{2}-\bar{x}_{2} \\
          x_{3}-\bar{x}_{3} \\
          ..                \\
          x_{n}-\bar{x}_{n} 
 \end{array}
 \right)
  \left(
  \begin{array}{ccccc}
          x_{1}-\bar{x}_{1} &
          x_{2}-\bar{x}_{2} &
          x_{3}-\bar{x}_{3} &
          ..                &
          x_{n}-\bar{x}_{n} 
  \end{array}
  \right)
\end{equation}
对角线上是对应元素的方差，其他是相对于两个元素的协方差

*** Regularization
$$ Loss = \frac{1}{2}\sum^{N}_{i=1}(y_{i}-w^{t}\phi(x_{i}))^{2}+\frac{\lambda}{2}\sum^{M}_{j=1}|w_{j}|^{q} $$

|--------------------------------------------+---------------------------------------------------------|
| N                                          | example number                                          |
| M                                          | Eigenschaften Number, Diemension number                 |
| L0                                         | 控制网络中的非零权重                                    |
| L1                                         | 网络中的所有元素的绝对值之和,促使网络生成更多的稀疏矩阵 |
| L2                                         | 网络中的所有元素平方和,促使网络生成小比重的权值         |
| w                                          | w_{1}, w_{2}                                            |
| q=1                                        | l1 regularization                                       |
| q=2                                        | l2 regularization                                       |
| \lambda                                    | learning rate                                           |
| \sum^{N}_{i=1}(y_{i}-w^{t}\phi(x_{i}))^{2} | 依据w的同心圆                                           |
| \sum^{M}_{j=1}w_{j}^{q}                    | q=1, 菱形， q=2, 圆形                                   |
|                                            |                                                         |
|                                            |                                                         |

Loss 要最小，part1 刚好和 part2 相接，l1会在坐标轴上，所以如果有较小分
量，会被直接设为0

*** Error Bias-Variance-trade-off
#+begin_src
Error = Bias + Variance + noise
#+end_src

|----------+------+----------+--------------------------+---|
| Bias     | 偏差 | 欠拟合   | 发挥，观测等主观因素影响 |   |
| Variance | 方差 | 过过拟合 | 稳定性，模型的构建决定   |   |
| noise    | 噪音 |          | 统难度                   |   |
|----------+------+----------+--------------------------+---|

*** Multi variable Gaussian distribution
seeing the link 知乎  

$$
{\displaystyle f_{\mathbf {X} }(x_{1},\ldots ,x_{k})={\frac {\exp
\left(-{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm
{T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu
}})\right)}{\sqrt {(2\pi )^{k}|{\boldsymbol {\Sigma
}}|}}}}
$$

#+begin_src python

def gaussian(x,mean,cov):
    dim = np.shape(cov)[0] #维度
    #之所以加入单位矩阵是为了防止行列式为0的情况
    covdet = np.linalg.det(cov+np.eye(dim)*0.01) #协方差矩阵的行列式
    covinv = np.linalg.inv(cov+np.eye(dim)*0.01) #协方差矩阵的逆
    xdiff = x - mean
    #概率密度
    prob = 1.0/np.power(2*np.pi,1.0*dim/2)/np.sqrt(np.abs(covdet))*np.exp(-1.0/2*np.dot(np.dot(xdiff,covinv),xdiff))
    return prob

#+end_src

*** Mahalanobis distance 

$$ \Delta = \left(-{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm
{T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu
}})\right)
$$
$$ \Sigma = \sum U \Lambda U^{T} $$
$$ \Sigma^{-1} = \sum U \Lambda^{-1} U^{T} $$

$$ \Delta = -{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm
{T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu
}}) = -{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm
{T} }  U \Lambda^{-1} U^{T}({\mathbf {x} }-{\boldsymbol {\mu
}})
$$
马氏距离所使用的变换 : $$ Z = U^{T}(X - \mu) $$,


$$ D = \sqrt{ZZ^{T}} $$
关于新的坐标，U 是变换的旋转，$\Lambda$ 是基底的延伸，$(x-\mu)$ 是在其
上的投影，此后，在新坐标上，即为多变量，标准，不相关高斯分布

*** K-fold Cross Validation
|---------+---------------------------------------------|
| N       | total examples                              |
| K       | number of sub-fold                          |
| m       | number of each sub-fold                     |
| big K   | small bias, with over fitting, big variance |
| small K | big bias, without fitting, low variance     |
|---------+---------------------------------------------|

*** confusion matrix prediciton & accuary
|------------------+---------------+----------------|
|                  | Actually Ture | Actually False |
| predict Positive | TP            | FP             |
| predict Negative | FN            | TN             |

**** Sensitivity
namely, Ture Positive Rate, y axes of ROC

$$ Sensitivity = \frac{TP}{TP+FN} $$
**** 1 -  Sensitivity
namely, false Position, x axes of ROC

$$ 1 -  Sensitivity = \frac{FP}{FP + TN} $$

**** ROC Receivert Operator Characteristic
under the acceptable  x (1 -  Sensitivity) , we want the best  y (Sensitivity).
from side to side is all classifed to Positive to all classifed to negative

**** AUC Area under the Curve
je mehr Fachsgebiet, desto besser for the Method,
we use this target to choice our Method

**** precision

|-----+---------------+-------------------------------------------------+-------------------------------------------------|
| TPR | recall 查全 R | predict positive 测正 &  实正 actually positive | predict negative 测反 & 实正  actually positive |
| FPR |               | predict positive 测正 &  实反 actually negative | predict negative  测反 & 实反 actually negative |
|-----+---------------+-------------------------------------------------+-------------------------------------------------|
|     |               | precision 查准 P                                |                                                 |

R-P Couve : R P
ROC couve : FPR TPR

$$ F_{1} = \frac{1}{2} \cdot (\frac{1}{P}+\frac{1}{R})$$
$$ F_{\beta} = \frac{1}{\beta^{2}} \cdot
(\frac{\beta^{2}}{P}+\frac{1}{R})$$

关联性属性 ： 高中低 （3,2,1）
非关联性属性： 猪狗羊 （(1,0,0), (0,1,0),(0,0,1)）

*** Jacobin matrix 


for $$ Y_{m} = f(X_{n}), Y =(y_{1}, y_{2}, y_{3}....y_{m}), X = (x_{1}
,x_{2}....x_{n}) $$
$$ d_{Y} = J d_{x}$$,
$$
{\displaystyle \mathbf {J} ={\begin{bmatrix}{\dfrac {\partial \mathbf
{f} }{\partial x_{1}}}&\cdots &{\dfrac {\partial \mathbf {f}
}{\partial x_{n}}}\end{bmatrix}}={\begin{bmatrix}{\dfrac {\partial
f_{1}}{\partial x_{1}}}&\cdots &{\dfrac {\partial f_{1}}{\partial
x_{n}}}\\\vdots &\ddots &\vdots \\{\dfrac {\partial f_{m}}{\partial
x_{1}}}&\cdots &{\dfrac {\partial f_{m}}{\partial
x_{n}}}\end{bmatrix}}} 
$$
由球坐标系到直角坐标系的转化由 F: ℝ+ × [0, π] × [0, 2π) → ℝ3 函数给出，
其分量为：
$$
{\displaystyle {\begin{aligned}x&=r\sin \theta \cos \varphi
;\\y&=r\sin \theta \sin \varphi ;\\z&=r\cos \theta
.\end{aligned}}}
$$
此坐标变换的雅可比矩阵是
$$
{\displaystyle \mathbf {J} _{\mathbf {F} }(r,\theta ,\varphi
)={\begin{bmatrix}{\dfrac {\partial x}{\partial r}}&{\dfrac {\partial
x}{\partial \theta }}&{\dfrac {\partial x}{\partial \varphi
}}\\[1em]{\dfrac {\partial y}{\partial r}}&{\dfrac {\partial
y}{\partial \theta }}&{\dfrac {\partial y}{\partial \varphi
}}\\[1em]{\dfrac {\partial z}{\partial r}}&{\dfrac {\partial
z}{\partial \theta }}&{\dfrac {\partial z}{\partial \varphi
}}\end{bmatrix}}={\begin{bmatrix}\sin \theta \cos \varphi &r\cos
\theta \cos \varphi &-r\sin \theta \sin \varphi \\\sin \theta \sin
\varphi &r\cos \theta \sin \varphi &r\sin \theta \cos \varphi \\\cos
\theta &-r\sin \theta &0\end{bmatrix}}.}
$$
其雅可比行列式为 r2 sin θ，由于 dV = dx dy dz，如果做变数变换的话其体
积元(Volume element)，dV，会变成：dV = r2 sin θ dr dθ dφ。

*** Gini
吉尼系数
$$ Gini(D) = \sum^{|y|}_{k=1}\sum_{k^{'}\not = k} p_{k}p_{k^{'}} = 1- \sum^{|y|}_{k=1}p_{k}^{2}$$

** activation function
*** no activation
输出为实数空间或某个区间， 连续变化。直接有输出值和真实值比较
*** Sigmoid
$$ Sigmoid(x) = \frac{1}{1+e^{-x}}$$

导数：$$ \sigma'(x) = \sigma(x)(1-\sigma(x))$$

*** ReLU
$$ Relu(x) = 
\begin{cases}
x&  x >=0 \\
0&  x < 0
\end{cases}$$

$$ Relu(x) = max(0,x)$$

*** LeakyReLU

$$ LeakyReLU(x) = \begin{cases}
x& x >=0 \\
px& x <0, 0<p<<1
\end{cases}$$

*** Tanh
$$ tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$
$$ tanh(x) = 2.sigmoid(2x)-1$$
导数：$$\tanh'(x) = 1- \tanh^{2}(x)$$

*** MSE
$$ \mathcal{L} = MSE(y, o) =
\frac{1}{d_{out}}\sum_{i=1}^{d_{out}}(y_{i}-o^{i})^{2}$$
导数 ：$$ \frac{\partial \mathcal{L}}{\partial o_{i}}= (o_{i}-y_{i})$$

*** inf entropy 
$$ H(p) = -\sum_{i}P(i)\log_{2}^{P_{i}}$$
*** softmax
$$ p_{z_{i}} = \frac{e^{z_{i}}}{\sum_{j}e^{z_{j}}}$$
所有种类的概率之和为1
导数:
$$ \frac{ \partial p_{z_{i}}}{\partial z_{j}} = 
\begin{cases}
p_{i}(1-p_{j}) &  if i =j \\
-p_{i}p_{j}    & if \neq j
\end{cases}$$

*** cross entropy
在计算交叉熵时， 一般是和 softmax 函数一起使用的
$$ H(p||q) = -\sum_{i} p(i) \log_{2}^{q_{i}}$$
$$H(p||q) = H(p) + D_{KL}(p||q)$$

for One-hot coding
$$ H(p||q) = D_{KL}(p||q) = \sum_{i}y_{i}log(\frac{y_{j}}{o_{j}}) = 
1 \cdot \log\frac{1}{o_{i}} + \sum_{j!=i}0 \cdot \log \frac{0}{o_{j}}
= -\log o_{i}$$
o_i 为1 时，预测正确，交叉熵为0。
导数：
$$ \mathcal{L} = -\sum_{k}y_{k}\log(p_{k})$$
$$\begin{aligned}
 \frac{\partial \mathcal{L}}{\partial z_{i}} & =
-\sum_{k} y_{k} \frac{\partial \log(p_{k})}{\partial z_{i}} \\
&= -\sum_{k} y_{k} \frac{\partial \log(p_{k})}{\partial p_{k}} \cdot
\frac{\partial p_{k}}{\partial z_{i}} \\
&= -\sum_{k} y_{k} \frac{1}{\partial p_{k}} \cdot
\frac{\partial p_{k}}{\partial z_{i}} \\
\end{aligned}
$$

用上面 softmax 的导数结果，分为k=i 和k!=i两种情况
$$ \frac{\partial \mathcal{L}}{z_{i}}=p_{i}-y_{i}$$

** Algorithms
*** Ordinary Least Squares(OLS)
**** 正规化方程
正则化方程的推导，用高斯分布的多变量分布的Maxisum likelihood,能一起求得对weight和bias值 : 

但是要添加一列1到 train 和 test，至于在前面还是后面有点怪异。

目前认为，在后面的话，多变量和参数可以按需求访问

Loss function:
$$ J = \frac{1}{2m}\sum(h_{\theta}(x_{i})-y_{i})^{2}$$
$$ \sigma = \frac{1}{2m}(X \theta -y)^{T} (X \theta -y)$$
对$\theta$ 求导，并令其为0，
$$\theta = (X^{T}X)^{-1}X^{T}y $$
但是要求$X^{T}X$ 必须可逆。

**** 正则化正规化方程
$$w = (\Phi^{T}\Phi + \lambda I)^{-1} \Phi^{T}y $$


#+begin_src python :results output
  import numpy as np
  import random
  from sklearn import linear_model
  testsize = 5

  x = np.array([a for a in range(100)])
  onesx = np.ones(x.shape)
  X = np.c_[x,onesx]
  y = np.array([a*5 + 20 + random.randint(0,3) for a in range(100)])
  print("the X shape is {}, and y shape is {}".format(X.shape, y.shape))

  # weight = np.dot(np.dot(np.linalg.pinv(np.dot(X.T, X)), X.T), y)
  weight = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

  print("OLS : the weight is{}, and the bais is {} ".format(weight[:-1], weight[-1]))
  print("the predect of 5 ele is {}".format(np.dot(np.c_[np.arange(testsize),np.ones(testsize)], weight)))


#+end_src

#+RESULTS:
: the X shape is (100, 2), and y shape is (100,)
: OLS : the weight is[5.00063606], and the bais is 21.508514851485195 
: the predect of 5 ele is [21.50851485 26.50915092 31.50978698 36.51042304 41.51105911]

#+begin_src 
也可以是对变量 with multi variables
#+end_src

#+begin_src python :results output
  import numpy as np
  import random
  from sklearn import linear_model
  testsize = 5

  x = np.array([a for a in range(100)])
  onesx = np.ones(x.shape)
  X = np.c_[onesx, x, 2*x]
  y = np.array([a*5 + 20 + random.randint(0,3) for a in range(100)])
  print("the X shape is {}, and y shape is {}".format(X.shape, y.shape))

  # ordinary  least squares (正规化方法）
  weight = np.dot(np.dot(np.linalg.pinv(np.dot(X.T, X)), X.T), y)
  print("OLS : the weight is{}, and the bais is {} ".format(weight[:-1], weight[-1]))
  print("the predect of 5 ele is {}".format(np.dot(np.c_[np.arange(testsize), np.arange(testsize),np.ones(testsize)], weight)))


#+end_src

#+RESULTS:
: the X shape is (100, 3), and y shape is (100,)
: OLS : the weight is[21.35089109  0.9999964 ], and the bais is 1.999992799280042 
: the predect of 5 ele is [ 1.9999928  24.35088029 46.70176778 69.05265527 91.40354275]

*** 朴素贝叶斯 
假设各个属性完全独立
要判断某示例的分类，分别计算出该示例在每个分类中的概论乘以该类别中该示
例的各种属性出现的条件概论， 谁大选谁。（注意样本不足引起的某属性的条
件为零）

*** Decision List

(f1,v1),(f2,v2)....(fr,vr)
fi is a term in CNF, vi belongs {0,1}, and the last term fr is always
true. and each term can be viewed as if else extended. if fi is
matched, so vi is its value.

#+BEGIN_SRC 
for 0<k<n, k-CNF and k-DNF are proper 


#+END_SRC

*** Decision tree
**** 方法描述和变量
在训练集内以最大信息增益来确定划分属性，在各个子区内再重复剩下的属性

信息熵
$$ Ent(D)=-\sum^{y}_{k=1}p_{k}log_{2}^{p_{k}}$$

信息熵增益
$$ Gain(D,a) = Ent(D)-\sum^{V}_{v=1}\frac{|D^{v}|}{|D|}Ent(D^{v}) $$

吉尼系数
$$ Gini(D) = \sum^{|y|}_{k=1}\sum_{k^{'}\not = k} p_{k}p_{k^{'}} = 1- \sum^{|y|}_{k=1}p_{k}^{2}$$

**** 预剪枝和后剪枝
在利用训练集的最大信息增益确定划分属性后，用验证集来检验划分，如果验证
集的信息熵增加，（泛化结果不好)否定此次划分，设为叶节点

后剪枝是在这个树完成后，用验证集去检验每一个内节点，从下到上，如果去掉
该划分有更小的信息熵，则废除该划分。

**** 属性有连续值和缺失值
连续值离散化：排列该属性的所有取值n个，在n-1个区间中去中间值为离散值，
遍历所有离散值，找到最大信息增益的离散值，作二分。

缺失值，取出该属性的非缺失子集，再配以相应的比率计算信息增益，处理和以
前一样。如果选出的划分属性有缺失值，则给划分不作用到缺失样本，复制到每
个划分子集
**** 多变量决策
每个划分属性是多个属性（变量）的线性组合

*** Bagging
多次放回抽样，用不同抽样的数据集在多棵树上并行计算，
|-----------+---------------+--------------------------|
| More Tree | Bias remained | Variance reduce to limit |
所以刚开始选择偏差小，方差大的强模型

*** Random Forest
Random Forest = Decision Tree + Bagging + random Eigenschaften

|------------------+-------------|
| More deeper      | Bias reduce |
| More Eigenschaft | Bias reduce |
|------------------+-------------|
*** Boosting
固定数据集，在多个串行的模型上顺序计算，模型间强相关，防止过拟合，用弱
相关模型

*** gradient decent
当数据点很多是，正则化方法计算量将非常大，此时较多使用梯度下降

#+begin_src 
sklearn API
#+end_src

#+begin_src python :results output

  import numpy as np
  import random
  from sklearn import linear_model
  testsize = 5

  x = np.array([a for a in range(100)])
  onesx = np.ones(x.shape)
  X = np.c_[x, 2*x, onesx]
  y = np.array([a*5 + 20 + random.randint(0,3) for a in range(100)])
  print("the X shape is {}, and y shape is {}".format(X.shape, y.shape))

  # Sklearn API
  reg = linear_model.LinearRegression()
  model = reg.fit(X,y)
  print("Sklearn: the weith is {}, and the intercept is {}".format(model.coef_[:-1] ,model.intercept_))
  print("the predect of 3 ele is {}".format(model.predict(np.c_[np.arange(testsize), np.arange(testsize),np.ones(testsize)])))


  # manual 
  def featureNormalize(X):
      (m,n) = X.shape
      X_norm = X
      mu = np.zeros(n);
      sigma = np.zeros(n);
      for i in range(n):
          mu[i] = np.mean(X[:,i])
          sigma[i] = np.std(X[:,i])
          X_norm[:,i] = (X_norm[:,i]-mu[i])/sigma[i]
      return X_norm
  def computeCost(X, y, theta):
      return np.sum((np.dot(X,theta) -y)**2)/(2*len(y));

  def gradientDescent(X, y, theta, alpha, num_iters):
      m = len(y)
      J_history = np.zeros(num_iters);
      theta_len = len(theta);
      for num_iter in range(num_iters):
          theta = theta - (alpha/m)*np.dot(X.T,(np.dot(X,theta).reshape(-1)-y))
          J_history[num_iter] = computeCost(X, y, theta)
      return theta, J_history

  alpha = 0.0001
  num_iters = 400000
  theta = np.zeros(2+1)
  theta, J_history = gradientDescent(X, y, theta, alpha, num_iters)
  print("Greadient decent: the weight is {}, and the intercept is {}".format(theta[:-1],theta[-1]))
  print("the predect of 3 ele is {}".format(np.dot(np.c_[np.arange(testsize), np.arange(testsize),np.ones(testsize)], theta)))
#+end_src

#+RESULTS:
: the X shape is (100, 3), and y shape is (100,)
: Sklearn: the weith is [0.99925113 1.99850225], and the intercept is 21.64534653465344
: the predect of 3 ele is [21.64534653 24.64309991 27.64085329 30.63860666 33.63636004]
: Greadient decent: the weight is [0.99925367 1.99850734], and the intercept is 21.64450170230179
: the predect of 3 ele is [21.6445017  24.64226272 27.64002374 30.63778475 33.63554577]

*** linear regression

*** Support Vector Machine
**** without soft margin
对于点的划分，由decision theory:
$$\vec{w}\vec{u} +c \ge 0$$
距此线一个单位对点标注
$$\vec{w}{x_{+}}+b \ge 1$$
then y = 
$$\vec{w}{x_{-}}+b \le 1$$
then y = -1
So,
$$y(\vec{w}x+b) -1 \ge 0$$
最大化标+点和标-点的距离：
$$D_{max} = (x_{+}-x_{1})\frac{\vec{w}}{||w||} = \frac{2}{||w||}$$
等价于最小化$\frac{1}{2}||w||^{2}$, 再加上约束条件
$$L= \frac{1}{2}||w||^{2} -\sum
\alpha_{i}[y_{i}(\vec{w}\vec{x}+b)-1]$$
设L对w和b的偏导为0，$\vec{w} = \sum \alpha_{i}x_{i}y_{i}$,$\sum
\alpha_{i}y_{i}=0$.
再代回L，$$L=\sum \alpha_{i} - \frac{1}{2} \sum \sum \alpha_{i}
\alpha_{j} y_{i} y_{j}(x_{i}x_{j})$$

**** with soft margin
对于不能绝对线性分割的，可以允许某些点进入空白分割区域（从-1到1的区域）
| slack variable                              | \epsilon | t_n y(x_n) \ge 1-\epsilon_n | \epsilon > 0      |
| Controls trade-off between slack and margin | C        | C= \infty, if misclassified    | C \sum \epsilon_n |

this L satisfied the KKT condition, and can be solved.
| good classified    | a = 0 | \epsilon = 0 | C = 0      |
| on the margin      | a < C | \epsilon = 0 |            |
| violate the margin | a = C | \epsilon > 0 |            |
| misclassified      |       | \epsilon > 1 | C = \infty |

*** Neural network
**** Backpropagation
***** 感知机
对x的向后更正，$x^{'}= x - \eta \cdot \frac{dy}{dx}$.
对于感知机的传递功能，$y = w^{T}x + b$.
由于感知机没有激活函数，所以直接对$$\mathcal{L} = \frac{1}{n}
\sum^{n}_{i=1}(w\cdot x^{i} +b -y^{i})^{2}$$.
$$ \frac{\partial \mathcal{L}}{\partial w} = \frac{2}{n}
\sum^{n}_{i=1}(wx^{i}+b-y^{i})x^{i}$$
$$ \frac{\partial \mathcal{L}}{\partial b}= \frac{2}{n}\sum^{n}_{i=1}(wx^{i}+b -y^{i})$$

***** 多层神经网络

而对于多层神经网络，$z = w^{T}x + b$, $\frac{\partial z}{\partial w} =x$,  $\frac{\partial z}{\partial b} = 1$.
每层之间具有激活函数, $\sigma(z) = \frac{1}{1-e^{-z}}$,$\frac{\partial \sigma(x)}{\partial x} = \sigma (1-\sigma)$.
损失函数, $\mathcal{L} = \frac{1}{2}(\sigma - y^{i})^{2}$, $$\frac{\partial \mathcal{L}}{\partial \sigma} = (\sigma -y^{i})$$


$$\frac{\partial \mathcal{L}}{\partial w} = \frac{\partial
\mathcal{L}}{\partial \sigma} \cdot \frac{\partial \sigma }{\partial z} \cdot
\frac{\partial z}{\partial w}$$


$\frac{\partial \mathcal{L}}{\partial w} = (\sigma -
y)\sigma(1-\sigma) \cdot x$

$$\frac{\partial \mathcal{L}}{\partial b} = \frac{\partial
\mathcal{L}}{\partial \sigma} \cdot \frac{\partial \sigma }{\partial z} \cdot
\frac{\partial z}{\partial b}$$

$\frac{\partial  \mathcal{L}}{\partial b} = (\sigma -
y)\sigma(1-\sigma)$

如果对于多层神经网络，则需要逐层计算，其中$\frac{\partial
\mathcal{L}}{\partial w}$ 中的w就是相应层的权重，由最后的
L逐步回推到w。

*** Conversational neural Network

*** linear Discriminate Analysis
**** Fisher's linear discriminant
输入为j=0，1类样本，每类分别 $N_{j}$ 个样本
$\mu_j = \frac{1}{N_{j}} \sum x$ $x \in N_{j}$
$\Sigma_{j} = \sum(x-\mu_{j})(x-\mu_{j})^{T}$, $x \in N_{j}$

$argmax(J) = \frac{\omega^{T} (\mu_0-\mu_1)(\mu_0-\mu_1)^T
\omega}{\omega^T(\Sigma_0+\Sigma_1)\omega } =  \frac{\omega^{T} S_{b}
\omega}{\omega^T S_{w} \omega }$
**** Fisher's linear discriminant with Kernel method
$$ J(w) = \frac{(m_{2}-m_{1})^{2}}{s_{1}^{2} + s_{2}^{2}} = 
  \frac{w^{T}(m_{2}-m_{1})^{T}(m_{2}-m_{1}) w}{ w^{T}(s_{1}^{2} +
  s_{2}^{2})w}$$

$$ w = \sum^{L}_{k=1} \alpha_{k} \phi(x_{k}) $$

$$ m = \frac{1}{L_{i}} \sum^{Li}_{n=1}\phi(x_{n}^{i})$$

$$ w^{T} m_{i} = \alpha^{T}M_{i}$$

$$ M_{i} = \frac{1}{L_{i}}\sum^{L}_{k=1}\sum^{L_{i}}_{n=1}
k(x_{k},x_{n}^{i})$$

Numerator:$$w^{t}S_{B}w = \alpha^{T}M\alpha$$
Denominator:
$$ w^{T}S_{w}w = \alpha^{T} N \alpha$$
$$ N = \sum_{i=1,2}K_{i}(I-1/L)K_{i}^{T} $$
$$ (K_{i})_{n,m} = k(x_{n}, x_{m}^{i})$$

**** Probabilistic Generative Model
用贝叶斯定理求出每个可能的概率，再取最大的值
#+begin_src 
one two class case
#+end_src
$$ P(C_{1}|x) = \frac{P(C_{1}|x)P(C_{1})}{P(C_{1}|x)P(C_{1}) +
P(C_{2}|x)P(C_{2})} = \frac{1}{1+exp(log
\frac{P(C_{1}|x)P(C_{1})}{P(C_{2}|x)P(C_{2})} )}$$
即可以 Logistic sigmoid 函数求解

#+begin_src 
multi class case
#+end_src

$$P(C_{k}|x) = \frac{P(x|C_{k})P(C_{k})}{\sum_{j} P(x|C_{j})P(C_{j})}$$

即可以用 Softmax funtion 来求解

**** Probabilistic Discriminant Model
Better predictive performance if assumptions about class-conditional
distributions not correct.

和 generative model 一样求解，同样也有二分和多分类，但是该类问题设为
logical regression, See logical regression

*** logistic regression
用 logical sigmoid function 来作二分类判断，检验概率是否过半

*** Principe Component Analysis
**** PCA Algorithms
将原来的数据坐标进行线性组合，组成新的坐标基底，让数据在新基底
上投影最小化，以去除，压缩该些维度
1. 将数据中心化
2. 求出数据在所有特性的协方差矩阵
3. 如果矩阵是方阵，则可以直接特征值分解
4. 如果矩阵不是方阵，则先乘以转置，再特征值分解，注意此时求得特征值要开方
5. 如果不是方阵，也可以直接奇异值分解
6. 取出前面的需要的维度，多余的被压缩了
**** Probabilistic generative model for PCA
State the probabilistic generative model underlying Probabilistic PCA
with a K-dimensional latent space and observations $x\in R^{D}$ . Define
all three random variables and their distribution.

Hidden Variable z in K-dimension from probabilistic generative PCA:
we can transfer z into standard gaussian distribution,
$$p(\vec{z}) = N(0, I), \vec{z} \in R^{K}, \vec{z} \sim N(0, I)$$

observation variable x in D-dimension giving z:
$$p(\vec{x}|\vec{z}) = N(\vec{W}\vec{z} + u, \sigma^{2}I), \vec{x} \in
R^{D}$$
$$\vec{x} = Wz + u + \epsilon, \epsilon \sim N(0, \sigma^{2}I)$$

So, $p(x) = \int p(x|z)p(z)dz$
$$E(x) = E(Z + u + \epsilon) = u$$
$$Cove[x] = E[(Wz + u + \epsilon)(Wz + u + \epsilon)^{T}]
= E(W^{T}W) + E(\epsilon \epsilon^{T}) = WW^{T} + \sigma^{2}I$$

$$ x \sim N(u, Cov[x])$$

*** K-Means
输入样本集 D: $x_{1}, x_{1}, x_{2},,,x_{m}$
聚类数 k, 
最大迭代数 N,
期望输出: $C_{1}, C_{2},,,C_{k}$

随机初始化k个聚类中心，并作不同类别的标记
for i= 1,2,..N:
    初始化所有C
    计算每个点到每个中心的距离，并被最小距离的聚类中心标记
    对于所有相同标记的聚类更新中心，再重复上一步骤，直到没有变化为止
    

#+begin_src python :results output
  import random
  import numpy as np
  import matplotlib.pyplot as plt

  b = []
  for i in range(100):
      a = np.array(list([(20,50),(30,10),(60,30)]))
      for j in range(a.shape[0]):
          for k in range(a.shape[1]):
              a[j][k] += random.randint(0,30)
              b.append(a[j])

  b = np.array(b)
  plt.plot(b[:,0], b[:,1], 'ro')
  plt.title("toy data")
  plt.show()


  # sklearn API
  from sklearn.cluster import KMeans
  y_pred = KMeans(n_clusters=3, random_state=9).fit_predict(b)
  plt.scatter(b[:, 0], b[:, 1], c=y_pred)
  plt.title("toy data with sklearn API")
  plt.show()

  # manual
  def findClosestCentroids(X, centroids):
      distance = np.zeros((len(X),len(centroids)))
      for i in range(len(X)):
          for j in range(len(centroids)):
              distance[i,j] = np.linalg.norm(X[i,:]-centroids[j,:])
      return np.argmin(distance,axis=1)

  def computeCentroids(X, idx, K):
      centroids = np.zeros((K,X.shape[1]))
      for i in range(K):
          centroids[i,:] = np.mean(X[idx == i],axis = 0)
      return centroids


  def runkMeans(X,K,max_iters):
      indexs = np.random.choice(np.array(range(len(X))), K,replace=False)
      centroids = X[indexs]
      for max_iter in range(max_iters):
          idx = findClosestCentroids(X, centroids)
          centroids = computeCentroids(X, idx, K)
          colors = ['','','']
          for i in range(K):
              plt.scatter(X[idx==i, 0], X[idx==i, 1])
          plt.scatter(centroids[:, 0], centroids[:, 1], c='r')
          plt.title("toy data with manual {} time".format(max_iter))
          plt.show()
  K = 3
  max_iters = 3
  runkMeans(b,K,max_iters)

#+end_src

#+RESULTS:

*** EM algorithms
E step: compute responsibilites $\gamma_{nk}$ given current $\pi_{k},
\mu_{k}, \Sigma_{k}$
$$ \gamma_{nk} = \frac{\pi_{k} N(x|\mu_{k}, \Sigma_{k}}{ 
\sum_{k=1}^{K}\pi_{k}N(x|\mu_{k},\Sigma_{k})}$$

M step: update  $\pi_{k},\mu_{k}, \Sigma_{k}$ given $\gamma_{nk}$.
according to the derivative of $log p(x|\pi, \mu, \Sigma) =
\sum^{N}_{n=1}log \sum^{K}_{k=1} \pi N(x_{k}|\mu_{k}, \Sigma_{k})$
with respect to the $\pi_{k},\mu_{k}, \Sigma_{k}$,

cluster means: $$\mu_{k} = \frac{1}{N_{k}} \sum^{N}_{n=1} \gamma_{nk} x_{n}$$

cluster covariances: $$\Sigma_{k} =
\frac{1}{N_{k}}\sum^{N}_{n=1}\gamma_{nk}(x_{n}-\mu_{k})(x_{n}-\mu_{k})^{T}$$

cluster priors:$$\pi_{k} = \frac{N_{k}}{N}$$

** link
机器学习代码和课件可在此下 [fn:xiaoxiong]
[fn:xiaoxiong]https://jia666-my.sharepoint.com/:f:/g/personal/hmp121_xkx_me/EsaG70R5NXdMgC7V1aYwyQcB5C-8nFiSYTWG3Uur9ZvbEA?e=1D2uPf

* Deep learing

Recepte Field Size
viald = same: $RF = 1 + \sum (k_l -1)$
viald != same : $RF = (RF^{'} -1)*s + k$
* computer vision
** Ubungs 01

<<computer version ubung 01>>
Solution erkälerung
Mediuem 
under Quantil 25%
ober Quantil  75%

Entopie
$$ E = \sum h_{rel}(w) log^{\frac{1}{h_{rel}}(w)} = - \sum p log^{p}$$

Anisotorpie 

Paar-Grauwertmatrix
* sensor Fusion
** Sensor Dataverarbeitung
Tensor Fehler, Präzision: stochastisch
Richtigkeit: systematisch



** concepts
|----------------+-----------------------------------------------------|
| competitive    | many sensor for the same place für higher accuracy  |
| complementary  | many sensor for many places für higher completeness |
| dead reckoning | errors accumulation over previous knowlegde         |
|----------------+-----------------------------------------------------|

$$ y = H x + e $$
|----------------------+-----------------------------------------------|
| measurement equation | projects the state onto the measurement space |
| y                    | measurement                                   |
| x                    | state                                         |
| H                    | measurement matrix                            |
| e                    | measurement error                             |
|----------------------+-----------------------------------------------|

*Jacobian Matrix* one order


*Hessian Matrix*  two order

*Partial Matrix*
- $$ \frac{\partial}{\partial x}c^{T}x = \frac{\partial}{\partial x}x^{T}c  = c $$
- $$ \frac{\partial}{\partial x}x^{T}Ax = 2 Ax $$
- $$ \frac{\partial}{\partial x}Ax = \frac{\partial}{\partial x}x^{T}A = A $$

  


*** Sensor Data Fusion
